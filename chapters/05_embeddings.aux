\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Embeddings}{39}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Embeddings}{40}{section.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Confronto tra Smilodon e Thylacosmilus\relax }}{40}{table.caption.25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Ipotesi distribuzionale}{40}{subsection.5.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Confronto tra Smilodon e Thylacosmilus\relax }}{41}{figure.caption.26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Semantica lessicale}{41}{subsection.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Proprietà del significato lessicale}{42}{subsection.5.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{L'ipotesi di Osgood: il significato come punto in uno spazio vettoriale}{43}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}Semantica vettoriale}{44}{subsection.5.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Visualizzaizone 2-dimensionale di 200 di uno spazio 200-dimensionale dell'algortimo word2vec Embeddings per delle parole vicino alla parole \textit  {sweet}.\relax }}{45}{figure.caption.28}\protected@file@percent }
\newlabel{fig:embeddings_plot_sweets}{{5.2}{45}{Visualizzaizone 2-dimensionale di 200 di uno spazio 200-dimensionale dell'algortimo word2vec Embeddings per delle parole vicino alla parole \textit {sweet}.\relax }{figure.caption.28}{}}
\newlabel{fig:embeddings_plot_sweets@cref}{{[figure][2][5]5.2}{[1][45][]45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.5}Simple count-based embeddings}{45}{subsection.5.1.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Esempio di matrice di co-occorrenza con finestra di contesto di ampiezza 1.\relax }}{46}{table.caption.29}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Estratto reale della matrice word-context calcolata sul corpus Wikipedia, come mostrato in Fig.~5.3 di \blx@tocontentsinit {0}\cite {jurafsky}.\relax }}{47}{table.caption.30}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Estratto reale di vettori di co-occorrenza calcolati sul corpus Wikipedia (mostra solo alcune dimensioni del vettore).\relax }}{47}{figure.caption.31}\protected@file@percent }
\newlabel{fig:jurafsky53}{{5.3}{47}{Estratto reale di vettori di co-occorrenza calcolati sul corpus Wikipedia (mostra solo alcune dimensioni del vettore).\relax }{figure.caption.31}{}}
\newlabel{fig:jurafsky53@cref}{{[figure][3][5]5.3}{[1][46][]47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.6}Cosine Similarity}{48}{subsection.5.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.7}Word2Vec}{49}{subsection.5.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Il classificatore}{51}{section*.32}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Lo skip-gram apprende in totale due insiemi di embedding, uno per i target e uno per i contesti, per un totale di $2|V|$ vettori, ciascuno di dimensione $d$. L’addestramento del modello ha quindi un unico scopo: apprendere questi vettori in modo da massimizzare la probabilità che le parole realmente vicine nel testo risultino simili nei loro embedding\relax }}{53}{figure.caption.37}\protected@file@percent }
\newlabel{fig:skipgram_structure}{{5.4}{53}{Lo skip-gram apprende in totale due insiemi di embedding, uno per i target e uno per i contesti, per un totale di $2|V|$ vettori, ciascuno di dimensione $d$. L’addestramento del modello ha quindi un unico scopo: apprendere questi vettori in modo da massimizzare la probabilità che le parole realmente vicine nel testo risultino simili nei loro embedding\relax }{figure.caption.37}{}}
\newlabel{fig:skipgram_structure@cref}{{[figure][4][5]5.4}{[1][53][]53}}
\@writefile{toc}{\contentsline {subsubsection}{Algoritmo di apprendimento}{55}{section*.38}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.8}Proprietà semantiche degli embeddings}{56}{subsection.5.1.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{1. Influenza della finestra di contesto}{56}{section*.39}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{2. First-order vs. second-order similarity}{57}{section*.40}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{3. Analogical reasoning (modello del parallelogramma)}{57}{section*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{4. Struttura geometrica: ortogonalità e parallelismo}{58}{section*.42}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{5. Effetti pratici}{58}{section*.43}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.9}Embeddings as the input to neural net classifiers}{58}{subsection.5.1.9}\protected@file@percent }
\@setckpt{chapters/05_embeddings}{
\setcounter{page}{59}
\setcounter{equation}{3}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{5}
\setcounter{section}{1}
\setcounter{subsection}{9}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{4}
\setcounter{table}{3}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{lstnumber}{1}
\setcounter{parentequation}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{0}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{blx@maxsegment@0}{0}
\setcounter{blx@sectionciteorder@0}{6}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{Item}{17}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{48}
\setcounter{FancyVerbLine}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{lstlisting}{0}
\setcounter{section@level}{2}
}
