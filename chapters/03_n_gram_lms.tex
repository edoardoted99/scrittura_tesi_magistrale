


\chapter{N-gram Language Models}

Si consideri la frase:

\begin{quote}
	Il Po è il fiume più grande della \dots
\end{quote}
È naturale aspettarsi completamenti come \emph{Lombardia} o \emph{penisola italiana}, mentre parole come \emph{frigorifero} risultano improbabili. Questa intuizione — scegliere la parola più plausibile dato un contesto — è alla base dei \textit{language models} (LM), che stimano la probabilità di una parola successiva $P(w_t \mid w_{1:t-1})$.  
Tali modelli hanno applicazioni pratiche (correzione ortografica, riconoscimento vocale, predizione di parole) e sono il fondamento anche dei moderni LLM, addestrati tramite \emph{next-word prediction}. In questo capitolo ci concentriamo sugli \emph{n-grammi}, i modelli più semplici e trasparenti: una sequenza di $n$ parole che approssima
\[
P(w_t \mid w_{1:t-1}) \approx P(w_t \mid w_{t-n+1:t-1}),
\]
assumendo una dipendenza di Markov di ordine $n{-}1$. Gli n-grammi permettono di introdurre i concetti fondamentali della modellazione del linguaggio: stima delle probabilità dai corpus, valutazione con la \emph{perplexity}, generazione tramite \emph{sampling} e gestione della scarsità di dati con \emph{smoothing}, \emph{interpolation} e \emph{backoff}.

\section{N-Grams}

Si inizi a considerare il compito di calcolare la probabilità
\begin{equation*}
	P(w \mid h)
\end{equation*}
cioè la probabilità di una parola $w$ dato un contesto (o storia) $h$.  
Ad esempio, se $h =$ ``L’acqua del Po è così splendidamente'' e vogliamo conoscere la probabilità che la prossima parola sia ``blu'', stiamo cercando:
\[
P(\text{blu} \mid \text{L’acqua del Po è così splendidamente}).
\]
Un modo diretto per stimare questa probabilità è tramite le frequenze relative osservate in un corpus:  
\[
P(w \mid h) \approx \frac{C(h\,w)}{C(h)},
\]
dove $C(h\,w)$ è il numero di volte in cui la sequenza ``$h$ $w$'' compare nel corpus, e $C(h)$ è il numero di volte in cui compare la sola storia $h$. Tuttavia, anche usando corpora molto grandi, raramente troveremo abbastanza occorrenze per frasi lunghe: il linguaggio è creativo e nuove combinazioni di parole compaiono di continuo. Non possiamo quindi stimare con affidabilità le probabilità di intere frasi basandoci solo sui conteggi.

\subsection{La regola della catena}

Per affrontare il problema, decomponiamo la probabilità con la \textit{chain rule} della probabilità:
\[
P(w_{1:n}) = \prod_{k=1}^{n} P(w_k \mid w_{1:k-1}),
\]
dove $w_{1:k-1}$ indica la sequenza delle prime $k-1$ parole.  
In questo modo, la probabilità di una frase può essere calcolata come prodotto di probabilità condizionate di ogni parola dato il contesto precedente. Ma resta una difficoltà: non possiamo stimare in modo affidabile $P(w_k \mid w_{1:k-1})$ per contesti lunghi, poiché essi compaiono raramente nei dati.

\subsection{L’assunzione di Markov e i modelli n-gram}



Per risolvere il problema si introduce l’\textbf{assunzione di Markov}: invece di considerare tutta la storia, si approssima la dipendenza considerando solo le ultime $N{-}1$ parole.  Si assume quindi che la \textit{probabilità di una parola dipende solo dalla probabilità delle $N{-}1$ precedenti}. Ad esempio, un \textbf{bigramma} ($N=2$) approssima:
\[
P(w_n \mid w_{1:n-1}) \approx P(w_n \mid w_{n-1}),
\]
così che:
\[
P(\text{blu} \mid \text{L’acqua del Po è così splendidamente}) \approx P(\text{blu} \mid \text{splendidamente}).
\]
In generale, per un modello $n$-gram:
\[
P(w_n \mid w_{1:n-1}) \approx P(w_n \mid w_{n-N+1:n-1}).
\]
Sostituendo questa approssimazione nella regola della catena, otteniamo una stima per la probabilità di un’intera sequenza:
\[
P(w_{1:n}) \approx \prod_{k=1}^n P(w_k \mid w_{k-N+1:k-1}),
\]
che per un bigramma diventa
\begin{equation*}
	P(w_{1:n}) \approx \prod_{k=1}^n P(w_k \mid w_{k-2+1:k-1})  =\prod_{k=1}^n P(w_k \mid w_{k-1}).
\end{equation*}

\subsection{Stima MLE per modelli n-gram}

La \textbf{stima di massima verosimiglianza (MLE)} assegna la probabilità a una sequenza dividendo i conteggi osservati: la probabilità di una parola dipende dalla frequenza relativa al suo contesto.

\begin{equation}
	P(w_n \mid w_{n-1}) = \frac{C(w_{n-1}, w_n)}{C(w_{n-1})}
\end{equation}
Per generalizzare:
\begin{equation}
	P(w_n \mid w_{n-N+1:n-1}) = \frac{C(w_{n-N+1:n})}{C(w_{n-N+1:n-1})}
\end{equation}

Queste stime funzionano bene nei casi frequenti, ma soffrono quando i bigrammi o trigrammi sono rari o assenti. Per questo motivo, nei paragrafi successivi verranno introdotte tecniche di \textit{smoothing}.

\subsection{Calcolo in log-spazio e modelli n-gram estesi}

Per evitare problemi di \textit{underflow numerico}, le probabilità nei modelli di linguaggio vengono calcolate in \textbf{logaritmo}:
\[
\log(p_1 \cdot p_2 \cdot \ldots \cdot p_n) = \log p_1 + \log p_2 + \ldots + \log p_n.
\]
Tutte le operazioni di moltiplicazione diventano somme, più stabili e computazionalmente efficienti. Quando i dati lo permettono, si utilizzano modelli con contesti più lunghi (trigrammi, 4-grammi, 5-grammi). Esistono anche dataset su larga scala come Google N-grams o COCA, e tecniche avanzate come gli $\infty$-gram che usano strutture come \textit{suffix arrays} per gestire n arbitrario.

\subsection{Set di addestramento, sviluppo e test}

Per valutare un modello linguistico si usano tre insiemi distinti:
\begin{itemize}
	\item \textbf{Training set}: per stimare i parametri del modello (conteggi, probabilità).
	\item \textbf{Development set (devset)}: per testare modifiche durante lo sviluppo.
	\item \textbf{Test set}: usato una sola volta per valutazione finale e imparziale.
\end{itemize}

Un buon test set riflette il dominio applicativo. I modelli non devono mai "vedere" il test set in fase di training: ciò porterebbe a stime artificialmente alte e a \textit{overfitting}.

\subsection{Perplexity}

La \textbf{perplessità} misura quanto bene un modello predice un testo. È l’inverso normalizzato della probabilità del test set:
\[
\text{Perplexity}(W) = P(w_1, w_2, \ldots, w_N)^{-\frac{1}{N}} \, .
\]
oppure, per modelli n-gram:
\[
\text{Perplexity}(W) = \sqrt[N]{\prod_{i=1}^N \frac{1}{P(w_i \mid w_{i-n+1:i-1})}} \, .
\]
Per evitare problemi numerici, la formula può essere riscritta in \textbf{logaritmo}:
\[
\text{Perplexity}(W) = \exp\left(-\frac{1}{N} \sum_{i=1}^N \log P(w_i \mid w_{i-n+1:i-1})\right) \, .
\]
Più è bassa la perplessità, più il modello è predittivo.

\subsection{Perplessità come fattore medio di diramazione}

La perplessità può essere interpretata anche come il \textbf{fattore medio di diramazione} di una lingua. Il \textit{branching factor} rappresenta il numero medio di parole che possono seguire una determinata parola. Consideriamo un linguaggio artificiale molto semplice con un vocabolario costituito da tre colori:
\[
L = \{\text{red}, \text{blue}, \text{green}\}
\]
Supponiamo che questo linguaggio sia deterministico e che ogni parola possa seguire qualsiasi altra con probabilità uniforme. Il fattore di diramazione in questo caso è 3. Creiamo ora un modello linguistico probabilistico A, in cui ogni parola segue qualsiasi altra con probabilità \( \frac{1}{3} \), basato su un training set con conteggi uguali per i tre colori. Consideriamo il seguente test set:
\[
T = \text{red red red red blue}
\]
La perplessità del modello A sul test set T sarà:
\[
\text{Perplexity}_A(T) = P_A(\text{red red red red blue})^{-\frac{1}{5}} = \left(\frac{1}{3^5}\right)^{-\frac{1}{5}} = 3
\]
Ora consideriamo un secondo modello linguistico B, addestrato su un training set in cui la parola "red" è molto più frequente. Le probabilità sono le seguenti:
\[
P(\text{red}) = 0.8, \quad P(\text{green}) = 0.1, \quad P(\text{blue}) = 0.1
\]
Calcolando la perplessità sullo stesso test set per il modello B otteniamo:
\[
\text{Perplexity}_B(T) = P_B(\text{red red red red blue})^{-\frac{1}{5}} = (0.8^4 \cdot 0.1)^{-\frac{1}{5}} = 0.04096^{-\frac{1}{5}} \approx 1.89
\]
Anche se il vocabolario contiene sempre tre parole (quindi il branching factor "teorico" resta 3), il modello B è molto più sicuro su quale parola verrà dopo (prevede spesso "red"). Questo rende la sequenza più prevedibile e quindi la perplessità più bassa. La perplessità può quindi essere vista come un \textbf{fattore di diramazione pesato}, che tiene conto delle probabilità effettive assegnate dal modello linguistico.

\subsection{Campionamento da un modello linguistico}

Un modo fondamentale per comprendere e visualizzare il comportamento di un modello linguistico è \textbf{campionare} da esso. 

Campionare da una distribuzione significa scegliere dei valori in modo casuale, proporzionalmente alla loro probabilità. In questo contesto, campionare da un modello linguistico significa \textbf{generare frasi} secondo le probabilità apprese dal modello: le frasi più probabili verranno generate più frequentemente, mentre quelle improbabili appariranno raramente.

Questa tecnica fu proposta già nei primi lavori di Shannon (1948) e di Miller e Selfridge (1950). È particolarmente semplice da visualizzare nel caso dei modelli \textbf{unigramma}.

\subsubsection*{Esempio: campionamento da un modello unigramma}

Supponiamo di avere un modello unigramma in cui ogni parola ha una certa probabilità di comparire. Possiamo immaginare tutte le parole dell’inglese (o dell’italiano) disposte su un intervallo di probabilità compreso tra 0 e 1, ognuna occupando un sottointervallo proporzionale alla sua frequenza. 

La figura seguente (Fig. \ref{fig:sampling}) rappresenta visivamente questa distribuzione: scegliendo un numero casuale tra 0 e 1, troveremo l’intervallo in cui cade quel numero e stamperemo la parola corrispondente. Ripetendo il processo finché non si genera un token di fine frase (es. \texttt{</s>}), otteniamo una frase campionata.

\begin{figure}[h]
	\centering
	%\includegraphics[width=0.9\textwidth]{sampling_unigram.png} % Inserire immagine se disponibile
	\caption{Visualizzazione del campionamento da un modello unigramma. Le parole più frequenti (es. \textit{the}, \textit{of}, \textit{a}) occupano intervalli più grandi, quindi è più probabile che vengano selezionate.}
	\label{fig:sampling}
\end{figure}

\subsubsection*{Campionamento da modelli bigramma e oltre}

Lo stesso principio si applica ai modelli \textbf{bigramma} o \textbf{n-gramma} di ordine superiore:

\begin{itemize}
	\item Si inizia campionando una parola iniziale (es. \texttt{<s>}, o secondo le probabilità iniziali).
	\item Si sceglie poi una seconda parola in base alla distribuzione condizionata sul primo bigramma (\( P(w_2 \mid w_1) \)).
	\item Si continua campionando parole successive secondo le probabilità condizionate (\( P(w_n \mid w_{n-1}) \), o più in generale \( P(w_n \mid w_{n-N+1:n-1}) \)) finché non si genera il token \texttt{</s>}.
\end{itemize}

Questo tipo di generazione è utile non solo per \textbf{valutare l’apprendimento del modello}, ma anche come base per applicazioni di \textit{text generation}, \textit{autocomplete} o \textit{chatbot}.
\section{Smoothing, Interpolation e Backoff}

Uno dei problemi principali dei modelli n-gram è la presenza di \textbf{zeri}: sequenze di parole possibili che non compaiono nel \emph{training set} ma che possono apparire nel \emph{test set}. In questi casi la stima MLE assegna probabilità zero, con due conseguenze:  
\begin{enumerate}
	\item si sottostima la probabilità di sequenze plausibili;  
	\item la probabilità di un’intera frase può diventare zero, rendendo impossibile il calcolo della \emph{perplexity}.  
\end{enumerate}

Per affrontare questo problema si utilizzano tecniche di \textbf{smoothing} o \textbf{discounting}, che ridistribuiscono parte della massa di probabilità dagli eventi frequenti a quelli rari o non osservati. Esistono varie strategie, tra cui \emph{Laplace smoothing}, \emph{add-k smoothing}, \emph{interpolation} e \emph{backoff}.

\subsection{Laplace (Add-One) Smoothing}

Il metodo più semplice consiste nell’aggiungere 1 a tutti i conteggi prima della normalizzazione. Per un unigramma:
\[
P_{\text{Laplace}}(w_i) = \frac{c_i + 1}{N + V},
\]
dove $c_i$ è la frequenza della parola $w_i$, $N$ il numero totale di token e $V$ la dimensione del vocabolario.  

Per i bigrammi:
\[
P_{\text{Laplace}}(w_n \mid w_{n-1}) = \frac{C(w_{n-1}, w_n) + 1}{C(w_{n-1}) + V}.
\]

Questo metodo garantisce probabilità non nulle, ma introduce una forte distorsione: sequenze molto frequenti vengono ``schiacciate'' e quelle mai osservate ricevono troppa probabilità. Per questo motivo non è usato nei moderni modelli di linguaggio, ma resta utile come base concettuale.

\subsection{Add-k Smoothing}

Un’estensione del metodo precedente consiste nell’aggiungere non 1 ma una costante $k$ (anche frazionaria):
\[
P_{\text{Add-k}}(w_n \mid w_{n-1}) = \frac{C(w_{n-1}, w_n) + k}{C(w_{n-1}) + kV}.
\]

La scelta di $k$ viene fatta tipicamente ottimizzando su un \emph{devset}. Sebbene riduca gli effetti negativi dell’add-one, anche questo metodo non si dimostra particolarmente efficace per il \emph{language modeling}.

\subsection{Interpolazione}

Un approccio più robusto è combinare modelli di diverso ordine. Ad esempio, se un trigramma non è mai stato osservato, possiamo stimarne la probabilità tramite il corrispondente bigramma o unigramma. La \textbf{linear interpolation} calcola quindi:
\[
\hat{P}(w_n \mid w_{n-2}, w_{n-1}) = \lambda_1 P(w_n) + \lambda_2 P(w_n \mid w_{n-1}) + \lambda_3 P(w_n \mid w_{n-2}, w_{n-1}),
\]
con $\lambda_1 + \lambda_2 + \lambda_3 = 1$.  

I pesi $\lambda$ non sono fissati a priori, ma appresi da un \emph{held-out set}, scegliendo quelli che massimizzano la probabilità dei dati di validazione. Questo metodo consente di sfruttare al meglio le informazioni disponibili, bilanciando specificità e robustezza.

\subsection{Backoff e Stupid Backoff}

In alternativa all’interpolazione, si può ricorrere al \textbf{backoff}: se un $n$-gramma non è disponibile, si ``retrocede'' a un $(n{-}1)$-gramma, e così via fino agli unigrammi. Nei modelli classici, per garantire una distribuzione di probabilità corretta, si effettua una forma di \emph{discounting}.  

Una variante molto usata in applicazioni pratiche è lo \textbf{stupid backoff}. Qui non si cerca di mantenere una distribuzione valida, ma si applica una semplice regola:
\[
S(w_n \mid h) =
\begin{cases}
	\dfrac{C(h w_n)}{C(h)} & \text{se } C(h w_n) > 0, \\
	\lambda \, S(w_n \mid h') & \text{altrimenti},
\end{cases}
\]
dove $h'$ è il contesto accorciato e $\lambda$ un fattore costante (tipicamente $0.4$).  

Lo \emph{stupid backoff} non definisce una vera distribuzione probabilistica, ma è molto efficace su grandi corpus e in scenari di ricerca su larga scala.

\section{Perplessità ed Entropia}

Abbiamo introdotto la \textbf{perplessità} come misura di valutazione per i modelli n-gram.  
In realtà essa nasce dal concetto di \textbf{entropia} in teoria dell’informazione, che fornisce il legame con la \textbf{cross-entropia}. Per distinguere correttamente i concetti, introduciamo alcune definizioni fondamentali:

\begin{itemize}
	\item \textbf{Vocabolario} ($V$): l’insieme finito delle parole (token) conosciute dal modello. Ad esempio, in un corpus ridotto di italiano potremmo avere
	\[
	V = \{\text{gatto}, \text{cane}, \text{dorme}, \text{corre}\}.
	\]
	
	\item \textbf{Linguaggio} ($L$): l’insieme di tutte le \emph{sequenze di parole} costruibili a partire da $V$. Ad esempio:
	\[
	L = \{\text{gatto dorme}, \ \text{cane corre}, \ \text{gatto corre}, \ldots \}.
	\]
	In generale $L$ è potenzialmente infinito, poiché le sequenze possono avere lunghezza arbitraria.
	
	\item \textbf{Sequenze di parole} ($w_{1:n}$): una specifica frase di lunghezza $n$, cioè una realizzazione concreta di elementi di $V$.  
	Per esempio $w_{1:3} = (\text{il}, \ \text{gatto}, \ \text{dorme})$.
\end{itemize}

Quando in formule di entropia compare una somma del tipo
\[
\sum_{w_{1:n} \in L} p(w_{1:n}) \log p(w_{1:n}),
\]
il simbolo $L$ indica l’insieme di tutte le sequenze possibili di lunghezza $n$ (non il solo vocabolario).  
In altre parole, mentre $V$ è l’insieme statico delle parole, $L$ rappresenta l’insieme dinamico di tutte le frasi che la lingua può generare.


\subsection{Entropia}

L’entropia di una variabile casuale $X$ con distribuzione $p(x)$ è definita come:
\[
H(X) = - \sum_{x \in \chi} p(x) \log_2 p(x).
\]
Intuitivamente, rappresenta il numero medio di bit necessari a codificare un’informazione in modo ottimale.  
Ad esempio, se tutti gli eventi sono equiprobabili ($p(x) = 1/|\chi|$), l’entropia diventa $\log_2 |\chi|$.  
\subsection{Entropia di una sequenza}

Finora abbiamo considerato l’entropia di una singola variabile casuale (ad esempio, una parola).  
Per un linguaggio naturale, tuttavia, è più utile ragionare su \textbf{sequenze di parole}.  
Sia dunque $W = (w_1, w_2, \ldots, w_n)$ una frase di lunghezza $n$.  
L’entropia della sequenza è definita come:

\[
H(w_1, \ldots, w_n) = - \sum_{w_{1:n} \in L} p(w_{1:n}) \log p(w_{1:n}),
\]

dove $L$ rappresenta l’insieme di tutte le sequenze possibili di lunghezza $n$ in una lingua.  
In altre parole, consideriamo tutte le frasi di lunghezza $n$, ne valutiamo la probabilità e ne calcoliamo l’informazione media.

\paragraph{Tasso di entropia.}  
Poiché l’entropia cresce con la lunghezza della sequenza, conviene normalizzarla per il numero di parole.  
Si definisce quindi il \textbf{tasso di entropia} (o entropia per parola) come:

\[
H(L) = \lim_{n \to \infty} \frac{1}{n} H(w_1, \ldots, w_n).
\]

Questo valore rappresenta la quantità media di informazione (in bit) che ogni parola porta con sé in una lingua.

\paragraph{Il teorema di Shannon–McMillan–Breiman.}  
Questo teorema dice che, se la lingua può essere vista come un processo stocastico stazionario (le regole non cambiano nel tempo) ed ergodico (osservando a lungo una sequenza, si vedono tutte le probabilità “vere”), allora: non è necessario calcolare la somma su tutte le frasi possibili e l’entropia può essere stimata osservando una sola sequenza molto lunga.  
Formalmente:

\[
H(L) = \lim_{n \to \infty} - \frac{1}{n} \log p(w_1, \ldots, w_n).
\]

\paragraph{Intuizione.}  
Il risultato dice che se prendiamo un testo sufficientemente lungo, la media della sorpresa per parola (cioè $-\log p(w_i)$) si stabilizza, e tale valore coincide con l’entropia della lingua.  
In questo modo l’entropia di un linguaggio diventa un concetto misurabile a partire da testi reali, senza dover enumerare tutte le possibili frasi.
\subsection{Cross-Entropy}

Finora abbiamo supposto di conoscere la distribuzione reale $p$ che genera i dati (cioè la “vera lingua”).  
Nella pratica, però, non conosciamo mai $p$: possiamo solo stimarla tramite un modello $m$ (ad esempio, un modello n-gram).  

In questi casi si usa la \textbf{cross-entropy}, che misura quanto bene il modello $m$ approssima la distribuzione reale $p$:

\[
H(p,m) = \lim_{n \to \infty} -\frac{1}{n} \sum_{W \in L} p(W) \log m(W).
\]

Qui stiamo calcolando l’informazione media rispetto alla distribuzione vera $p$, ma valutata con le probabilità fornite dal modello $m$.  
In pratica, chiediamo: \emph{se i dati sono generati da $p$, quanto “costa” codificarli usando il modello $m$?}

\paragraph{Stima su sequenze lunghe.}  
Grazie al teorema di Shannon–McMillan–Breiman, non serve considerare tutte le possibili frasi: basta una sequenza sufficientemente lunga.  
Per un testo di $N$ parole possiamo stimare:

\[
H(W) = -\frac{1}{N} \log m(w_1, \ldots, w_N).
\]

\paragraph{Relazione con l’entropia.}  
Per definizione, la cross-entropy è sempre maggiore o uguale all’entropia vera:
\[
H(p) \leq H(p,m).
\]

- Se il modello $m$ coincide con la distribuzione reale $p$, allora $H(p,m) = H(p)$.  
- Se il modello è impreciso, la cross-entropy è più grande, perché $m$ “spende” più bit del necessario per descrivere i dati.  

\paragraph{Interpretazione intuitiva.}  
Immaginiamo di voler comprimere un testo in italiano.  
- Se usassimo la vera distribuzione $p$ della lingua, otterremmo la massima compressione possibile, pari all’entropia $H(p)$.  
- Usando invece un modello approssimato $m$ (ad esempio un modello bigramma), la compressione sarà meno efficiente: in media useremo più bit per parola.  
La cross-entropy misura esattamente questo “surplus” di informazione richiesto da un modello approssimato rispetto alla distribuzione ideale.
\subsection{Relazione con la Perplessità}

La \textbf{perplessità} è una misura derivata direttamente dalla cross-entropy.  
Ricordiamo che la cross-entropy $H(W)$ ci dice quanti bit di informazione, in media, sono necessari per codificare una parola di un testo usando un modello linguistico $P$.  

Se l’entropia è misurata in bit, allora $2^{H(W)}$ ha un’interpretazione molto intuitiva: rappresenta il \textbf{numero medio di alternative equiprobabili} che il modello deve considerare a ogni passo.  

\paragraph{Intuizione.}  
- Se $H(W) = 1$, significa che in media servono 1 bit per parola: il modello è incerto come se dovesse scegliere tra 2 alternative equiprobabili (come il lancio di una moneta).  
- Se $H(W) = 3$, servono 3 bit per parola: il modello è incerto come se dovesse scegliere tra 8 alternative equiprobabili (come un dado a 8 facce).  
- Se $H(W) = 2.585$, il modello è incerto come se avesse circa 6 alternative ugualmente probabili: questo è il caso del dado a 6 facce equo.  

Per questo motivo la perplessità viene definita come:
\[
\text{Perplexity}(W) = 2^{H(W)}.
\]

\paragraph{Forma pratica.}  
Sostituendo l’espressione della cross-entropy, otteniamo:
\[
\text{Perplexity}(W) = \exp\left(-\frac{1}{N} \sum_{i=1}^N \log P(w_i \mid w_{i-n+1:i-1})\right).
\]

Questa scrittura usa il logaritmo naturale: l’esponenziale ricostruisce il “numero equivalente di scelte equiprobabili”.  

\paragraph{Interpretazione.}  
La perplessità può quindi essere letta come un \textbf{fattore medio di diramazione}:  
quante possibilità il modello considera plausibili per ogni parola.  
Un modello con bassa perplessità è meno “perplesso”: ha meno incertezza e si avvicina di più a catturare la vera distribuzione della lingua.  

\subsection{Entropia e Compressione dei Dati}

Il concetto di entropia non è solo teorico, ma ha applicazioni dirette in informatica, in particolare nella \textbf{compressione dati}.  
Shannon ha dimostrato che l'entropia di una sorgente è il \textbf{limite inferiore} del numero medio di bit necessari per rappresentarne i simboli senza perdita di informazione. In altre parole, nessun algoritmo di compressione potrà mai scendere sotto l’entropia: è un vincolo fondamentale.

\paragraph{Codici a lunghezza fissa.}  
Se si rappresentano i simboli con lo stesso numero di bit, non si tiene conto delle probabilità. Ad esempio, il codice ASCII assegna sempre 8 bit per carattere, anche se alcune lettere sono molto più frequenti di altre. Questo approccio è semplice, ma inefficiente.

\paragraph{Codici a lunghezza variabile.}  
Per avvicinarsi al limite teorico dell’entropia, si usano codici che tengono conto della distribuzione di probabilità:
\begin{itemize}
	\item \textbf{Codifica di Huffman}: assegna sequenze di bit più corte ai simboli frequenti e più lunghe a quelli rari. È usata in algoritmi di compressione come \texttt{ZIP}, \texttt{JPEG}, \texttt{MP3}.
	\item \textbf{Arithmetic coding}: rappresenta l’intera sequenza come un unico numero reale in $[0,1)$. È ancora più efficiente di Huffman ed è usata in standard moderni di compressione come \texttt{H.264} e \texttt{HEVC}.
\end{itemize}

\paragraph{Esempio intuitivo.}  
Nel linguaggio naturale, non tutte le lettere hanno la stessa probabilità (in inglese la lettera \texttt{e} è molto più frequente di \texttt{z}). Una codifica ottimizzata può sfruttare questa distribuzione, riducendo il numero medio di bit per carattere.  
Ad esempio, mentre l’ASCII usa 8 bit per ogni carattere, l’entropia stimata per l’inglese è circa $1.5$ bit per carattere. I moderni algoritmi di compressione testuale si avvicinano a questo limite, ottenendo in pratica circa $2$--$3$ bit per carattere.

\medskip
In sintesi, l’entropia non solo fornisce una misura dell’incertezza, ma stabilisce anche il \textbf{limite teorico della compressione senza perdita}. La perplessità nei modelli linguistici è quindi strettamente connessa a questo concetto: modelli migliori riducono l’incertezza e, di conseguenza, la quantità di informazione necessaria a descrivere un testo.


