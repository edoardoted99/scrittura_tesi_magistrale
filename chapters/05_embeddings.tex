\chapter{Embeddings}

\epigraph{
    Nets are for fish; Once you get the fish you can forget the net.\\
    Words are for meaning; Once you get the meaning you can forget the words.
}{Zhuangzi}

\newpage

\section{Embeddings}
L’asfalto per cui Los Angeles è famosa si trova soprattutto sulle sue autostrade. 
Ma nel mezzo della città c’è un’altra distesa di asfalto, le pozze di catrame di La Brea, 
e questo asfalto conserva milioni di ossa fossili risalenti all’ultima delle ere glaciali dell’Epoca Pleistocenica. 
Uno di questi fossili è lo \textit{Smilodon}, o tigre dai denti a sciabola, immediatamente riconoscibile 
per i suoi lunghi canini. Circa cinque milioni di anni fa viveva un tipo completamente diverso di 
tigre dai denti a sciabola, 
chiamata \textit{Thylacosmilus}, in Argentina e in altre parti del Sud America. 
Thylacosmilus era un marsupiale, mentre Smilodon era un mammifero placentato, 
ma Thylacosmilus aveva gli stessi lunghi canini superiori e, come Smilodon, 
una protezione ossea (una flangia) sulla mandibola inferiore.
La somiglianza tra questi due mammiferi è uno dei numerosi esempi di evoluzione parallela o convergente, 
in cui particolari contesti o ambienti portano all’evoluzione di strutture molto simili in specie differenti (Gould, 1980).

\begin{table}[h!]
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Caratteristica} & \textbf{Smilodon} & \textbf{Thylacosmilus} \\
        \midrule
        Tipo & Mammifero placentato & Marsupiale \\
        Area geografica & Nord e Sud America & Sud America (Argentina) \\
        Epoca & Pleistocene & Miocene \\
        Periodo & 2.5M – 10k anni fa & 9M – 3M anni fa \\
        Denti a sciabola & Sì & Sì \\
        Flangia mandibolare & Sì & Sì \\
        Relazione evolutiva & Felino (o simile) & Marsupiale predatore \\
        Tipo di evoluzione & --- & Convergente con Smilodon \\
        \bottomrule
    \end{tabular}
    \caption{Confronto tra Smilodon e Thylacosmilus}
\end{table}


\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{pictures/smilodon_scheleton.jpg}
        \caption{Smilodon}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{pictures/Thylacosmilus_Holotype_FMNH.jpg}
        \caption{Thylacosmilus}
    \end{subfigure}
    \caption{Confronto tra Smilodon e Thylacosmilus}
\end{figure}

\subsection{Ipotesi distribuzionale}

Il ruolo del contesto è importante anche per la somiglianza di un tipo di organismo meno biologico: 
la parola. \textit{Le parole che compaiono in contesti simili tendono ad avere significati simili.}
Questo legame tra somiglianza nella distribuzione delle parole e somiglianza nel loro significato è chiamato 
\textbf{ipotesi distribuzionale}. L’ipotesi fu formulata per la prima volta negli anni 
’50 da linguisti come Joos (1950), Harris (1954) e Firth (1957), che notarono che parole sinonime 
(come oculist ed eye-doctor) tendevano a comparire nello stesso ambiente (ad esempio vicino a parole come eye o examined), 
con la quantità di differenza di significato tra due parole ``corrispondente più o meno alla quantità di differenza nei 
loro ambienti'' (Harris, 1954, p. 157). 

\subsection{Semantica lessicale}
Iniziamo introducendo qualche principio di base sul significato delle parole. 
Come possiamo rappresentare il significato di una parola in un computer?
In un \textit{n}-gram language model, ogni parola è rappresentata come una 
stringa di lettere o come un indice in una lista di un vocabolario.

Questa, tuttavia, non è una vera rappresentazione del significato della parola.
La parola \textit{cat}, ad esempio, può essere rappresentata semplicemente come la 
stringa ``cat'' oppure come l'indice 2 in una tabella di vocabolario. 
Ma questa codifica non ci dice nulla sul fatto che un gatto sia un animale,
che sia simile a un cane, o che abbia certe proprietà. 
In altre parole, il modello non ``capisce'' il significato: tratta ogni parola come
un semplice simbolo distinto dagli altri.

Il risultato è che il significato non è contenuto nella rappresentazione, ma può
essere solo inferito indirettamente dalla distribuzione della parola nel testo
(dove appare, con quali parole co-occorre, quali sequenze forma). 
Questa idea è alla base dell'ipotesi distribuzionale: parole che compaiono in 
contesti simili tendono ad avere significati simili.

Proprio perché le rappresentazioni simboliche degli \textit{n}-gram non possono
catturare similitudini o relazioni semantiche, sono stati sviluppati modelli più
moderni come gli \textit{embeddings}, che rappresentano ogni parola tramite un 
vettore numerico capace di riflettere somiglianze e strutture semantiche.

In alcune tradizioni filosofiche, per rappresentare il significato di una parola 
si usava semplicemente il suo corrispettivo scritto in maiuscoletto; ad esempio, 
il significato di ``dog'' veniva rappresentato come \textsc{dog}, e quello di 
``cat'' come \textsc{cat}. In altri casi si aggiungeva un apostrofo, come in 
\textsc{dog'}. Questa convenzione, che a volte compare anche in corsi introduttivi 
di logica, riflette una visione puramente simbolica del significato: il simbolo 
che rappresenta la parola \textit{è} il suo significato.

Si tratta però di un modello estremamente limitato e insoddisfacente. 
Scrivere \textsc{dog} al posto di ``dog'' non aggiunge alcuna informazione su ciò 
che un cane è, su quali proprietà abbia, o su come il suo significato sia 
relazionato a quello di parole simili come ``cat'' o a parole opposte come ``rock''. 
Il simbolo è solo una versione stilizzata della parola stessa.

La debolezza di questo approccio è ben illustrata da una battuta attribuita alla 
semanticista Barbara Partee: se chiediamo ``qual è il significato della vita?'', 
la risposta, secondo questo modello, sarebbe semplicemente \textsc{life'}. 
Ovviamente, sappiamo che il significato non può essere ridotto a un'etichetta 
arbitraria.

Ciò che ci interessa, invece, è un modello che ci permetta di cogliere 
somiglianze e differenze tra significati: ad esempio, che ``cat'' sia più simile a 
``dog'' che a ``table'', che ``hot'' sia l'opposto di ``cold'', o che verbi come 
``buy'', ``sell'' e ``pay'' descrivano lo stesso evento di acquisto da tre 
prospettive differenti. Idealmente, una rappresentazione del significato dovrebbe 
permettere anche di derivare inferenze utili per compiti di comprensione del 
linguaggio, come il question answering o il dialogo.


\subsection{Proprietà del significato lessicale}

Possiamo analizzare il significato delle parole attraverso una serie di proprietà
linguistiche fondamentali:

\begin{itemize}
    \item \textbf{Lemma e forme flesse}.  
    Il lemma è la forma canonica di una parola (ad es.\ \textit{mouse}); 
    le sue forme flesse (\textit{mice}, \textit{sung}, \textit{duermes}) sono dette 
    \textit{wordforms}. Ogni lemma può avere molteplici significati, chiamati 
    \textit{sensi} della parola.

    \item \textbf{Polisemia}.  
    Un lemma può essere associato a più sensi distinti (ad es.\ \textit{mouse} come 
    animale oppure come dispositivo elettronico). Questo rende necessaria la 
    \textit{disambiguazione del senso}.

    \item \textbf{Sinonimia}.  
    Due parole (o sensi) sono sinonimi se hanno significati molto simili 
    (ad es.\ \textit{car}/\textit{automobile}). La sinonimia non è mai perfetta: 
    secondo il \textit{principio di contrasto}, differenze formali corrispondono a 
    differenze di significato.

    \item \textbf{Similarità tra parole}.  
    Due parole possono essere simili pur non essendo sinonimi 
    (ad es.\ \textit{cat} e \textit{dog}). La similarità lessicale è utile per 
    compiti come question answering, parafrasi e confronto tra frasi.

    \item \textbf{Correlatezza semantica (relatedness)}.  
    Parole che non sono simili possono comunque essere semanticamente collegate, 
    perché partecipano allo stesso evento o dominio (ad es.\ \textit{coffee} e 
    \textit{cup}). Le parole possono inoltre appartenere allo stesso 
    \textit{campo semantico} (ospedale, ristorante, casa).

    \item \textbf{Connotazione}.  
    Le parole hanno componenti affettive, legate a emozioni, opinioni o valori. 
    Alcune hanno connotazione positiva (\textit{wonderful}), altre negativa 
    (\textit{dreary}). Le connotazioni sono centrali per compiti come 
    sentiment analysis e stance detection.

\end{itemize}


\subsubsection{L'ipotesi di Osgood: il significato come punto in uno spazio vettoriale}

Un contributo fondamentale alla rappresentazione del significato proviene dal 
lavoro di Osgood et al.\ (1957), che studiarono la componente affettiva delle parole. 
Osgood mostrò che i giudizi emotivi associati a una parola possono essere descritti 
lungo tre dimensioni principali:

\begin{enumerate}
    \item \textbf{Valenza}: quanto la parola è percepita come positiva o negativa.
    \item \textbf{Arousal}: quanto la parola induce attivazione emotiva.
    \item \textbf{Dominanza}: quanto la parola implica controllo o sottomissione.
\end{enumerate}

Ogni parola può quindi essere rappresentata come una tripla di valori numerici 
che ne definiscono la posizione in questo spazio tridimensionale. Ad esempio:

\[
\textit{heartbreak} \rightarrow [2.5,\ 5.7,\ 3.6]
\]

L’intuizione rivoluzionaria di Osgood è che il significato di una parola possa essere 
rappresentato come un \textbf{vettore in uno spazio semantico}. Questa idea 
anticipa direttamente i moderni modelli di \textit{word embeddings}, in cui ogni 
parola è descritta come un punto in uno spazio multidimensionale che cattura 
somiglianze, connotazioni e relazioni semantiche.


\subsection{Semantica vettoriale}
Vector semantics è lo standard di rappresentaizone per la rappresentaizone del singificato delle parole nel nlp.
Le radici di questo approccio risalgono agli anni '50 quando due idee conversero:
\begin{enumerate}
    \item L’idea di Osgood (1957), descritta sopra, di rappresentare la connotazione di una parola come un punto in uno spazio tridimensionale.
    \item L'ipotesi ditribuzionale. La proposta di linguisti come Joos (1950), Harris (1954) e Firth (1957) di definire il significato di una parola tramite la sua distribuzione nell’uso linguistico, cioè tramite le parole che la circondano o gli ambienti grammaticali in cui appare.
\end{enumerate}
Questa seconda idea si basa sull'intuizione che due parole che compaiono in 
contesti simili tendono ad avere significati simili. Possiamo illustrare questa 
intuizione con un semplice esempio.

Supponiamo di non conoscere il significato della parola \textit{ongchoi}, ma di 
incontrarla nei seguenti contesti:

\begin{enumerate}
    \setcounter{enumi}{2}
    \item \textit{L'ongchoi è deliziosa saltata con aglio.}
    \item \textit{L'ongchoi è ottima servita con riso.}
    \item \textit{...foglie di ongchoi con salse salate...}
\end{enumerate}

Ora immaginiamo di aver già visto molte di queste parole-contesto in altri esempi, 
come:

\begin{enumerate}
    \setcounter{enumi}{5}
    \item \textit{...gli spinaci saltati con aglio serviti sul riso...}
    \item \textit{...le coste, con i loro gambi e foglie, sono molto gustose...}
    \item \textit{...il cavolo riccio e altre verdure a foglia dal sapore salato...}
\end{enumerate}

Il fatto che \textit{ongchoi} compaia insieme a parole come \textit{riso}, 
\textit{aglio}, \textit{deliziosa} e \textit{salata}, proprio come \textit{spinaci}, 
\textit{coste} o \textit{cavolo riccio}, suggerisce che l'ongchoi sia una 
\textbf{verdura a foglia} simile a queste altre verdure.

Questa è esattamente l'intuizione alla base della semantica vettoriale: possiamo 
rappresentare il significato di una parola osservando e contando le parole che 
compaiono nei suoi contesti. Parole che condividono contesti simili tendono ad 
avere vettori simili e quindi significati simili. I vettori per rappresentare le parole vengono chiamati \textbf{word embeddings}.
La parola ``embedding'' deriva storicamente  dalla suo significato mateamtica ovvero quello di mppare da uno spazio ad un altro.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.7\textwidth]{pictures/embeddings_plot_sweets.png}
    \caption{Visualizzaizone 2-dimensionale di 200 di uno spazio 200-dimensionale dell'algortimo word2vec Embeddings
    per delle parole vicino alla parole \textit{sweet}.}
    \label{fig:embeddings_plot_sweets}
\end{figure}

La figura \ref{fig:embeddings_plot_sweets} mostra una visualizzazione bidimensionale di un insieme di word embeddings appresi dall'algoritmo Word2Vec.
Si vede come parole semanticamente simili (ipoesi distribuzionale) compaiono vicine nello spazio vettoriale (idea di semantica vettoriale).
\subsection{Simple count-based embeddings}

Introduciamo ora il primo modo di calcolare i word vector embeddings. 
Il metodo più semplice è basato sulla \textbf{matrice di co-occorrenza}, 
un modo per rappresentare quanto spesso le parole co-occorrono. 
Definiremo un particolare tipo di co-occorrenza, la \textbf{word-context matrix}, 
nella quale ogni riga della matrice rappresenta una parola del vocabolario 
e ogni colonna rappresenta quanto spesso ogni altra parola del vocabolario 
appare vicina alla parola target. 
Si tratta quindi di una matrice quadrata di dimensione $|V| \times |V|$, 
dove $|V|$ è la dimensione del vocabolario.

Ogni cella $M_{i,j}$ della matrice conterrà il numero di volte che la parola $w_j$ 
compare nel contesto della parola $w_i$. 
Ma cosa significa esattamente “vicina”? 
Possiamo definire una \textbf{finestra di contesto} di ampiezza $k$ che indica 
quante parole a sinistra e a destra consideriamo attorno alla parola target.

\medskip

\noindent
Supponiamo di avere la frase ``il gatto mangia il topo'' e di utilizzare una 
finestra di contesto di ampiezza pari a $k=1$. 
Il vocabolario sarà:

\begin{equation*}
V = \{\textrm{il}, \textrm{gatto}, \textrm{mangia}, \textrm{topo}\}
\end{equation*}

\begin{table}[h!]
\centering
\begin{tabular}{lcccc}
\hline
\textbf{Parola ($w_i$)} & \textbf{il} & \textbf{gatto} & \textbf{mangia} & \textbf{topo} \\
\hline
il      & 0 & 1 & 0 & 1 \\
gatto   & 1 & 0 & 1 & 0 \\
mangia  & 0 & 1 & 0 & 1 \\
topo    & 1 & 0 & 1 & 0 \\
\hline
\end{tabular}
\caption{Esempio di matrice di co-occorrenza con finestra di contesto di ampiezza 1.}
\end{table}

Ogni riga della matrice rappresenta quindi un vettore che descrive una parola.

\medskip

Finora abbiamo visto un esempio molto semplice costruito su una singola frase. 
In un corpus reale, tuttavia, le co-occorrenze sono molto più numerose e i vettori 
risultanti hanno tipicamente dimensioni molto grandi e sono estremamente sparsi. 
Per illustrare questo scenario, riportiamo un estratto reale della matrice di 
co-occorrenza calcolata sul corpus Wikipedia, come mostrato in Fig.~5.3 
di \cite{jurafsky}.

In questo esempio consideriamo quattro parole target 
(\textit{cherry}, \textit{strawberry}, \textit{digital}, \textit{information}) 
e, per scopi illustrativi, solo sei parole di contesto selezionate: 
\textit{aardvark}, \textit{computer}, \textit{data}, \textit{result}, 
\textit{pie}, \textit{sugar}. 
I valori riportati nella tabella seguente sono esattamente quelli presenti nel libro:

\begin{table}[h!]
\centering
\begin{tabular}{lcccccc}
\hline
\textbf{Parola} & \textbf{aardvark} & \textbf{computer} & \textbf{data} & \textbf{result} & \textbf{pie} & \textbf{sugar} \\
\hline
cherry       & 0 & 2    & 8    & 9    & 442 & 25 \\
strawberry   & 0 & 0    & 0    & 1    & 60  & 19 \\
digital      & 0 & 1670 & 1683 & 85   & 5   & 4  \\
information  & 0 & 3325 & 3982 & 378  & 5   & 13 \\
\hline
\end{tabular}
\caption{Estratto reale della matrice word-context calcolata sul corpus Wikipedia, 
come mostrato in Fig.~5.3 di \cite{jurafsky}.}
\end{table}

Come si può osservare, i valori possono essere molto elevati: 
ad esempio, la parola \textit{information} co-occorre 3982 volte con \textit{data} 
e 3325 volte con \textit{computer}. 
Questi valori derivano da milioni di occorrenze nel corpus Wikipedia, motivo per cui 
sono molto più grandi rispetto agli esempi introduttivi.

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\linewidth]{pictures/word_context.png}
\caption{Estratto reale di vettori di co-occorrenza calcolati sul corpus Wikipedia 
(mostra solo alcune dimensioni del vettore).}
\label{fig:jurafsky53}
\end{figure}
\noindent
È importante notare che $|V|$, la dimensionalità del vettore, coincide in genere 
con la dimensione del vocabolario, che nelle applicazioni pratiche può variare 
tra 10.000 e 50.000 parole. Poiché tuttavia la grande maggioranza delle celle 
della matrice di co-occorrenza contiene valore zero, i vettori risultanti sono 
estremamente \textbf{sparsi}. Per gestire in modo efficiente tale sparsità vengono 
utilizzate strutture dati e algoritmi specializzati, come rappresentazioni 
\textit{compressed sparse row} (CSR) o \textit{compressed sparse column} (CSC).

\noindent
Inoltre, invece di usare le semplici frequenze di co-occorrenza, è possibile 
applicare delle funzioni di \textit{pesatura} che ne trasformano i valori per 
migliorare la qualità degli embeddings ottenuti. Alcune di queste trasformazioni 
hanno lo scopo di ridurre l’effetto delle parole molto frequenti o di enfatizzare 
co-occorrenze particolarmente informative. Un esempio classico è il 
\textit{tf-idf} (term frequency–inverse document frequency), che introduce un peso 
maggiore alle parole specifiche e meno comuni. Approfondiremo questo e altri schemi 
di pesatura più avanti.

\medskip

\noindent
Avendo ora sviluppato una certa intuizione sulla rappresentazione distribuzionale 
delle parole, il passo successivo consiste nel definire un modo per 
\textbf{misurare la similarità} tra i vettori ottenuti dalla matrice parola-contesto.  
Il metodo più utilizzato è la \textit{cosine similarity}, che descriviamo di seguito.

\subsection{Cosine Similarity}

La \textbf{cosine similarity} è una misura di similarità tra vettori che valuta 
il coseno dell’angolo compreso tra essi nello spazio vettoriale.  
Data la sua indipendenza dalla lunghezza dei vettori, risulta particolarmente 
adatta a confrontare vettori di frequenze o di pesi, come quelli derivati da 
matrici parola-contesto.

\noindent
Dati due vettori $u$ e $v$, la similarità coseno è definita come:

\begin{equation}
\text{cosine\_sim}(u, v) = 
\frac{u \cdot v}{\|u\| \, \|v\|}
= \frac{\sum_i u_i v_i}{\sqrt{\sum_i u_i^2} \, \sqrt{\sum_i v_i^2}}.
\end{equation}

\noindent
Il valore risultante è compreso tra $-1$ e $1$:
\begin{itemize}
    \item $1$ indica che i vettori puntano nella stessa direzione (massima similarità),
    \item $0$ indica che sono ortogonali (nessuna similarità),
    \item valori negativi indicano direzioni opposte (molto raro nei contesti di NLP).
\end{itemize}

\noindent
Nelle applicazioni di elaborazione del linguaggio naturale la cosine similarity è 
spesso preferita alla distanza Euclidea, perché ci interessa confrontare il 
\textit{pattern} delle co-occorrenze piuttosto che le loro magnitudini assolute.  
Ad esempio, due parole che co-occorrono con gli stessi termini di contesto, anche 
se con frequenze diverse, risulteranno comunque simili.

\medskip

\noindent
La cosine similarity è quindi il principale strumento per valutare la similarità 
tra vettori distribuzionali e rappresenta un passaggio fondamentale prima di 
introdurre i modelli predittivi come Word2Vec, che nascono proprio per superare 
i limiti computazionali e concettuali degli embeddings basati su conteggi.


\subsection{Word2Vec}

Abbiamo visto come costruire vettori semantici basati sui conteggi di 
co-occorrenza, che portano a rappresentazioni ad altissima dimensionalità e 
fortemente sparse. Introduciamo ora una rappresentazione più potente ed 
efficiente: gli \textbf{embeddings densi}, piccoli vettori continui (tipicamente 
50–300 dimensioni) che possono contenere anche valori negativi.

A differenza dei vettori sparsi da 50.000 dimensioni derivati dalle matrici di 
co-occorrenza, questi vettori sono \textbf{densi} e molto più compatti. Ciò 
permette ai modelli di apprendere un numero significativamente inferiore di 
pesi, rendendo l’addestramento più rapido ed efficiente. Inoltre, i vettori densi 
tendono a catturare meglio le relazioni semantiche. Per esempio, nella 
rappresentazione sparsa tradizionale, due sinonimi come \textit{car} e 
\textit{automobile} possono avere coordinate totalmente diverse e non risultare 
vicini nello spazio vettoriale, mentre gli embeddings densi riescono a 
rappresentarne la similarità in modo molto più naturale.

In questa sezione introduciamo uno dei metodi più famosi per calcolare 
embeddings densi: il \textbf{skip-gram with negative sampling} (SGNS). 
L'algoritmo skip-gram è uno dei due modelli del pacchetto software 
\textbf{word2vec} proposto da Mikolov et al. (2013) e viene spesso indicato 
direttamente come ``word2vec''. Questi metodi sono estremamente veloci, 
efficienti da addestrare e ampiamente disponibili come modelli 
pre–addestrati.

\medskip

\noindent
Gli embeddings ottenuti con word2vec sono \textbf{statici}: ogni parola del 
vocabolario è associata a un unico vettore che rimane invariato a prescindere dal 
contesto in cui la parola appare. Successivamente introdurremo invece gli 
\textbf{embeddings contestuali} (o \textbf{contextual embeddings}), come quelli 
prodotti da modelli Transformer quali BERT e GPT, che generano un vettore diverso 
per ciascuna occorrenza della parola, adattandosi al contesto e catturando così i 
diversi sensi che una parola può assumere.

\medskip

\noindent
L’intuizione alla base del modello skip-gram è semplice ma estremamente 
efficace. Invece di contare quante volte una parola $w$ compare vicino a una 
parola target (ad esempio \textit{albicocca}), addestriamo un classificatore su 
una task di predizione binaria che risponde alla domanda:

\begin{center}
``Qual è la probabilità che la parola $w$ compaia nel contesto della parola 
\textit{albicocca}?''
\end{center}

\noindent
Non siamo realmente interessati alla predizione in sé; ciò che ci interessa sono 
i \textbf{pesi} appresi dal classificatore per svolgere questo compito. Quei pesi 
diventano le nostre rappresentazioni distribuzionali, ovvero gli embeddings.

In questo modo, invece di costruire manualmente vettori basati su conteggi, è il 
modello stesso che apprende automaticamente una rappresentazione densa e 
informativa capace di catturare relazioni semantiche e sintattiche tra parole.

\medskip

\noindent
L'intuizione rivoluzionaria alla base di questo approccio è che il semplice testo 
continuo può essere utilizzato come \textbf{segnale di supervisione implicito} 
per addestrare il classificatore. In altre parole, ogni parola che compare nel 
contesto della parola target (ad esempio una parola $c$ che appare vicino a 
\textit{albicocca}) fornisce automaticamente un’etichetta positiva alla domanda 
``È probabile che la parola $c$ compaia nel contesto di \textit{albicocca}?''. 

\noindent
Questo tipo di segnale, noto come \textbf{self-supervision}, consente di evitare 
qualsiasi forma di annotazione manuale. L’idea fu inizialmente proposta 
nell’ambito del \textit{neural language modeling}, quando Bengio et al. (2003) e 
Collobert et al. (2011) mostrarono che un modello neurale in grado di prevedere la 
parola successiva poteva utilizzare proprio la parola seguente nel testo come 
supervisione, apprendendo contestualmente anche una rappresentazione distribuzionale 
per ogni parola.

\noindent
Approfondiremo i modelli neurali nel capitolo successivo, ma è utile notare che 
word2vec rappresenta una versione molto più semplice rispetto ai neural language 
models classici, per due motivi principali. 
Primo, word2vec semplifica il compito: invece di prevedere la parola successiva, 
formula una \textbf{task di classificazione binaria}. 
Secondo, word2vec semplifica l’architettura: al posto di una rete neurale profonda 
con livelli nascosti, utilizza una semplice \textbf{regressione logistica}, molto più 
facile da addestrare.

\noindent
L’intuizione alla base dello skip-gram with negative sampling può essere riassunta 
nei seguenti passi:

\begin{enumerate}
    \item Considerare la parola target e una parola del suo contesto come un 
          \textbf{esempio positivo}.
    \item Campionare casualmente altre parole dal vocabolario per ottenere 
          \textbf{esempi negativi}.
    \item Addestrare un classificatore di regressione logistica a distinguere 
          esempi positivi ed esempi negativi.
    \item Utilizzare i pesi appresi dal modello come \textbf{embeddings} delle parole.
\end{enumerate}

\subsubsection{Il classificatore}

Per comprendere il modello skip-gram, cominciamo dalla \textbf{task di classificazione} che esso deve svolgere.  
L’idea alla base è estremamente semplice: dato una parola target $w$ e una parola candidata $c$, vogliamo stimare la probabilità che $c$ sia realmente una parola di contesto per $w$.

Consideriamo una frase come la seguente:

\begin{quote}
\emph{``\dots limone, un cucchiaio di marmellata di albicocca, un pizzico \dots''}
\end{quote}

e supponiamo di utilizzare una finestra di contesto di ampiezza $\pm 2$.  
La parola target è dunque \textit{albicocca}, mentre le parole che la circondano sono:

\[
c_1=\text{un},\quad 
c_2=\text{cucchiaio},\quad 
w=\text{albicocca},\quad 
c_3=\text{marmellata},\quad 
c_4=\text{di}.
\]

Ogni coppia $(w,c_i)$ costituisce un \textbf{esempio positivo} per il nostro classificatore.  
Vogliamo che il modello assegni:

\[
(\text{albicocca}, \text{marmellata}) \longrightarrow \text{probabilità alta}
\]
\[
(\text{albicocca}, \text{formichiere}) \longrightarrow \text{probabilità bassa}
\]

Formalmente, il classificatore stima:

\begin{equation}
P(+ \mid w, c)
\tag{5.11}
\end{equation}

cioè la probabilità che $c$ sia un vero contesto di $w$.  
Naturalmente, la probabilità che $c$ \emph{non} sia un contesto è:

\[
P(- \mid w, c) = 1 - P(+ \mid w, c).
\]

\medskip
\noindent
\textbf{Come calcoliamo questa probabilità?}

L'intuizione dello skip-gram è che due parole sono buoni vicini nel testo se i loro \textbf{vettori di embedding} sono simili.  
Usiamo quindi il prodotto scalare tra i vettori densi della parola target $\mathbf{w}$ e della parola di contesto $\mathbf{c}$:

\[
\textrm{Similarity}(w,c) \sim \mathbf{w}\cdot \mathbf{c}.
\]

Il prodotto scalare può assumere qualsiasi valore reale, quindi per trasformarlo in una probabilità compresa tra 0 e 1 applichiamo la funzione sigmoide:

\begin{equation}
\sigma(x)=\frac{1}{1+e^{-x}}.
\end{equation}

Otteniamo così il modello probabilistico del classificatore:

\begin{equation}
P(+ \mid w, c)=\sigma(\mathbf{w} \cdot \mathbf{c}).
\tag{5.15}
\end{equation}

Analogamente:

\[
P(-\mid w,c) = \sigma(-\mathbf{w}\cdot \mathbf{c}).
\]

\medskip
\noindent
\textbf{Più parole nel contesto: il caso generale}

Finora abbiamo considerato una singola parola $c$, ma nella realtà abbiamo una finestra con $L$ parole di contesto:

\[
c_1, c_2, \ldots, c_L.
\]

Lo skip-gram adotta una semplificazione fondamentale:  
\textit{le parole nel contesto sono considerate indipendenti l’una dall’altra}.  
Ciò consente di modellare la probabilità complessiva come un semplice prodotto:

\begin{equation}
P(+ \mid w, c_{1:L}) = \prod_{i=1}^L \sigma(\mathbf{w}\cdot\mathbf{c_i}).
\tag{5.17}
\end{equation}

Per stabilità numerica si lavora quasi sempre con il logaritmo:

\begin{equation}
\log P(+ \mid w, c_{1:L}) = \sum_{i=1}^L \log \sigma(\mathbf{w}\cdot \mathbf{c_i}).
\tag{5.18}
\end{equation}

\medskip
\noindent
In sintesi, lo skip-gram addestra un classificatore che assegna una probabilità alla coppia ``parola target + finestra di contesto'' basandosi sulla similarità tra i rispettivi vettori di embedding.  
Per calcolare questa probabilità sono necessari due tipi di vettori per ogni parola del vocabolario:

\begin{itemize}
    \item un vettore quando la parola compare come \textbf{target} (matrice $W$),
    \item un vettore distinto quando la parola compare come \textbf{contesto} o \textbf{rumore} (matrice $C$).
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{pictures/embeddings_w2vec.png}
    \caption{Lo skip-gram apprende in totale due insiemi di embedding, uno per i target e uno per i contesti, per un totale di $2|V|$ vettori, ciascuno di dimensione $d$.  
        L’addestramento del modello ha quindi un unico scopo: apprendere questi vettori in modo da massimizzare la probabilità che le parole realmente vicine nel testo risultino simili nei loro embedding}
    \label{fig:skipgram_structure}
\end{figure}

\medskip
\noindent
\textbf{Nota sulla dimensione del contesto.}
Se la finestra ha ampiezza $\pm k$ parole, allora il numero totale di parole nel contesto è:

\[
L = 2k.
\]

Ad esempio, una finestra $\pm 2$ come nel nostro caso produce $L = 4$ parole di contesto.  
Per questo motivo, nelle formule usiamo la notazione compatta $c_{1:L}$ per indicare le $L$ parole attorno al target $w$.
\medskip
\noindent
\textbf{Esempio concreto delle matrici $W$ e $C$.}
Riprendiamo la frase d’esempio e supponiamo che il vocabolario locale sia:

\[
V = \{\text{limone},\ \text{un},\ \text{cucchiaio},\ \text{marmellata},\ \text{albicocca},\ \text{pizzico}\}.
\]

Supponiamo inoltre che la dimensione degli embedding sia $d=3$ (solo per semplicità espositiva).  
Lo skip-gram mantiene \emph{due} vettori per ogni parola: uno come \textbf{target} (matrice $W$) e uno come \textbf{contesto} (matrice $C$).  
In forma schematica:

\[
W =
\begin{bmatrix}
\mathbf{w}_{\text{limone}} \\
\mathbf{w}_{\text{un}} \\
\mathbf{w}_{\text{cucchiaio}} \\
\mathbf{w}_{\text{marmellata}} \\
\mathbf{w}_{\text{albicocca}} \\
\mathbf{w}_{\text{pizzico}} \\
\end{bmatrix}
=
\begin{bmatrix}
0.1 & 0.3 & -0.2 \\
-0.4 & 0.2 & 0.1 \\
0.6 & -0.1 & 0.0 \\
-0.2 & 0.5 & 0.4 \\
0.8 & 0.7 & -0.3 \\
-0.1 & -0.4 & 0.2
\end{bmatrix}
\]

\[
C =
\begin{bmatrix}
\mathbf{c}_{\text{limone}} \\
\mathbf{c}_{\text{un}} \\
\mathbf{c}_{\text{cucchiaio}} \\
\mathbf{c}_{\text{marmellata}} \\
\mathbf{c}_{\text{albicocca}} \\
\mathbf{c}_{\text{pizzico}} \\
\end{bmatrix}
=
\begin{bmatrix}
0.0 & -0.2 & 0.1 \\
-0.3 & 0.4 & 0.5 \\
0.2 & 0.1 & -0.3 \\
0.7 & -0.1 & 0.2 \\
-0.5 & 0.6 & 0.3 \\
0.1 & -0.4 & -0.2
\end{bmatrix}
\]

Per la coppia positiva $(\text{albicocca}, \text{marmellata})$ il classificatore calcolerebbe:

\[
\mathbf{w}_{\text{albicocca}} \cdot \mathbf{c}_{\text{marmellata}}
= (0.8)(0.7) + (0.7)(-0.1) + (-0.3)(0.2)
= 0.56 - 0.07 - 0.06 = 0.43,
\]

\[
P(+ \mid w=\text{albicocca}, c=\text{marmellata})
= \sigma(0.43) \approx 0.605.
\]

Per una coppia negativa come $(\text{albicocca},\ \text{pizzico})$:

\[
\mathbf{w}_{\text{albicocca}} \cdot \mathbf{c}_{\text{pizzico}}
= (0.8)(0.1) + (0.7)(-0.4) + (-0.3)(-0.2)
= 0.08 - 0.28 + 0.06
= -0.14,
\]

\[
P(+ \mid w=\text{albicocca}, c=\text{pizzico})
= \sigma(-0.14) \approx 0.465.
\]

Questo esempio numerico mostra concretamente come il modello sfrutti i due insiemi di embedding $W$ e $C$ per assegnare una probabilità alla relazione di contesto.
\subsubsection{Algoritmo di apprendimento}
L’algoritmo procede assegnando inizialmente un vettore di embedding \emph{casuale} 
sia per ogni parola come \textbf{target} (matrice $W$) sia per ogni parola come 
\textbf{contesto} (matrice $C$).  
A partire da questi vettori iniziali, l’apprendimento consiste nello spostare 
iterativamente gli embedding in modo che:

\begin{itemize}
    \item la parola target $w$ diventi più simile (dot product maggiore) ai vettori
          delle parole che compaiono realmente nel suo contesto;
    \item la parola target $w$ diventi meno simile (dot product minore) ai vettori
          delle parole che non compaiono nel suo contesto.
\end{itemize}

Per capire come funziona, consideriamo un singolo esempio di training tratto 
dalla frase vista in precedenza:

\begin{quote}
\emph{``\dots limone, un cucchiaio di marmellata di albicocca, un pizzico \dots''}
\end{quote}

Con una finestra di contesto di ampiezza $\pm 2$, otteniamo la parola target 
$w=\text{albicocca}$ e le $L=4$ parole di contesto:

\[
c_1 = \text{un},\qquad 
c_2 = \text{cucchiaio},\qquad
c_3 = \text{marmellata},\qquad
c_4 = \text{di}.
\]

Ciascuna coppia $(w, c_i)$ costituisce un \textbf{esempio positivo}.  
Per addestrare un classificatore binario, però, servono anche esempi negativi.  
Lo skip-gram con negative sampling (SGNS) genera per ogni esempio positivo 
$(w, c_{pos})$ un certo numero $k$ di esempi negativi scegliendo parole casuali 
dal vocabolario (dette \textit{noise words}), che non devono essere il target $w$.  
Ad esempio, con $k=2$:

\[
(\text{albicocca}, \text{marmellata}) 
\Rightarrow
\text{negativi: } (\text{albicocca}, \text{aardvark}),\; 
(\text{albicocca}, \text{seven})
\]

Le noise words non vengono scelte in modo uniforme, ma seguono una distribuzione 
\textbf{pesata}:

\[
P_\alpha(w)=\frac{\text{count}(w)^\alpha}{\sum_{w'} \text{count}(w')^\alpha},
\]

dove tipicamente $\alpha = 0.75$.  
Questa scelta aumenta leggermente la probabilità delle parole rare, migliorando 
le prestazioni del modello.

\medskip
\noindent
Dato un esempio positivo $(w,c_{pos})$ e i corrispondenti $k$ esempi negativi 
$c_{neg1},\ldots,c_{negk}$, lo skip-gram definisce la seguente funzione di perdita:

\begin{equation}
L = - \log \sigma(c_{pos} \cdot w)
    - \sum_{i=1}^k \log \sigma(-c_{neg_i} \cdot w),
\end{equation}

che esprime il desiderio di:
\begin{itemize}
    \item massimizzare il dot product tra $w$ e $c_{pos}$;
    \item minimizzare il dot product tra $w$ e i $k$ contesti negativi.
\end{itemize}

L’ottimizzazione avviene tramite \textbf{stochastic gradient descent (SGD)}:  
per ogni esempio si aggiornano sia il vettore target $w$ nella matrice $W$, 
sia i vettori di contesto in $C$, avvicinando il target ai contesti corretti e 
allontanandolo dai contesti negativi.

\medskip
\noindent
Ricordiamo che il modello mantiene due embedding distinti per ogni parola $i$:
\[
w_i \in W \qquad\text{(embedding target)}, 
\]
\[
c_i \in C \qquad\text{(embedding contesto)}.
\]
Al termine dell’addestramento, gli embedding finali possono essere ottenuti usando 
solo $W$ oppure combinando i due vettori, ad esempio con $w_i + c_i$.

Come nei metodi basati sui conteggi (ad es. tf-idf), la dimensione della finestra 
di contesto $L$ influenza la qualità degli embedding e viene spesso ottimizzata su 
un insieme di validazione.

\subsection{Proprietà semantiche degli embeddings}

Gli embeddings catturano diversi tipi di informazione semantica e sintattica.  
In questa sezione riassumiamo in modo schematico le proprietà più rilevanti.

\subsubsection{1. Influenza della finestra di contesto}

La dimensione della finestra di contesto ($L = 2k$) determina il tipo di similarità appresa:

\begin{itemize}
    \item \textbf{Finestra piccola} ($\pm 1$ o $\pm 2$)  
    $\rightarrow$ similarità più \textbf{syntactic-like}: parole con stesso ruolo grammaticale.  
    Esempio: \textit{scrive} $\approx$ \textit{dice}, \textit{risponde}.
    
    \item \textbf{Finestra ampia} ($\pm 5$ o più)  
    $\rightarrow$ similarità più \textbf{topical-like}: parole dello stesso ambito tematico.  

    Esempio: \textit{ospedale} $\approx$ \textit{ambulanze}, \textit{infermieri}.
\end{itemize}

\medskip

\subsubsection{2. First-order vs. second-order similarity}

\begin{itemize}
    \item \textbf{First-order co-occurrence} (associazione sintagmatica):  
    due parole compaiono vicine nel testo.  
    Esempio: \textit{scrisse}–\textit{libro}, \textit{poema}.
    
    \item \textbf{Second-order co-occurrence} (associazione paradigmatica):  
    due parole hanno contesti simili, anche se non compaiono mai insieme.  
    Esempio: \textit{scrisse}–\textit{disse}–\textit{osservò}.
\end{itemize}

Gli embeddings dense (come word2vec) catturano soprattutto la second-order similarity.

\medskip

\subsubsection{3. Analogical reasoning (modello del parallelogramma)}

Gli embeddings rappresentano non solo il significato singolo, ma anche le \textbf{relazioni} tra parole.

Il principio è:

\[
b^{*} \approx b - a + a^{*}
\]

Esempi classici:

\[
\text{king} - \text{man} + \text{woman} \approx \text{queen}
\]

\[
\text{Paris} - \text{France} + \text{Italy} \approx \text{Rome}
\]

\begin{itemize}
    \item Funziona bene per relazioni frequenti e regolari:
    capitali, genere grammaticale, flessione morfologica.
    \item Meno efficace per relazioni astratte o parole rare.
\end{itemize}

\medskip

\subsubsection{4. Struttura geometrica: ortogonalità e parallelismo}

Nel loro spazio vettoriale, gli embeddings possono essere interpretati geometricamente:

\begin{itemize}
    \item \textbf{Parallelismo}: vettori paralleli indicano relazioni analoghe  
    (es. direzione “maschio→femmina” simile per più parole).
    
    \item \textbf{Ortogonalità}: vettori ortogonali indicano concetti indipendenti  
    (similarità coseno $= 0$).
    
    \item \textbf{Direzione}: direzioni dello spazio codificano proprietà semantiche  
    (genere, tempo verbale, grado comparativo).
\end{itemize}

\medskip

\subsubsection{5. Effetti pratici}

\begin{itemize}
    \item Gli embeddings incorporano automaticamente informazione sintattica e semantica.
    \item Le relazioni emergono senza supervisione (self-supervised learning).
    \item La scelta di finestra, dimensione del vettore e algoritmo influenza fortemente il risultato.
\end{itemize}



\subsection{Embeddings as the input to neural net classifiers}
Un tool 