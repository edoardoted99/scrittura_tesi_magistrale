\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {3}Word Embeddings}{37}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Dalla semantica alla rappresentazione vettoriale}{38}{subsection.3.1}\protected@file@percent }
\newlabel{sec:semantics_to_vectors}{{3.1}{38}{Dalla semantica alla rappresentazione vettoriale}{subsection.3.1}{}}
\newlabel{sec:semantics_to_vectors@cref}{{[subsection][1][3]3.1}{[1][38][]38}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Simboli e significati}{38}{subsubsection.3.1.1}\protected@file@percent }
\newlabel{subsubsec:symbols_meanings}{{3.1.1}{38}{Simboli e significati}{subsubsection.3.1.1}{}}
\newlabel{subsubsec:symbols_meanings@cref}{{[subsubsection][1][3,1]3.1.1}{[1][38][]38}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Gli assi del linguaggio}{38}{subsubsection.3.1.2}\protected@file@percent }
\newlabel{subsubsec:linguistic_axes}{{3.1.2}{38}{Gli assi del linguaggio}{subsubsection.3.1.2}{}}
\newlabel{subsubsec:linguistic_axes@cref}{{[subsubsection][2][3,1]3.1.2}{[1][38][]38}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Rappresentazione del mapping tra lo spazio discreto dei simboli (Significanti) e lo spazio continuo dei vettori (Significati). L'obiettivo è apprendere una funzione $f$ tale che simboli diversi con significati simili vengano proiettati in vettori vicini nello spazio matematico.\relax }}{39}{figure.caption.15}\protected@file@percent }
\newlabel{fig:signifier_signified_mapping}{{10}{39}{Rappresentazione del mapping tra lo spazio discreto dei simboli (Significanti) e lo spazio continuo dei vettori (Significati). L'obiettivo è apprendere una funzione $f$ tale che simboli diversi con significati simili vengano proiettati in vettori vicini nello spazio matematico.\relax }{figure.caption.15}{}}
\newlabel{fig:signifier_signified_mapping@cref}{{[figure][10][]10}{[1][38][]39}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}L'ipotesi distribuzionale}{39}{subsubsection.3.1.3}\protected@file@percent }
\newlabel{subsubsec:distributional_hypothesis}{{3.1.3}{39}{L'ipotesi distribuzionale}{subsubsection.3.1.3}{}}
\newlabel{subsubsec:distributional_hypothesis@cref}{{[subsubsection][3][3,1]3.1.3}{[1][39][]39}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Ferdinand de Saussure, fondatore della linguistica strutturale e teorico dei rapporti sintagmatici e associativi (paradigmatici) del linguaggio \blx@tocontentsinit {0}\cite {saussure_wikipedia_image}.\relax }}{40}{figure.caption.16}\protected@file@percent }
\newlabel{fig:saussure}{{11}{40}{Ferdinand de Saussure, fondatore della linguistica strutturale e teorico dei rapporti sintagmatici e associativi (paradigmatici) del linguaggio \cite {saussure_wikipedia_image}.\relax }{figure.caption.16}{}}
\newlabel{fig:saussure@cref}{{[figure][11][]11}{[1][38][]40}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Rappresentazione degli assi del linguaggio. L'asse orizzontale mostra la sequenza lineare (sintagma), quello verticale le alternative possibili (paradigma).\relax }}{41}{figure.caption.17}\protected@file@percent }
\newlabel{fig:linguistic_axes_final}{{12}{41}{Rappresentazione degli assi del linguaggio. L'asse orizzontale mostra la sequenza lineare (sintagma), quello verticale le alternative possibili (paradigma).\relax }{figure.caption.17}{}}
\newlabel{fig:linguistic_axes_final@cref}{{[figure][12][]12}{[1][38][]41}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Ipotesi di Osgood: il significato come vettore}{42}{subsubsection.3.1.4}\protected@file@percent }
\newlabel{subsubsec:osgood_hypothesis}{{3.1.4}{42}{Ipotesi di Osgood: il significato come vettore}{subsubsection.3.1.4}{}}
\newlabel{subsubsec:osgood_hypothesis@cref}{{[subsubsection][4][3,1]3.1.4}{[1][42][]42}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.5}Verso i word embeddings}{43}{subsubsection.3.1.5}\protected@file@percent }
\newlabel{subsubsec:towards_embeddings}{{3.1.5}{43}{Verso i word embeddings}{subsubsection.3.1.5}{}}
\newlabel{subsubsec:towards_embeddings@cref}{{[subsubsection][5][3,1]3.1.5}{[1][42][]43}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Convergenza delle due ipotesi fondamentali negli embeddings moderni. L'ipotesi di Osgood fornisce il \emph  {formato} della rappresentazione (vettori numerici), mentre l'ipotesi distribuzionale indica \emph  {cosa} deve essere catturato (relazioni contestuali tra parole).\relax }}{43}{figure.caption.18}\protected@file@percent }
\newlabel{fig:convergenza_ipotesi}{{13}{43}{Convergenza delle due ipotesi fondamentali negli embeddings moderni. L'ipotesi di Osgood fornisce il \emph {formato} della rappresentazione (vettori numerici), mentre l'ipotesi distribuzionale indica \emph {cosa} deve essere catturato (relazioni contestuali tra parole).\relax }{figure.caption.18}{}}
\newlabel{fig:convergenza_ipotesi@cref}{{[figure][13][]13}{[1][42][]43}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Classificazione delle principali tipologie di embeddings trattate nel capitolo.\relax }}{44}{figure.caption.19}\protected@file@percent }
\newlabel{fig:classificazione_embeddings}{{14}{44}{Classificazione delle principali tipologie di embeddings trattate nel capitolo.\relax }{figure.caption.19}{}}
\newlabel{fig:classificazione_embeddings@cref}{{[figure][14][]14}{[1][44][]44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Embeddings statici}{45}{subsection.3.2}\protected@file@percent }
\newlabel{sec:static_embeddings}{{3.2}{45}{Embeddings statici}{subsection.3.2}{}}
\newlabel{sec:static_embeddings@cref}{{[subsection][2][3]3.2}{[1][44][]45}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Embeddings count-based}{45}{subsubsection.3.2.1}\protected@file@percent }
\newlabel{subsubsec:count_based}{{3.2.1}{45}{Embeddings count-based}{subsubsection.3.2.1}{}}
\newlabel{subsubsec:count_based@cref}{{[subsubsection][1][3,2]3.2.1}{[1][45][]45}}
\@writefile{toc}{\contentsline {paragraph}{Matrice termine-documento.}{45}{section*.20}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Term-document matrix per quattro parole in quattro opere di Shakespeare. Ogni cella contiene il numero di occorrenze della parola (riga) nel documento (colonna).\relax }}{45}{table.caption.21}\protected@file@percent }
\newlabel{tab:term_document_shakespeare}{{2}{45}{Term-document matrix per quattro parole in quattro opere di Shakespeare. Ogni cella contiene il numero di occorrenze della parola (riga) nel documento (colonna).\relax }{table.caption.21}{}}
\newlabel{tab:term_document_shakespeare@cref}{{[table][2][]2}{[1][45][]45}}
\@writefile{toc}{\contentsline {paragraph}{Matrice termine-termine.}{46}{section*.22}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Estratto di una matrice termine--termine calcolata sul corpus Wikipedia. Ogni cella contiene il numero di co-occorrenze tra la parola target (riga) e la parola di contesto (colonna) all'interno di una finestra di contesto locale.\relax }}{46}{table.caption.23}\protected@file@percent }
\newlabel{tab:term_term_wikipedia}{{3}{46}{Estratto di una matrice termine--termine calcolata sul corpus Wikipedia. Ogni cella contiene il numero di co-occorrenze tra la parola target (riga) e la parola di contesto (colonna) all'interno di una finestra di contesto locale.\relax }{table.caption.23}{}}
\newlabel{tab:term_term_wikipedia@cref}{{[table][3][]3}{[1][46][]46}}
\@writefile{toc}{\contentsline {paragraph}{Riduzione dimensionale tramite SVD.}{47}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cosine similarity.}{48}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Word2Vec: un approccio predittivo}{48}{subsubsection.3.2.2}\protected@file@percent }
\newlabel{subsubsec:word2vec}{{3.2.2}{48}{Word2Vec: un approccio predittivo}{subsubsection.3.2.2}{}}
\newlabel{subsubsec:word2vec@cref}{{[subsubsection][2][3,2]3.2.2}{[1][48][]48}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Word2Vec come fusione tra teoria e implementazione. A sinistra, l'ipotesi di Osgood si traduce nella scelta di vettori continui e nel prodotto scalare come misura di similarità. A destra, l'ipotesi distribuzionale si concretizza nella finestra di contesto e nella modellazione di co-occorrenze. Word2Vec unifica elegantemente queste due linee, apprendendo vettori che massimizzano la predizione di co-occorrenze locali.\relax }}{48}{figure.caption.26}\protected@file@percent }
\newlabel{fig:word2vec_teoria_implementazione}{{15}{48}{Word2Vec come fusione tra teoria e implementazione. A sinistra, l'ipotesi di Osgood si traduce nella scelta di vettori continui e nel prodotto scalare come misura di similarità. A destra, l'ipotesi distribuzionale si concretizza nella finestra di contesto e nella modellazione di co-occorrenze. Word2Vec unifica elegantemente queste due linee, apprendendo vettori che massimizzano la predizione di co-occorrenze locali.\relax }{figure.caption.26}{}}
\newlabel{fig:word2vec_teoria_implementazione@cref}{{[figure][15][]15}{[1][48][]48}}
\@writefile{toc}{\contentsline {paragraph}{Il classificatore e la funzione sigmoide.}{49}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Perché due matrici? Il ruolo di $W$ e $C$.}{50}{section*.28}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Lo skip-gram apprende in totale due insiemi di embedding, uno per i target ($W$) e uno per i contesti ($C$), per un totale di $2|V|$ vettori. L'addestramento mira a massimizzare la probabilità che parole vicine nel testo abbiano vettori simili.\relax }}{50}{figure.caption.29}\protected@file@percent }
\newlabel{fig:skipgram_structure}{{16}{50}{Lo skip-gram apprende in totale due insiemi di embedding, uno per i target ($W$) e uno per i contesti ($C$), per un totale di $2|V|$ vettori. L'addestramento mira a massimizzare la probabilità che parole vicine nel testo abbiano vettori simili.\relax }{figure.caption.29}{}}
\newlabel{fig:skipgram_structure@cref}{{[figure][16][]16}{[1][50][]50}}
\@writefile{toc}{\contentsline {paragraph}{Finestra di contesto e precisione semantica.}{50}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Geometria dell'analogia: il modello del parallelogramma.}{51}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Limiti degli embeddings statici}{51}{subsubsection.3.2.3}\protected@file@percent }
\newlabel{subsubsec:static_limits}{{3.2.3}{51}{Limiti degli embeddings statici}{subsubsection.3.2.3}{}}
\newlabel{subsubsec:static_limits@cref}{{[subsubsection][3][3,2]3.2.3}{[1][51][]51}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Rappresentazione geometrica del modello del parallelogramma nello spazio latente.\relax }}{52}{figure.caption.32}\protected@file@percent }
\newlabel{fig:parallelogramma}{{17}{52}{Rappresentazione geometrica del modello del parallelogramma nello spazio latente.\relax }{figure.caption.32}{}}
\newlabel{fig:parallelogramma@cref}{{[figure][17][]17}{[1][51][]52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Embeddings dinamici (contestuali)}{53}{subsection.3.3}\protected@file@percent }
\newlabel{sec:dynamic_embeddings}{{3.3}{53}{Embeddings dinamici (contestuali)}{subsection.3.3}{}}
\newlabel{sec:dynamic_embeddings@cref}{{[subsection][3][3]3.3}{[1][53][]53}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}RNN: memoria sequenziale}{54}{subsubsection.3.3.1}\protected@file@percent }
\newlabel{subsubsec:rnn}{{3.3.1}{54}{RNN: memoria sequenziale}{subsubsection.3.3.1}{}}
\newlabel{subsubsec:rnn@cref}{{[subsubsection][1][3,3]3.3.1}{[1][54][]54}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Flusso di informazione in una RNN. Ogni stato nascosto $h_t$ integra l'input corrente $x_t$ con la memoria accumulata nello stato precedente $h_{t-1}$.\relax }}{55}{figure.caption.33}\protected@file@percent }
\newlabel{fig:rnn_flow}{{18}{55}{Flusso di informazione in una RNN. Ogni stato nascosto $h_t$ integra l'input corrente $x_t$ con la memoria accumulata nello stato precedente $h_{t-1}$.\relax }{figure.caption.33}{}}
\newlabel{fig:rnn_flow@cref}{{[figure][18][]18}{[1][55][]55}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}LSTM: controllare il flusso di informazione}{56}{subsubsection.3.3.2}\protected@file@percent }
\newlabel{subsubsec:lstm}{{3.3.2}{56}{LSTM: controllare il flusso di informazione}{subsubsection.3.3.2}{}}
\newlabel{subsubsec:lstm@cref}{{[subsubsection][2][3,3]3.3.2}{[1][56][]56}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Schema semplificato di una cella LSTM. Il cell state (linea orizzontale) attraversa la cella subendo modifiche controllate dai tre gate, che decidono rispettivamente cosa dimenticare, cosa aggiungere, e cosa esporre come output.\relax }}{57}{figure.caption.34}\protected@file@percent }
\newlabel{fig:lstm_gates}{{19}{57}{Schema semplificato di una cella LSTM. Il cell state (linea orizzontale) attraversa la cella subendo modifiche controllate dai tre gate, che decidono rispettivamente cosa dimenticare, cosa aggiungere, e cosa esporre come output.\relax }{figure.caption.34}{}}
\newlabel{fig:lstm_gates@cref}{{[figure][19][]19}{[1][57][]57}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Bidirezionalità}{58}{subsubsection.3.3.3}\protected@file@percent }
\newlabel{subsubsec:bidirectional}{{3.3.3}{58}{Bidirezionalità}{subsubsection.3.3.3}{}}
\newlabel{subsubsec:bidirectional@cref}{{[subsubsection][3][3,3]3.3.3}{[1][58][]58}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Architettura Bi-LSTM. La sequenza viene processata in entrambe le direzioni. L'embedding finale di ogni token è la concatenazione degli stati forward e backward, integrando così il contesto completo.\relax }}{59}{figure.caption.35}\protected@file@percent }
\newlabel{fig:bilstm}{{20}{59}{Architettura Bi-LSTM. La sequenza viene processata in entrambe le direzioni. L'embedding finale di ogni token è la concatenazione degli stati forward e backward, integrando così il contesto completo.\relax }{figure.caption.35}{}}
\newlabel{fig:bilstm@cref}{{[figure][20][]20}{[1][59][]59}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Encoder-Decoder: trasformare sequenze}{60}{subsubsection.3.3.4}\protected@file@percent }
\newlabel{subsubsec:encoder_decoder}{{3.3.4}{60}{Encoder-Decoder: trasformare sequenze}{subsubsection.3.3.4}{}}
\newlabel{subsubsec:encoder_decoder@cref}{{[subsubsection][4][3,3]3.3.4}{[1][60][]60}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Architettura Encoder-Decoder. L'encoder comprime la sequenza sorgente nel context vector $c$. Il decoder genera la sequenza target condizionato su $c$, un token alla volta. Tutta l'informazione deve passare attraverso $c$: questo è il bottleneck.\relax }}{61}{figure.caption.36}\protected@file@percent }
\newlabel{fig:encoder_decoder}{{21}{61}{Architettura Encoder-Decoder. L'encoder comprime la sequenza sorgente nel context vector $c$. Il decoder genera la sequenza target condizionato su $c$, un token alla volta. Tutta l'informazione deve passare attraverso $c$: questo è il bottleneck.\relax }{figure.caption.36}{}}
\newlabel{fig:encoder_decoder@cref}{{[figure][21][]21}{[1][61][]61}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.5}Attenzione: superare il bottleneck}{62}{subsubsection.3.3.5}\protected@file@percent }
\newlabel{subsubsec:attention}{{3.3.5}{62}{Attenzione: superare il bottleneck}{subsubsection.3.3.5}{}}
\newlabel{subsubsec:attention@cref}{{[subsubsection][5][3,3]3.3.5}{[1][62][]62}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Meccanismo di attenzione. Per generare ``gatto'', il decoder interroga tutti gli stati dell'encoder. I pesi $\alpha _{ij}$ determinano quanto ogni posizione contribuisce al context vector $c_i$. In questo esempio, l'attenzione si concentra su $h_2$ (``cat''), che contribuisce per l'80\%.\relax }}{64}{figure.caption.37}\protected@file@percent }
\newlabel{fig:attention_mechanism}{{22}{64}{Meccanismo di attenzione. Per generare ``gatto'', il decoder interroga tutti gli stati dell'encoder. I pesi $\alpha _{ij}$ determinano quanto ogni posizione contribuisce al context vector $c_i$. In questo esempio, l'attenzione si concentra su $h_2$ (``cat''), che contribuisce per l'80\%.\relax }{figure.caption.37}{}}
\newlabel{fig:attention_mechanism@cref}{{[figure][22][]22}{[1][63][]64}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.6}Transformer: attenzione senza ricorrenza}{64}{subsubsection.3.3.6}\protected@file@percent }
\newlabel{subsubsec:transformer}{{3.3.6}{64}{Transformer: attenzione senza ricorrenza}{subsubsection.3.3.6}{}}
\newlabel{subsubsec:transformer@cref}{{[subsubsection][6][3,3]3.3.6}{[1][64][]64}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Self-attention: il token ``dorme'' assegna pesi di attenzione a tutte le posizioni della propria sequenza. Il peso maggiore va a ``gatto'' (linea spessa), catturando la dipendenza soggetto-verbo nonostante la distanza.\relax }}{66}{figure.caption.38}\protected@file@percent }
\newlabel{fig:self_attention}{{23}{66}{Self-attention: il token ``dorme'' assegna pesi di attenzione a tutte le posizioni della propria sequenza. Il peso maggiore va a ``gatto'' (linea spessa), catturando la dipendenza soggetto-verbo nonostante la distanza.\relax }{figure.caption.38}{}}
\newlabel{fig:self_attention@cref}{{[figure][23][]23}{[1][65][]66}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.7}BERT: codifica bidirezionale}{67}{subsubsection.3.3.7}\protected@file@percent }
\newlabel{subsubsec:bert}{{3.3.7}{67}{BERT: codifica bidirezionale}{subsubsection.3.3.7}{}}
\newlabel{subsubsec:bert@cref}{{[subsubsection][7][3,3]3.3.7}{[1][66][]67}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Masked Language Modeling: il token mascherato viene predetto integrando informazione da tutto il contesto circostante—sia sinistro che destro.\relax }}{68}{figure.caption.39}\protected@file@percent }
\newlabel{fig:mlm}{{24}{68}{Masked Language Modeling: il token mascherato viene predetto integrando informazione da tutto il contesto circostante—sia sinistro che destro.\relax }{figure.caption.39}{}}
\newlabel{fig:mlm@cref}{{[figure][24][]24}{[1][67][]68}}
\@writefile{toc}{\contentsline {paragraph}{Pre-training e fine-tuning.}{68}{section*.40}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Il problema dell'opacità.}{68}{section*.41}\protected@file@percent }
\@setckpt{chapters/03_embeddings}{
\setcounter{page}{70}
\setcounter{equation}{28}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{subsubsection}{7}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{24}
\setcounter{table}{3}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{lstnumber}{1}
\setcounter{parentequation}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{55}
\setcounter{maxnames}{1}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{blx@maxsegment@0}{0}
\setcounter{blx@sectionciteorder@0}{54}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{Item}{75}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{58}
\setcounter{FancyVerbLine}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{24}
\setcounter{definition}{0}
\setcounter{lstlisting}{0}
\setcounter{section@level}{4}
}
