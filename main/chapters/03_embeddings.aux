\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {3}Word Embeddings}{25}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Dalla semantica alla rappresentazione vettoriale}{26}{subsection.3.1}\protected@file@percent }
\newlabel{sec:semantics_to_vectors}{{3.1}{26}{Dalla semantica alla rappresentazione vettoriale}{subsection.3.1}{}}
\newlabel{sec:semantics_to_vectors@cref}{{[subsection][1][3]3.1}{[1][26][]26}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Simboli e significati}{26}{subsubsection.3.1.1}\protected@file@percent }
\newlabel{subsubsec:symbols_meanings}{{3.1.1}{26}{Simboli e significati}{subsubsection.3.1.1}{}}
\newlabel{subsubsec:symbols_meanings@cref}{{[subsubsection][1][3,1]3.1.1}{[1][26][]26}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Gli assi del linguaggio}{26}{subsubsection.3.1.2}\protected@file@percent }
\newlabel{subsubsec:linguistic_axes}{{3.1.2}{26}{Gli assi del linguaggio}{subsubsection.3.1.2}{}}
\newlabel{subsubsec:linguistic_axes@cref}{{[subsubsection][2][3,1]3.1.2}{[1][26][]26}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Rappresentazione del mapping tra lo spazio discreto dei simboli (Significanti) e lo spazio continuo dei vettori (Significati). L'obiettivo è apprendere una funzione $f$ tale che simboli diversi con significati simili vengano proiettati in vettori vicini nello spazio matematico.\relax }}{27}{figure.caption.13}\protected@file@percent }
\newlabel{fig:signifier_signified_mapping}{{10}{27}{Rappresentazione del mapping tra lo spazio discreto dei simboli (Significanti) e lo spazio continuo dei vettori (Significati). L'obiettivo è apprendere una funzione $f$ tale che simboli diversi con significati simili vengano proiettati in vettori vicini nello spazio matematico.\relax }{figure.caption.13}{}}
\newlabel{fig:signifier_signified_mapping@cref}{{[figure][10][]10}{[1][26][]27}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Rappresentazione degli assi del linguaggio. L'asse orizzontale mostra la sequenza lineare (sintagma), quello verticale le alternative possibili (paradigma).\relax }}{27}{figure.caption.14}\protected@file@percent }
\newlabel{fig:linguistic_axes}{{11}{27}{Rappresentazione degli assi del linguaggio. L'asse orizzontale mostra la sequenza lineare (sintagma), quello verticale le alternative possibili (paradigma).\relax }{figure.caption.14}{}}
\newlabel{fig:linguistic_axes@cref}{{[figure][11][]11}{[1][26][]27}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}L'ipotesi distribuzionale}{28}{subsubsection.3.1.3}\protected@file@percent }
\newlabel{subsubsec:distributional_hypothesis}{{3.1.3}{28}{L'ipotesi distribuzionale}{subsubsection.3.1.3}{}}
\newlabel{subsubsec:distributional_hypothesis@cref}{{[subsubsection][3][3,1]3.1.3}{[1][28][]28}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Ipotesi di Osgood: il significato come vettore}{29}{subsubsection.3.1.4}\protected@file@percent }
\newlabel{subsubsec:osgood_hypothesis}{{3.1.4}{29}{Ipotesi di Osgood: il significato come vettore}{subsubsection.3.1.4}{}}
\newlabel{subsubsec:osgood_hypothesis@cref}{{[subsubsection][4][3,1]3.1.4}{[1][29][]29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.5}Verso i word embeddings}{30}{subsubsection.3.1.5}\protected@file@percent }
\newlabel{subsubsec:towards_embeddings}{{3.1.5}{30}{Verso i word embeddings}{subsubsection.3.1.5}{}}
\newlabel{subsubsec:towards_embeddings@cref}{{[subsubsection][5][3,1]3.1.5}{[1][30][]30}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Classificazione delle principali tipologie di embeddings trattate nel capitolo.\relax }}{31}{figure.caption.15}\protected@file@percent }
\newlabel{fig:classificazione_embeddings}{{12}{31}{Classificazione delle principali tipologie di embeddings trattate nel capitolo.\relax }{figure.caption.15}{}}
\newlabel{fig:classificazione_embeddings@cref}{{[figure][12][]12}{[1][31][]31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Embeddings statici}{31}{subsection.3.2}\protected@file@percent }
\newlabel{sec:static_embeddings}{{3.2}{31}{Embeddings statici}{subsection.3.2}{}}
\newlabel{sec:static_embeddings@cref}{{[subsection][2][3]3.2}{[1][31][]31}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Embeddings count-based}{32}{subsubsection.3.2.1}\protected@file@percent }
\newlabel{subsubsec:count_based}{{3.2.1}{32}{Embeddings count-based}{subsubsection.3.2.1}{}}
\newlabel{subsubsec:count_based@cref}{{[subsubsection][1][3,2]3.2.1}{[1][32][]32}}
\@writefile{toc}{\contentsline {paragraph}{Matrice termine-documento.}{32}{section*.16}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Term-document matrix per quattro parole in quattro opere di Shakespeare. Ogni cella contiene il numero di occorrenze della parola (riga) nel documento (colonna).\relax }}{32}{table.caption.17}\protected@file@percent }
\newlabel{tab:term_document_shakespeare}{{1}{32}{Term-document matrix per quattro parole in quattro opere di Shakespeare. Ogni cella contiene il numero di occorrenze della parola (riga) nel documento (colonna).\relax }{table.caption.17}{}}
\newlabel{tab:term_document_shakespeare@cref}{{[table][1][]1}{[1][32][]32}}
\@writefile{toc}{\contentsline {paragraph}{Matrice termine-termine.}{33}{section*.18}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Estratto di una matrice termine--termine calcolata sul corpus Wikipedia. Ogni cella contiene il numero di co-occorrenze tra la parola target (riga) e la parola di contesto (colonna) all'interno di una finestra di contesto locale.\relax }}{33}{table.caption.19}\protected@file@percent }
\newlabel{tab:term_term_wikipedia}{{2}{33}{Estratto di una matrice termine--termine calcolata sul corpus Wikipedia. Ogni cella contiene il numero di co-occorrenze tra la parola target (riga) e la parola di contesto (colonna) all'interno di una finestra di contesto locale.\relax }{table.caption.19}{}}
\newlabel{tab:term_term_wikipedia@cref}{{[table][2][]2}{[1][33][]33}}
\@writefile{toc}{\contentsline {paragraph}{Riduzione dimensionale tramite SVD.}{34}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cosine similarity.}{35}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Word2Vec: un approccio predittivo}{35}{subsubsection.3.2.2}\protected@file@percent }
\newlabel{subsubsec:word2vec}{{3.2.2}{35}{Word2Vec: un approccio predittivo}{subsubsection.3.2.2}{}}
\newlabel{subsubsec:word2vec@cref}{{[subsubsection][2][3,2]3.2.2}{[1][35][]35}}
\@writefile{toc}{\contentsline {paragraph}{Il classificatore e la funzione sigmoide.}{36}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Perché due matrici? Il ruolo di $W$ e $C$.}{36}{section*.23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Lo skip-gram apprende in totale due insiemi di embedding, uno per i target ($W$) e uno per i contesti ($C$), per un totale di $2|V|$ vettori. L'addestramento mira a massimizzare la probabilità che parole vicine nel testo abbiano vettori simili.\relax }}{37}{figure.caption.24}\protected@file@percent }
\newlabel{fig:skipgram_structure}{{13}{37}{Lo skip-gram apprende in totale due insiemi di embedding, uno per i target ($W$) e uno per i contesti ($C$), per un totale di $2|V|$ vettori. L'addestramento mira a massimizzare la probabilità che parole vicine nel testo abbiano vettori simili.\relax }{figure.caption.24}{}}
\newlabel{fig:skipgram_structure@cref}{{[figure][13][]13}{[1][36][]37}}
\@writefile{toc}{\contentsline {paragraph}{Finestra di contesto e precisione semantica.}{37}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Geometria dell'analogia: il modello del parallelogramma.}{37}{section*.26}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Rappresentazione geometrica del modello del parallelogramma nello spazio latente.\relax }}{38}{figure.caption.27}\protected@file@percent }
\newlabel{fig:parallelogramma}{{14}{38}{Rappresentazione geometrica del modello del parallelogramma nello spazio latente.\relax }{figure.caption.27}{}}
\newlabel{fig:parallelogramma@cref}{{[figure][14][]14}{[1][37][]38}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Limiti degli embeddings statici}{38}{subsubsection.3.2.3}\protected@file@percent }
\newlabel{subsubsec:static_limits}{{3.2.3}{38}{Limiti degli embeddings statici}{subsubsection.3.2.3}{}}
\newlabel{subsubsec:static_limits@cref}{{[subsubsection][3][3,2]3.2.3}{[1][38][]38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Embeddings dinamici (contestuali)}{39}{subsection.3.3}\protected@file@percent }
\newlabel{sec:dynamic_embeddings}{{3.3}{39}{Embeddings dinamici (contestuali)}{subsection.3.3}{}}
\newlabel{sec:dynamic_embeddings@cref}{{[subsection][3][3]3.3}{[1][39][]39}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}RNN e LSTM: memoria sequenziale}{40}{subsubsection.3.3.1}\protected@file@percent }
\newlabel{subsubsec:rnn_lstm_compendium}{{3.3.1}{40}{RNN e LSTM: memoria sequenziale}{subsubsection.3.3.1}{}}
\newlabel{subsubsec:rnn_lstm_compendium@cref}{{[subsubsection][1][3,3]3.3.1}{[1][40][]40}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Illustrazione del funzionamento di una RNN. Ad ogni passo l'informazione contestuale viene passata a quello successivo attraverso lo stato nascosto $h_t$.\relax }}{41}{figure.caption.28}\protected@file@percent }
\newlabel{fig:rnn_flow}{{15}{41}{Illustrazione del funzionamento di una RNN. Ad ogni passo l'informazione contestuale viene passata a quello successivo attraverso lo stato nascosto $h_t$.\relax }{figure.caption.28}{}}
\newlabel{fig:rnn_flow@cref}{{[figure][15][]15}{[1][41][]41}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Encoder-Decoder e il problema del bottleneck}{42}{subsubsection.3.3.2}\protected@file@percent }
\newlabel{subsubsec:encoder_decoder_bottleneck}{{3.3.2}{42}{Encoder-Decoder e il problema del bottleneck}{subsubsection.3.3.2}{}}
\newlabel{subsubsec:encoder_decoder_bottleneck@cref}{{[subsubsection][2][3,3]3.3.2}{[1][42][]42}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Schema Encoder--Decoder: quando il contesto $c$ coincide con il solo stato nascosto finale dell'encoder, tutta l'informazione della sequenza sorgente deve attraversare un \emph  {collo di bottiglia} rappresentazionale prima di essere utilizzata dal decoder.\relax }}{43}{figure.caption.29}\protected@file@percent }
\newlabel{fig:encoder_decoder_bottleneck}{{16}{43}{Schema Encoder--Decoder: quando il contesto $c$ coincide con il solo stato nascosto finale dell'encoder, tutta l'informazione della sequenza sorgente deve attraversare un \emph {collo di bottiglia} rappresentazionale prima di essere utilizzata dal decoder.\relax }{figure.caption.29}{}}
\newlabel{fig:encoder_decoder_bottleneck@cref}{{[figure][16][]16}{[1][43][]43}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Meccanismo di attenzione: superare il bottleneck}{44}{subsubsection.3.3.3}\protected@file@percent }
\newlabel{subsubsec:attention_mechanism}{{3.3.3}{44}{Meccanismo di attenzione: superare il bottleneck}{subsubsection.3.3.3}{}}
\newlabel{subsubsec:attention_mechanism@cref}{{[subsubsection][3][3,3]3.3.3}{[1][44][]44}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Nel meccanismo di attenzione, a ciascun passo di decodifica $i$ il decoder utilizza un contesto \emph  {dinamico} $c_i$ (diverso per ogni token generato), calcolato come funzione di tutti gli stati nascosti dell'encoder.\relax }}{44}{figure.caption.30}\protected@file@percent }
\newlabel{fig:attention_dynamic_context}{{17}{44}{Nel meccanismo di attenzione, a ciascun passo di decodifica $i$ il decoder utilizza un contesto \emph {dinamico} $c_i$ (diverso per ogni token generato), calcolato come funzione di tutti gli stati nascosti dell'encoder.\relax }{figure.caption.30}{}}
\newlabel{fig:attention_dynamic_context@cref}{{[figure][17][]17}{[1][44][]44}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Verso i Transformer}{46}{subsubsection.3.3.4}\protected@file@percent }
\newlabel{subsubsec:towards_transformers}{{3.3.4}{46}{Verso i Transformer}{subsubsection.3.3.4}{}}
\newlabel{subsubsec:towards_transformers@cref}{{[subsubsection][4][3,3]3.3.4}{[1][45][]46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}BERT: embeddings bidirezionali e il problema dell'opacità}{46}{subsection.3.4}\protected@file@percent }
\newlabel{sec:bert}{{3.4}{46}{BERT: embeddings bidirezionali e il problema dell'opacità}{subsection.3.4}{}}
\newlabel{sec:bert@cref}{{[subsection][4][3]3.4}{[1][46][]46}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Il principio di Vapnik: risolvere il problema giusto}{47}{subsubsection.3.4.1}\protected@file@percent }
\newlabel{subsubsec:vapnik_principle}{{3.4.1}{47}{Il principio di Vapnik: risolvere il problema giusto}{subsubsection.3.4.1}{}}
\newlabel{subsubsec:vapnik_principle@cref}{{[subsubsection][1][3,4]3.4.1}{[1][47][]47}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Illustrazione del principio di Vapnik applicato ai modelli linguistici. La generazione autoregressiva è un problema più generale della comprensione contestuale; BERT risolve direttamente il problema più specifico senza passare per quello generale.\relax }}{49}{figure.caption.31}\protected@file@percent }
\newlabel{fig:vapnik_principle}{{18}{49}{Illustrazione del principio di Vapnik applicato ai modelli linguistici. La generazione autoregressiva è un problema più generale della comprensione contestuale; BERT risolve direttamente il problema più specifico senza passare per quello generale.\relax }{figure.caption.31}{}}
\newlabel{fig:vapnik_principle@cref}{{[figure][18][]18}{[1][49][]49}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Architettura di BERT}{49}{subsubsection.3.4.2}\protected@file@percent }
\newlabel{subsubsec:bert_architecture}{{3.4.2}{49}{Architettura di BERT}{subsubsection.3.4.2}{}}
\newlabel{subsubsec:bert_architecture@cref}{{[subsubsection][2][3,4]3.4.2}{[1][49][]49}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Architettura di BERT. L'input viene codificato tramite embeddings (token, posizione, segmento), processato da una pila di Transformer encoder, e produce embeddings contestuali per ogni posizione. Il token \texttt  {[MASK]} viene predetto utilizzando l'intero contesto bidirezionale.\relax }}{50}{figure.caption.32}\protected@file@percent }
\newlabel{fig:bert_architecture}{{19}{50}{Architettura di BERT. L'input viene codificato tramite embeddings (token, posizione, segmento), processato da una pila di Transformer encoder, e produce embeddings contestuali per ogni posizione. Il token \texttt {[MASK]} viene predetto utilizzando l'intero contesto bidirezionale.\relax }{figure.caption.32}{}}
\newlabel{fig:bert_architecture@cref}{{[figure][19][]19}{[1][50][]50}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Composizione dell'input in BERT. Ogni token è rappresentato dalla somma di tre embeddings: token, posizione e segmento.\relax }}{51}{figure.caption.33}\protected@file@percent }
\newlabel{fig:bert_input_representation}{{20}{51}{Composizione dell'input in BERT. Ogni token è rappresentato dalla somma di tre embeddings: token, posizione e segmento.\relax }{figure.caption.33}{}}
\newlabel{fig:bert_input_representation@cref}{{[figure][20][]20}{[1][51][]51}}
\@setckpt{chapters/03_embeddings}{
\setcounter{page}{54}
\setcounter{equation}{31}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{subsubsection}{2}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{20}
\setcounter{table}{2}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{lstnumber}{1}
\setcounter{parentequation}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{0}
\setcounter{maxnames}{1}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{blx@maxsegment@0}{0}
\setcounter{blx@sectionciteorder@0}{19}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{Item}{70}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{48}
\setcounter{FancyVerbLine}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{15}
\setcounter{definition}{0}
\setcounter{lstlisting}{0}
\setcounter{section@level}{3}
}
