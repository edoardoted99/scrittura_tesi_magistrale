\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {3}Word Embeddings}{25}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Dalla semantica alla rappresentazione vettoriale}{26}{subsection.3.1}\protected@file@percent }
\newlabel{sec:semantics_to_vectors}{{3.1}{26}{Dalla semantica alla rappresentazione vettoriale}{subsection.3.1}{}}
\newlabel{sec:semantics_to_vectors@cref}{{[subsection][1][3]3.1}{[1][26][]26}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Simboli e significati}{26}{subsubsection.3.1.1}\protected@file@percent }
\newlabel{subsubsec:symbols_meanings}{{3.1.1}{26}{Simboli e significati}{subsubsection.3.1.1}{}}
\newlabel{subsubsec:symbols_meanings@cref}{{[subsubsection][1][3,1]3.1.1}{[1][26][]26}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Gli assi del linguaggio}{26}{subsubsection.3.1.2}\protected@file@percent }
\newlabel{subsubsec:linguistic_axes}{{3.1.2}{26}{Gli assi del linguaggio}{subsubsection.3.1.2}{}}
\newlabel{subsubsec:linguistic_axes@cref}{{[subsubsection][2][3,1]3.1.2}{[1][26][]26}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Rappresentazione del mapping tra lo spazio discreto dei simboli (Significanti) e lo spazio continuo dei vettori (Significati). L'obiettivo è apprendere una funzione $f$ tale che simboli diversi con significati simili vengano proiettati in vettori vicini nello spazio matematico.\relax }}{27}{figure.caption.13}\protected@file@percent }
\newlabel{fig:signifier_signified_mapping}{{10}{27}{Rappresentazione del mapping tra lo spazio discreto dei simboli (Significanti) e lo spazio continuo dei vettori (Significati). L'obiettivo è apprendere una funzione $f$ tale che simboli diversi con significati simili vengano proiettati in vettori vicini nello spazio matematico.\relax }{figure.caption.13}{}}
\newlabel{fig:signifier_signified_mapping@cref}{{[figure][10][]10}{[1][26][]27}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}L'ipotesi distribuzionale}{27}{subsubsection.3.1.3}\protected@file@percent }
\newlabel{subsubsec:distributional_hypothesis}{{3.1.3}{27}{L'ipotesi distribuzionale}{subsubsection.3.1.3}{}}
\newlabel{subsubsec:distributional_hypothesis@cref}{{[subsubsection][3][3,1]3.1.3}{[1][27][]27}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Ferdinand de Saussure, fondatore della linguistica strutturale e teorico dei rapporti sintagmatici e associativi (paradigmatici) del linguaggio \blx@tocontentsinit {0}\cite {saussure_wikipedia_image}.\relax }}{28}{figure.caption.14}\protected@file@percent }
\newlabel{fig:saussure}{{11}{28}{Ferdinand de Saussure, fondatore della linguistica strutturale e teorico dei rapporti sintagmatici e associativi (paradigmatici) del linguaggio \cite {saussure_wikipedia_image}.\relax }{figure.caption.14}{}}
\newlabel{fig:saussure@cref}{{[figure][11][]11}{[1][26][]28}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Rappresentazione degli assi del linguaggio. L'asse orizzontale mostra la sequenza lineare (sintagma), quello verticale le alternative possibili (paradigma).\relax }}{29}{figure.caption.15}\protected@file@percent }
\newlabel{fig:linguistic_axes}{{12}{29}{Rappresentazione degli assi del linguaggio. L'asse orizzontale mostra la sequenza lineare (sintagma), quello verticale le alternative possibili (paradigma).\relax }{figure.caption.15}{}}
\newlabel{fig:linguistic_axes@cref}{{[figure][12][]12}{[1][26][]29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Ipotesi di Osgood: il significato come vettore}{30}{subsubsection.3.1.4}\protected@file@percent }
\newlabel{subsubsec:osgood_hypothesis}{{3.1.4}{30}{Ipotesi di Osgood: il significato come vettore}{subsubsection.3.1.4}{}}
\newlabel{subsubsec:osgood_hypothesis@cref}{{[subsubsection][4][3,1]3.1.4}{[1][29][]30}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.5}Verso i word embeddings}{30}{subsubsection.3.1.5}\protected@file@percent }
\newlabel{subsubsec:towards_embeddings}{{3.1.5}{30}{Verso i word embeddings}{subsubsection.3.1.5}{}}
\newlabel{subsubsec:towards_embeddings@cref}{{[subsubsection][5][3,1]3.1.5}{[1][30][]30}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Convergenza delle due ipotesi fondamentali negli embeddings moderni. L'ipotesi di Osgood fornisce il \emph  {formato} della rappresentazione (vettori numerici), mentre l'ipotesi distribuzionale indica \emph  {cosa} deve essere catturato (relazioni contestuali tra parole).\relax }}{31}{figure.caption.16}\protected@file@percent }
\newlabel{fig:convergenza_ipotesi}{{13}{31}{Convergenza delle due ipotesi fondamentali negli embeddings moderni. L'ipotesi di Osgood fornisce il \emph {formato} della rappresentazione (vettori numerici), mentre l'ipotesi distribuzionale indica \emph {cosa} deve essere catturato (relazioni contestuali tra parole).\relax }{figure.caption.16}{}}
\newlabel{fig:convergenza_ipotesi@cref}{{[figure][13][]13}{[1][30][]31}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Classificazione delle principali tipologie di embeddings trattate nel capitolo.\relax }}{32}{figure.caption.17}\protected@file@percent }
\newlabel{fig:classificazione_embeddings}{{14}{32}{Classificazione delle principali tipologie di embeddings trattate nel capitolo.\relax }{figure.caption.17}{}}
\newlabel{fig:classificazione_embeddings@cref}{{[figure][14][]14}{[1][32][]32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Embeddings statici}{32}{subsection.3.2}\protected@file@percent }
\newlabel{sec:static_embeddings}{{3.2}{32}{Embeddings statici}{subsection.3.2}{}}
\newlabel{sec:static_embeddings@cref}{{[subsection][2][3]3.2}{[1][32][]32}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Embeddings count-based}{33}{subsubsection.3.2.1}\protected@file@percent }
\newlabel{subsubsec:count_based}{{3.2.1}{33}{Embeddings count-based}{subsubsection.3.2.1}{}}
\newlabel{subsubsec:count_based@cref}{{[subsubsection][1][3,2]3.2.1}{[1][33][]33}}
\@writefile{toc}{\contentsline {paragraph}{Matrice termine-documento.}{33}{section*.18}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Term-document matrix per quattro parole in quattro opere di Shakespeare. Ogni cella contiene il numero di occorrenze della parola (riga) nel documento (colonna).\relax }}{33}{table.caption.19}\protected@file@percent }
\newlabel{tab:term_document_shakespeare}{{1}{33}{Term-document matrix per quattro parole in quattro opere di Shakespeare. Ogni cella contiene il numero di occorrenze della parola (riga) nel documento (colonna).\relax }{table.caption.19}{}}
\newlabel{tab:term_document_shakespeare@cref}{{[table][1][]1}{[1][33][]33}}
\@writefile{toc}{\contentsline {paragraph}{Matrice termine-termine.}{34}{section*.20}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Estratto di una matrice termine--termine calcolata sul corpus Wikipedia. Ogni cella contiene il numero di co-occorrenze tra la parola target (riga) e la parola di contesto (colonna) all'interno di una finestra di contesto locale.\relax }}{34}{table.caption.21}\protected@file@percent }
\newlabel{tab:term_term_wikipedia}{{2}{34}{Estratto di una matrice termine--termine calcolata sul corpus Wikipedia. Ogni cella contiene il numero di co-occorrenze tra la parola target (riga) e la parola di contesto (colonna) all'interno di una finestra di contesto locale.\relax }{table.caption.21}{}}
\newlabel{tab:term_term_wikipedia@cref}{{[table][2][]2}{[1][34][]34}}
\@writefile{toc}{\contentsline {paragraph}{Riduzione dimensionale tramite SVD.}{35}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cosine similarity.}{36}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Word2Vec: un approccio predittivo}{36}{subsubsection.3.2.2}\protected@file@percent }
\newlabel{subsubsec:word2vec}{{3.2.2}{36}{Word2Vec: un approccio predittivo}{subsubsection.3.2.2}{}}
\newlabel{subsubsec:word2vec@cref}{{[subsubsection][2][3,2]3.2.2}{[1][36][]36}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Word2Vec come fusione tra teoria e implementazione. A sinistra, l'ipotesi di Osgood si traduce nella scelta di vettori continui e nel prodotto scalare come misura di similarità. A destra, l'ipotesi distribuzionale si concretizza nella finestra di contesto e nella modellazione di co-occorrenze. Word2Vec unifica elegantemente queste due linee, apprendendo vettori che massimizzano la predizione di co-occorrenze locali.\relax }}{36}{figure.caption.24}\protected@file@percent }
\newlabel{fig:word2vec_teoria_implementazione}{{15}{36}{Word2Vec come fusione tra teoria e implementazione. A sinistra, l'ipotesi di Osgood si traduce nella scelta di vettori continui e nel prodotto scalare come misura di similarità. A destra, l'ipotesi distribuzionale si concretizza nella finestra di contesto e nella modellazione di co-occorrenze. Word2Vec unifica elegantemente queste due linee, apprendendo vettori che massimizzano la predizione di co-occorrenze locali.\relax }{figure.caption.24}{}}
\newlabel{fig:word2vec_teoria_implementazione@cref}{{[figure][15][]15}{[1][36][]36}}
\@writefile{toc}{\contentsline {paragraph}{Il classificatore e la funzione sigmoide.}{37}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Perché due matrici? Il ruolo di $W$ e $C$.}{38}{section*.26}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Lo skip-gram apprende in totale due insiemi di embedding, uno per i target ($W$) e uno per i contesti ($C$), per un totale di $2|V|$ vettori. L'addestramento mira a massimizzare la probabilità che parole vicine nel testo abbiano vettori simili.\relax }}{38}{figure.caption.27}\protected@file@percent }
\newlabel{fig:skipgram_structure}{{16}{38}{Lo skip-gram apprende in totale due insiemi di embedding, uno per i target ($W$) e uno per i contesti ($C$), per un totale di $2|V|$ vettori. L'addestramento mira a massimizzare la probabilità che parole vicine nel testo abbiano vettori simili.\relax }{figure.caption.27}{}}
\newlabel{fig:skipgram_structure@cref}{{[figure][16][]16}{[1][38][]38}}
\@writefile{toc}{\contentsline {paragraph}{Finestra di contesto e precisione semantica.}{38}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Geometria dell'analogia: il modello del parallelogramma.}{39}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Limiti degli embeddings statici}{39}{subsubsection.3.2.3}\protected@file@percent }
\newlabel{subsubsec:static_limits}{{3.2.3}{39}{Limiti degli embeddings statici}{subsubsection.3.2.3}{}}
\newlabel{subsubsec:static_limits@cref}{{[subsubsection][3][3,2]3.2.3}{[1][39][]39}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Rappresentazione geometrica del modello del parallelogramma nello spazio latente.\relax }}{40}{figure.caption.30}\protected@file@percent }
\newlabel{fig:parallelogramma}{{17}{40}{Rappresentazione geometrica del modello del parallelogramma nello spazio latente.\relax }{figure.caption.30}{}}
\newlabel{fig:parallelogramma@cref}{{[figure][17][]17}{[1][39][]40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Embeddings dinamici (contestuali)}{41}{subsection.3.3}\protected@file@percent }
\newlabel{sec:dynamic_embeddings}{{3.3}{41}{Embeddings dinamici (contestuali)}{subsection.3.3}{}}
\newlabel{sec:dynamic_embeddings@cref}{{[subsection][3][3]3.3}{[1][41][]41}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}RNN e LSTM: memoria sequenziale}{42}{subsubsection.3.3.1}\protected@file@percent }
\newlabel{subsubsec:rnn_lstm_compendium}{{3.3.1}{42}{RNN e LSTM: memoria sequenziale}{subsubsection.3.3.1}{}}
\newlabel{subsubsec:rnn_lstm_compendium@cref}{{[subsubsection][1][3,3]3.3.1}{[1][42][]42}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Illustrazione del funzionamento di una RNN. Ad ogni passo l'informazione contestuale viene passata a quello successivo attraverso lo stato nascosto $h_t$.\relax }}{42}{figure.caption.31}\protected@file@percent }
\newlabel{fig:rnn_flow}{{18}{42}{Illustrazione del funzionamento di una RNN. Ad ogni passo l'informazione contestuale viene passata a quello successivo attraverso lo stato nascosto $h_t$.\relax }{figure.caption.31}{}}
\newlabel{fig:rnn_flow@cref}{{[figure][18][]18}{[1][42][]42}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Encoder-Decoder e il problema del bottleneck}{44}{subsubsection.3.3.2}\protected@file@percent }
\newlabel{subsubsec:encoder_decoder_bottleneck}{{3.3.2}{44}{Encoder-Decoder e il problema del bottleneck}{subsubsection.3.3.2}{}}
\newlabel{subsubsec:encoder_decoder_bottleneck@cref}{{[subsubsection][2][3,3]3.3.2}{[1][44][]44}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Schema Encoder--Decoder: quando il contesto $c$ coincide con il solo stato nascosto finale dell'encoder, tutta l'informazione della sequenza sorgente deve attraversare un \emph  {collo di bottiglia} rappresentazionale prima di essere utilizzata dal decoder.\relax }}{45}{figure.caption.32}\protected@file@percent }
\newlabel{fig:encoder_decoder_bottleneck}{{19}{45}{Schema Encoder--Decoder: quando il contesto $c$ coincide con il solo stato nascosto finale dell'encoder, tutta l'informazione della sequenza sorgente deve attraversare un \emph {collo di bottiglia} rappresentazionale prima di essere utilizzata dal decoder.\relax }{figure.caption.32}{}}
\newlabel{fig:encoder_decoder_bottleneck@cref}{{[figure][19][]19}{[1][44][]45}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Meccanismo di attenzione: superare il bottleneck}{45}{subsubsection.3.3.3}\protected@file@percent }
\newlabel{subsubsec:attention_mechanism}{{3.3.3}{45}{Meccanismo di attenzione: superare il bottleneck}{subsubsection.3.3.3}{}}
\newlabel{subsubsec:attention_mechanism@cref}{{[subsubsection][3][3,3]3.3.3}{[1][45][]45}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Nel meccanismo di attenzione, a ciascun passo di decodifica $i$ il decoder utilizza un contesto \emph  {dinamico} $c_i$ (diverso per ogni token generato), calcolato come funzione di tutti gli stati nascosti dell'encoder.\relax }}{46}{figure.caption.33}\protected@file@percent }
\newlabel{fig:attention_dynamic_context}{{20}{46}{Nel meccanismo di attenzione, a ciascun passo di decodifica $i$ il decoder utilizza un contesto \emph {dinamico} $c_i$ (diverso per ogni token generato), calcolato come funzione di tutti gli stati nascosti dell'encoder.\relax }{figure.caption.33}{}}
\newlabel{fig:attention_dynamic_context@cref}{{[figure][20][]20}{[1][45][]46}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Verso i Transformer}{47}{subsubsection.3.3.4}\protected@file@percent }
\newlabel{subsubsec:towards_transformers}{{3.3.4}{47}{Verso i Transformer}{subsubsection.3.3.4}{}}
\newlabel{subsubsec:towards_transformers@cref}{{[subsubsection][4][3,3]3.3.4}{[1][47][]47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}BERT: embeddings bidirezionali e il problema dell'opacità}{48}{subsection.3.4}\protected@file@percent }
\newlabel{sec:bert}{{3.4}{48}{BERT: embeddings bidirezionali e il problema dell'opacità}{subsection.3.4}{}}
\newlabel{sec:bert@cref}{{[subsection][4][3]3.4}{[1][47][]48}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Il principio di Vapnik: risolvere il problema giusto}{48}{subsubsection.3.4.1}\protected@file@percent }
\newlabel{subsubsec:vapnik_principle}{{3.4.1}{48}{Il principio di Vapnik: risolvere il problema giusto}{subsubsection.3.4.1}{}}
\newlabel{subsubsec:vapnik_principle@cref}{{[subsubsection][1][3,4]3.4.1}{[1][48][]48}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Architettura di BERT}{50}{subsubsection.3.4.2}\protected@file@percent }
\newlabel{subsubsec:bert_architecture}{{3.4.2}{50}{Architettura di BERT}{subsubsection.3.4.2}{}}
\newlabel{subsubsec:bert_architecture@cref}{{[subsubsection][2][3,4]3.4.2}{[1][50][]50}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Illustrazione del principio di Vapnik applicato ai modelli linguistici. La generazione autoregressiva è un problema più generale della comprensione contestuale; BERT risolve direttamente il problema più specifico senza passare per quello generale.\relax }}{51}{figure.caption.34}\protected@file@percent }
\newlabel{fig:vapnik_principle}{{21}{51}{Illustrazione del principio di Vapnik applicato ai modelli linguistici. La generazione autoregressiva è un problema più generale della comprensione contestuale; BERT risolve direttamente il problema più specifico senza passare per quello generale.\relax }{figure.caption.34}{}}
\newlabel{fig:vapnik_principle@cref}{{[figure][21][]21}{[1][50][]51}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Architettura di BERT. L'input viene codificato tramite embeddings (token, posizione, segmento), processato da una pila di Transformer encoder, e produce embeddings contestuali per ogni posizione. Il token \texttt  {[MASK]} viene predetto utilizzando l'intero contesto bidirezionale.\relax }}{52}{figure.caption.35}\protected@file@percent }
\newlabel{fig:bert_architecture}{{22}{52}{Architettura di BERT. L'input viene codificato tramite embeddings (token, posizione, segmento), processato da una pila di Transformer encoder, e produce embeddings contestuali per ogni posizione. Il token \texttt {[MASK]} viene predetto utilizzando l'intero contesto bidirezionale.\relax }{figure.caption.35}{}}
\newlabel{fig:bert_architecture@cref}{{[figure][22][]22}{[1][51][]52}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Composizione dell'input in BERT. Ogni token è rappresentato dalla somma di tre embeddings: token, posizione e segmento.\relax }}{53}{figure.caption.36}\protected@file@percent }
\newlabel{fig:bert_input_representation}{{23}{53}{Composizione dell'input in BERT. Ogni token è rappresentato dalla somma di tre embeddings: token, posizione e segmento.\relax }{figure.caption.36}{}}
\newlabel{fig:bert_input_representation@cref}{{[figure][23][]23}{[1][52][]53}}
\@setckpt{chapters/03_embeddings}{
\setcounter{page}{55}
\setcounter{equation}{31}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{subsubsection}{2}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{23}
\setcounter{table}{2}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{lstnumber}{1}
\setcounter{parentequation}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{15}
\setcounter{maxnames}{1}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{blx@maxsegment@0}{0}
\setcounter{blx@sectionciteorder@0}{21}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{Item}{70}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{48}
\setcounter{FancyVerbLine}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{15}
\setcounter{definition}{0}
\setcounter{lstlisting}{0}
\setcounter{section@level}{3}
}
