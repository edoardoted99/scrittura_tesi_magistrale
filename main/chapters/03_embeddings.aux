\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {3}Embeddings}{15}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Introduzione}{16}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}L'ipotesi distribuzionale}{16}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Ipotesi di Osgood}{17}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Embeddings}{18}{section.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Classificazione delle principali tipologie di embeddings trattate nel capitolo.\relax }}{19}{figure.caption.7}\protected@file@percent }
\newlabel{fig:classificazione_embeddings}{{4}{19}{Classificazione delle principali tipologie di embeddings trattate nel capitolo.\relax }{figure.caption.7}{}}
\newlabel{fig:classificazione_embeddings@cref}{{[figure][4][]4}{[1][18][]19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Embeddings count-based}{19}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}Matrice termine-documento}{19}{subsubsection.7.1.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Term-document matrix per quattro parole in quattro opere di Shakespeare. Ogni cella contiene il numero di occorrenze della parola (riga) nel documento (colonna). \blx@tocontentsinit {0}\cite {wang2024disentangledrepresentationlearning}\relax }}{20}{table.caption.8}\protected@file@percent }
\newlabel{tab:term_document_shakespeare}{{1}{20}{Term-document matrix per quattro parole in quattro opere di Shakespeare. Ogni cella contiene il numero di occorrenze della parola (riga) nel documento (colonna). \cite {wang2024disentangledrepresentationlearning}\relax }{table.caption.8}{}}
\newlabel{tab:term_document_shakespeare@cref}{{[table][1][]1}{[1][19][]20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.2}Matrice termine-termine}{20}{subsubsection.7.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Estratto di una matrice termine--termine calcolata sul corpus Wikipedia. Ogni cella contiene il numero di co-occorrenze tra la parola target (riga) e la parola di contesto (colonna) all’interno di una finestra di contesto locale \blx@tocontentsinit {0}\cite {wang2024disentangledrepresentationlearning}.\relax }}{21}{table.caption.9}\protected@file@percent }
\newlabel{tab:term_term_wikipedia}{{2}{21}{Estratto di una matrice termine--termine calcolata sul corpus Wikipedia. Ogni cella contiene il numero di co-occorrenze tra la parola target (riga) e la parola di contesto (colonna) all’interno di una finestra di contesto locale \cite {wang2024disentangledrepresentationlearning}.\relax }{table.caption.9}{}}
\newlabel{tab:term_term_wikipedia@cref}{{[table][2][]2}{[1][21][]21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Riduzione dimensionale tramite SVD}{22}{subsection.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Cosine Similarity}{23}{subsection.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Word2Vec: un approccio predittivo}{24}{subsection.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.1}Il classificatore e la funzione sigmoide}{24}{subsubsection.7.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.2}Apprendimento e Negative Sampling}{25}{subsubsection.7.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.3}Perché due matrici? Il ruolo di $W$ e $C$}{25}{subsubsection.7.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Lo skip-gram apprende in totale due insiemi di embedding, uno per i target ($W$) e uno per i contesti ($C$), per un totale di $2|V|$ vettori. L’addestramento mira a massimizzare la probabilità che parole vicine nel testo abbiano vettori simili.\relax }}{26}{figure.caption.10}\protected@file@percent }
\newlabel{fig:skipgram_structure}{{5}{26}{Lo skip-gram apprende in totale due insiemi di embedding, uno per i target ($W$) e uno per i contesti ($C$), per un totale di $2|V|$ vettori. L’addestramento mira a massimizzare la probabilità che parole vicine nel testo abbiano vettori simili.\relax }{figure.caption.10}{}}
\newlabel{fig:skipgram_structure@cref}{{[figure][5][]5}{[1][26][]26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Proprietà semantiche degli embeddings}{26}{subsection.7.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Rappresentazione geometrica del modello del parallelogramma applicato all'analogia di genere.\relax }}{28}{figure.caption.11}\protected@file@percent }
\newlabel{fig:parallelogramma}{{6}{28}{Rappresentazione geometrica del modello del parallelogramma applicato all'analogia di genere.\relax }{figure.caption.11}{}}
\newlabel{fig:parallelogramma@cref}{{[figure][6][]6}{[1][27][]28}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Embeddings dinamici}{28}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Reti Neurali Ricorrenti}{29}{section.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Illustrazione del funzionamento di una RNN. Ad ogni passo l'informazione contestuale viene consegnata a quello successivo.\relax }}{30}{figure.caption.12}\protected@file@percent }
\newlabel{fig:rnn_flow}{{7}{30}{Illustrazione del funzionamento di una RNN. Ad ogni passo l'informazione contestuale viene consegnata a quello successivo.\relax }{figure.caption.12}{}}
\newlabel{fig:rnn_flow@cref}{{[figure][7][]7}{[1][30][]30}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Rappresentazione srotolata (unrolled) di una RNN. Le frecce orizzontali mostrano il passaggio dello stato nascosto attraverso il tempo (pesi $\mathbf  {U}$), mentre quelle verticali indicano l'elaborazione dell'input ($\mathbf  {W}$) e la generazione dell'output ($\mathbf  {V}$).\relax }}{31}{figure.caption.13}\protected@file@percent }
\newlabel{fig:rnn_unrolled}{{8}{31}{Rappresentazione srotolata (unrolled) di una RNN. Le frecce orizzontali mostrano il passaggio dello stato nascosto attraverso il tempo (pesi $\mathbf {U}$), mentre quelle verticali indicano l'elaborazione dell'input ($\mathbf {W}$) e la generazione dell'output ($\mathbf {V}$).\relax }{figure.caption.13}{}}
\newlabel{fig:rnn_unrolled@cref}{{[figure][8][]8}{[1][31][]31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}RNN come Language Models}{31}{subsection.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Generaizone di Embeddings tramite RNN}{32}{subsection.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}RNN Bidirezionali (Bi-RNN)}{33}{subsection.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Il problema del Gradiente Svanente}{33}{subsection.9.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}LSTM: Long Short-Term Memory}{34}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Meccanismi di Gating}{35}{subsection.10.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Rappresentazione di una singola unità LSTM come grafo computazionale. Gli input consistono nell'input attuale $x_t$, lo stato nascosto precedente $h_{t-1}$ e il contesto precedente $c_{t-1}$. Gli output sono il nuovo stato nascosto $h_t$ e il contesto aggiornato $c_t$ \blx@tocontentsinit {0}\cite {jm3}.\relax }}{35}{figure.caption.14}\protected@file@percent }
\newlabel{fig:lstm_unit}{{9}{35}{Rappresentazione di una singola unità LSTM come grafo computazionale. Gli input consistono nell'input attuale $x_t$, lo stato nascosto precedente $h_{t-1}$ e il contesto precedente $c_{t-1}$. Gli output sono il nuovo stato nascosto $h_t$ e il contesto aggiornato $c_t$ \cite {jm3}.\relax }{figure.caption.14}{}}
\newlabel{fig:lstm_unit@cref}{{[figure][9][]9}{[1][34][]35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Le equazioni del modello}{35}{subsection.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Modularità ed Embeddings}{36}{subsection.10.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Architettura Encoder-Decoder e limite del \textit  {bottleneck}}{36}{section.11}\protected@file@percent }
\newlabel{sec:encoder_decoder_rnn}{{11}{36}{Architettura Encoder-Decoder e limite del \textit {bottleneck}}{section.11}{}}
\newlabel{sec:encoder_decoder_rnn@cref}{{[section][11][]11}{[1][36][]36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1}Il problema \textit  {sequence-to-sequence}}{37}{subsection.11.1}\protected@file@percent }
\newlabel{eq:seq2seq_chain_rule}{{40}{37}{Il problema \textit {sequence-to-sequence}}{equation.11.40}{}}
\newlabel{eq:seq2seq_chain_rule@cref}{{[equation][40][]40}{[1][37][]37}}
\newlabel{eq:encoder_recurrence}{{41}{37}{Il problema \textit {sequence-to-sequence}}{equation.11.41}{}}
\newlabel{eq:encoder_recurrence@cref}{{[equation][41][]41}{[1][37][]37}}
\newlabel{eq:context_vector}{{42}{37}{Il problema \textit {sequence-to-sequence}}{equation.11.42}{}}
\newlabel{eq:context_vector@cref}{{[equation][42][]42}{[1][37][]37}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Schema formale dell'encoder--decoder ricorrente: l'encoder produce una sequenza di stati $h^{(e)}_1,\dots  ,h^{(e)}_n$ e il suo stato finale $h^{(e)}_n$ viene identificato con il vettore di contesto $c$, usato per inizializzare il decoder ($h^{(d)}_0$) e, nella variante mostrata, reso disponibile a ogni passo di decodifica. Questa dipendenza da un unico vettore di contesto anticipa il problema del \textit  {bottleneck} discusso in seguito. Adattata da \blx@tocontentsinit {0}\cite {jm3}.\relax }}{38}{figure.caption.15}\protected@file@percent }
\newlabel{fig:seq2seq_formal}{{10}{38}{Schema formale dell'encoder--decoder ricorrente: l'encoder produce una sequenza di stati $h^{(e)}_1,\dots ,h^{(e)}_n$ e il suo stato finale $h^{(e)}_n$ viene identificato con il vettore di contesto $c$, usato per inizializzare il decoder ($h^{(d)}_0$) e, nella variante mostrata, reso disponibile a ogni passo di decodifica. Questa dipendenza da un unico vettore di contesto anticipa il problema del \textit {bottleneck} discusso in seguito. Adattata da \cite {jm3}.\relax }{figure.caption.15}{}}
\newlabel{fig:seq2seq_formal@cref}{{[figure][10][]10}{[1][37][]38}}
\newlabel{eq:decoder_recurrence}{{43}{38}{Il problema \textit {sequence-to-sequence}}{equation.11.43}{}}
\newlabel{eq:decoder_recurrence@cref}{{[equation][43][]43}{[1][37][]38}}
\newlabel{eq:decoder_softmax}{{44}{38}{Il problema \textit {sequence-to-sequence}}{equation.11.44}{}}
\newlabel{eq:decoder_softmax@cref}{{[equation][44][]44}{[1][37][]38}}
\newlabel{eq:decoder_init}{{45}{38}{Il problema \textit {sequence-to-sequence}}{equation.11.45}{}}
\newlabel{eq:decoder_init@cref}{{[equation][45][]45}{[1][38][]38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2}Il limite del \textit  {bottleneck} informativo}{39}{subsection.11.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Schema Encoder--Decoder: quando il contesto $c$ coincide con il solo stato nascosto finale dell'encoder, tutta l'informazione della sequenza sorgente deve attraversare un \emph  {collo di bottiglia} rappresentazionale prima di essere utilizzata dal decoder.\relax }}{39}{figure.caption.16}\protected@file@percent }
\newlabel{fig:encoder_decoder_bottleneck}{{11}{39}{Schema Encoder--Decoder: quando il contesto $c$ coincide con il solo stato nascosto finale dell'encoder, tutta l'informazione della sequenza sorgente deve attraversare un \emph {collo di bottiglia} rappresentazionale prima di essere utilizzata dal decoder.\relax }{figure.caption.16}{}}
\newlabel{fig:encoder_decoder_bottleneck@cref}{{[figure][11][]11}{[1][39][]39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3}Soluzione al bottleneck: Meccanismo dell'attenzione}{40}{subsection.11.3}\protected@file@percent }
\newlabel{subsec:attention}{{11.3}{40}{Soluzione al bottleneck: Meccanismo dell'attenzione}{subsection.11.3}{}}
\newlabel{subsec:attention@cref}{{[subsection][3][11]11.3}{[1][40][]40}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Nel meccanismo di attenzione, a ciascun passo di decodifica $i$ il decoder utilizza un contesto \emph  {dinamico} $c_i$ (diverso per ogni token generato), calcolato come funzione di tutti gli stati nascosti dell'encoder. Adattata da \blx@tocontentsinit {0}\cite {jm3}.\relax }}{40}{figure.caption.17}\protected@file@percent }
\newlabel{fig:attention_dynamic_context}{{12}{40}{Nel meccanismo di attenzione, a ciascun passo di decodifica $i$ il decoder utilizza un contesto \emph {dinamico} $c_i$ (diverso per ogni token generato), calcolato come funzione di tutti gli stati nascosti dell'encoder. Adattata da \cite {jm3}.\relax }{figure.caption.17}{}}
\newlabel{fig:attention_dynamic_context@cref}{{[figure][12][]12}{[1][40][]40}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Schema encoder--decoder con attenzione, focalizzato sul calcolo di $c_i$. Per ogni stato precedente del decoder $h^{(d)}_{i-1}$ si calcola un punteggio di rilevanza rispetto a ciascuno stato dell'encoder $h^{(e)}_j$; i punteggi vengono normalizzati in pesi $\alpha _{ij}$ e usati per ottenere $c_i$ come somma pesata degli stati dell'encoder. Adattata da \blx@tocontentsinit {0}\cite {jm3}.\relax }}{41}{figure.caption.18}\protected@file@percent }
\newlabel{fig:attention_ci_computation}{{13}{41}{Schema encoder--decoder con attenzione, focalizzato sul calcolo di $c_i$. Per ogni stato precedente del decoder $h^{(d)}_{i-1}$ si calcola un punteggio di rilevanza rispetto a ciascuno stato dell'encoder $h^{(e)}_j$; i punteggi vengono normalizzati in pesi $\alpha _{ij}$ e usati per ottenere $c_i$ come somma pesata degli stati dell'encoder. Adattata da \cite {jm3}.\relax }{figure.caption.18}{}}
\newlabel{fig:attention_ci_computation@cref}{{[figure][13][]13}{[1][40][]41}}
\newlabel{eq:decoder_with_attention_jm}{{46}{41}{Soluzione al bottleneck: Meccanismo dell'attenzione}{equation.11.46}{}}
\newlabel{eq:decoder_with_attention_jm@cref}{{[equation][46][]46}{[1][41][]41}}
\newlabel{eq:attention_score_general}{{47}{42}{Soluzione al bottleneck: Meccanismo dell'attenzione}{equation.11.47}{}}
\newlabel{eq:attention_score_general@cref}{{[equation][47][]47}{[1][41][]42}}
\newlabel{eq:attention_dot_score}{{48}{42}{Soluzione al bottleneck: Meccanismo dell'attenzione}{equation.11.48}{}}
\newlabel{eq:attention_dot_score@cref}{{[equation][48][]48}{[1][42][]42}}
\newlabel{eq:attention_weights_softmax}{{49}{42}{Soluzione al bottleneck: Meccanismo dell'attenzione}{equation.11.49}{}}
\newlabel{eq:attention_weights_softmax@cref}{{[equation][49][]49}{[1][42][]42}}
\newlabel{eq:attention_context_weighted_sum}{{50}{42}{Soluzione al bottleneck: Meccanismo dell'attenzione}{equation.11.50}{}}
\newlabel{eq:attention_context_weighted_sum@cref}{{[equation][50][]50}{[1][42][]42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4}Verso i Transformer}{42}{subsection.11.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12}Il Transformer}{43}{section.12}\protected@file@percent }
\newlabel{sec:transformer}{{12}{43}{Il Transformer}{section.12}{}}
\newlabel{sec:transformer@cref}{{[section][12][]12}{[1][43][]43}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Schema di un Transformer causale (left-to-right) per language modeling. Ogni token in input viene codificato (embedding del token e della posizione), processato da una pila di blocchi Transformer, e infine proiettato tramite una testa di language modeling per predire il token successivo.\relax }}{43}{figure.caption.19}\protected@file@percent }
\newlabel{fig:transformer_overview}{{14}{43}{Schema di un Transformer causale (left-to-right) per language modeling. Ogni token in input viene codificato (embedding del token e della posizione), processato da una pila di blocchi Transformer, e infine proiettato tramite una testa di language modeling per predire il token successivo.\relax }{figure.caption.19}{}}
\newlabel{fig:transformer_overview@cref}{{[figure][14][]14}{[1][43][]43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1}Self-attention}{45}{subsection.12.1}\protected@file@percent }
\newlabel{subsec:self_attention}{{12.1}{45}{Self-attention}{subsection.12.1}{}}
\newlabel{subsec:self_attention@cref}{{[subsection][1][12]12.1}{[1][45][]45}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Esempio di distribuzione dei pesi di self-attention $\alpha $: per costruire una rappresentazione contestuale del token \emph  {it} in uno strato superiore, il modello attribuisce peso maggiore ad alcuni token precedenti particolarmente informativi (ad es.\ \emph  {chicken} e \emph  {road}). In quel punto della sequenza, infatti, la coreferenza del pronome non è ancora disambiguata; è quindi plausibile che la rappresentazione di \emph  {it} debba incorporare evidenza proveniente da entrambe le possibili entità.\relax }}{45}{figure.caption.20}\protected@file@percent }
\newlabel{fig:selfattn_alpha}{{15}{45}{Esempio di distribuzione dei pesi di self-attention $\alpha $: per costruire una rappresentazione contestuale del token \emph {it} in uno strato superiore, il modello attribuisce peso maggiore ad alcuni token precedenti particolarmente informativi (ad es.\ \emph {chicken} e \emph {road}). In quel punto della sequenza, infatti, la coreferenza del pronome non è ancora disambiguata; è quindi plausibile che la rappresentazione di \emph {it} debba incorporare evidenza proveniente da entrambe le possibili entità.\relax }{figure.caption.20}{}}
\newlabel{fig:selfattn_alpha@cref}{{[figure][15][]15}{[1][45][]45}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.1.1}Motivazione: dalle rappresentazioni statiche alle rappresentazioni contestuali}{45}{subsubsection.12.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Self-attention causale (left-to-right): in posizione $i$ il modello combina esclusivamente i token nelle posizioni $j\le i$ e, tramite mascheramento, non può incorporare informazione proveniente da posizioni future.\relax }}{46}{figure.caption.21}\protected@file@percent }
\newlabel{fig:selfattn_flow}{{16}{46}{Self-attention causale (left-to-right): in posizione $i$ il modello combina esclusivamente i token nelle posizioni $j\le i$ e, tramite mascheramento, non può incorporare informazione proveniente da posizioni future.\relax }{figure.caption.21}{}}
\newlabel{fig:selfattn_flow@cref}{{[figure][16][]16}{[1][45][]46}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.1.2}Self-attention causale: dominio informativo e vincolo di autoregressione}{47}{subsubsection.12.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.1.3}Intuizione: self-attention come combinazione pesata del contesto}{47}{subsubsection.12.1.3}\protected@file@percent }
\newlabel{eq:selfattn_weighted_sum}{{51}{47}{Intuizione: self-attention come combinazione pesata del contesto}{equation.12.51}{}}
\newlabel{eq:selfattn_weighted_sum@cref}{{[equation][51][]51}{[1][47][]47}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.1.4}Scaled dot-product attention: ruoli di query, key e value}{48}{subsubsection.12.1.4}\protected@file@percent }
\newlabel{eq:qkv}{{52}{48}{Scaled dot-product attention: ruoli di query, key e value}{equation.12.52}{}}
\newlabel{eq:qkv@cref}{{[equation][52][]52}{[1][48][]48}}
\@writefile{toc}{\contentsline {paragraph}{Punteggi di compatibilità (scores).}{48}{section*.22}\protected@file@percent }
\newlabel{eq:score_scaled}{{53}{48}{Punteggi di compatibilità (scores)}{equation.12.53}{}}
\newlabel{eq:score_scaled@cref}{{[equation][53][]53}{[1][48][]48}}
\@writefile{toc}{\contentsline {paragraph}{Normalizzazione tramite softmax e vincolo causale.}{49}{section*.23}\protected@file@percent }
\newlabel{eq:alpha_softmax}{{54}{49}{Normalizzazione tramite softmax e vincolo causale}{equation.12.54}{}}
\newlabel{eq:alpha_softmax@cref}{{[equation][54][]54}{[1][48][]49}}
\@writefile{toc}{\contentsline {paragraph}{Aggregazione dei value e proiezione in output.}{49}{section*.24}\protected@file@percent }
\newlabel{eq:head_output}{{55}{49}{Aggregazione dei value e proiezione in output}{equation.12.55}{}}
\newlabel{eq:head_output@cref}{{[equation][55][]55}{[1][49][]49}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.1.5}Forma matriciale e dimensioni (utile per l’implementazione)}{49}{subsubsection.12.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Calcolo di una singola testa di self-attention causale: (i) proiezioni in query/key/value; (ii) punteggi di compatibilità via dot-product scalato; (iii) normalizzazione in pesi $\alpha _{ij}$; (iv) somma pesata dei value per ottenere $\mathrm  {head}_i$; (v) proiezione finale per ottenere $a_i$ in dimensione del modello.\relax }}{50}{figure.caption.25}\protected@file@percent }
\newlabel{fig:selfattn_head}{{17}{50}{Calcolo di una singola testa di self-attention causale: (i) proiezioni in query/key/value; (ii) punteggi di compatibilità via dot-product scalato; (iii) normalizzazione in pesi $\alpha _{ij}$; (iv) somma pesata dei value per ottenere $\mathrm {head}_i$; (v) proiezione finale per ottenere $a_i$ in dimensione del modello.\relax }{figure.caption.25}{}}
\newlabel{fig:selfattn_head@cref}{{[figure][17][]17}{[1][49][]50}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.1.6}Multi-head attention: pluralità di criteri di selezione}{51}{subsubsection.12.1.6}\protected@file@percent }
\newlabel{eq:qkv_multihead}{{56}{51}{Multi-head attention: pluralità di criteri di selezione}{equation.12.56}{}}
\newlabel{eq:qkv_multihead@cref}{{[equation][56][]56}{[1][51][]51}}
\newlabel{eq:multihead_concat}{{57}{51}{Multi-head attention: pluralità di criteri di selezione}{equation.12.57}{}}
\newlabel{eq:multihead_concat@cref}{{[equation][57][]57}{[1][51][]51}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.1.7}Osservazione conclusiva: self-attention e contestualizzazione progressiva}{51}{subsubsection.12.1.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Multi-head attention: più teste calcolano in parallelo combinazioni pesate del contesto con parametri indipendenti; gli output vengono concatenati e proiettati per ottenere un vettore finale nella stessa dimensionalità dell’input.\relax }}{52}{figure.caption.26}\protected@file@percent }
\newlabel{fig:multihead}{{18}{52}{Multi-head attention: più teste calcolano in parallelo combinazioni pesate del contesto con parametri indipendenti; gli output vengono concatenati e proiettati per ottenere un vettore finale nella stessa dimensionalità dell’input.\relax }{figure.caption.26}{}}
\newlabel{fig:multihead@cref}{{[figure][18][]18}{[1][51][]52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2}Blocco Transformer}{52}{subsection.12.2}\protected@file@percent }
\newlabel{subsec:transformer_block}{{12.2}{52}{Blocco Transformer}{subsection.12.2}{}}
\newlabel{subsec:transformer_block@cref}{{[subsection][2][12]12.2}{[1][52][]52}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.2.1}Input del blocco e informazione di posizione}{53}{subsubsection.12.2.1}\protected@file@percent }
\newlabel{eq:token_plus_positional}{{58}{53}{Input del blocco e informazione di posizione}{equation.12.58}{}}
\newlabel{eq:token_plus_positional@cref}{{[equation][58][]58}{[1][53][]53}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.2.2}Residual stream: il flusso informativo del token}{53}{subsubsection.12.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Architettura di un blocco Transformer nella variante \emph  {prenorm} e interpretazione tramite \emph  {residual stream}: ciascun modulo legge dallo stream del token e aggiunge il proprio output allo stesso stream tramite una somma residua \blx@tocontentsinit {0}\cite {jm3}.\relax }}{54}{figure.caption.27}\protected@file@percent }
\newlabel{fig:transformer_block_residual_stream}{{19}{54}{Architettura di un blocco Transformer nella variante \emph {prenorm} e interpretazione tramite \emph {residual stream}: ciascun modulo legge dallo stream del token e aggiunge il proprio output allo stesso stream tramite una somma residua \cite {jm3}.\relax }{figure.caption.27}{}}
\newlabel{fig:transformer_block_residual_stream@cref}{{[figure][19][]19}{[1][53][]54}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.2.3}Perché la LayerNorm (motivazione)}{55}{subsubsection.12.2.3}\protected@file@percent }
\newlabel{eq:layernorm}{{60}{55}{Perché la LayerNorm (motivazione)}{equation.12.60}{}}
\newlabel{eq:layernorm@cref}{{[equation][60][]60}{[1][55][]55}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.2.4}Feedforward network (ruolo)}{55}{subsubsection.12.2.4}\protected@file@percent }
\newlabel{eq:ffn}{{61}{55}{Feedforward network (ruolo)}{equation.12.61}{}}
\newlabel{eq:ffn@cref}{{[equation][61][]61}{[1][55][]55}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.2.5}Equazioni del blocco (variante \emph  {prenorm})}{56}{subsubsection.12.2.5}\protected@file@percent }
\newlabel{eq:transformer_block_prenorm}{{67}{56}{Equazioni del blocco (variante \emph {prenorm})}{equation.12.67}{}}
\newlabel{eq:transformer_block_prenorm@cref}{{[equation][67][]67}{[1][56][]56}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.2.6}L’attenzione come “movimento” di informazione tra stream}{56}{subsubsection.12.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3}Parallelizzazione del calcolo con una singola matrice $X$}{56}{subsection.12.3}\protected@file@percent }
\newlabel{subsec:parallelizing_with_X}{{12.3}{56}{Parallelizzazione del calcolo con una singola matrice $X$}{subsection.12.3}{}}
\newlabel{subsec:parallelizing_with_X@cref}{{[subsection][3][12]12.3}{[1][56][]56}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Interpretazione della self-attention: una testa può spostare informazione dal residual stream del token $A$ a quello del token $B$, integrando in $B$ contenuto recuperato da altre posizioni \blx@tocontentsinit {0}\cite {jm3}.\relax }}{57}{figure.caption.28}\protected@file@percent }
\newlabel{fig:attention_moves_info}{{20}{57}{Interpretazione della self-attention: una testa può spostare informazione dal residual stream del token $A$ a quello del token $B$, integrando in $B$ contenuto recuperato da altre posizioni \cite {jm3}.\relax }{figure.caption.28}{}}
\newlabel{fig:attention_moves_info@cref}{{[figure][20][]20}{[1][56][]57}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.3.1}Impacchettare la sequenza in una matrice}{57}{subsubsection.12.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.3.2}Self-attention in forma matriciale (una testa)}{57}{subsubsection.12.3.2}\protected@file@percent }
\newlabel{eq:QKV_matrix}{{68}{57}{Self-attention in forma matriciale (una testa)}{equation.12.68}{}}
\newlabel{eq:QKV_matrix@cref}{{[equation][68][]68}{[1][57][]57}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces La matrice $N\times N$ $QK^\top $: ogni cella contiene un confronto $q_i\cdot k_j$, calcolato simultaneamente con una singola moltiplicazione tra matrici \blx@tocontentsinit {0}\cite {jm3}.\relax }}{58}{figure.caption.29}\protected@file@percent }
\newlabel{fig:qkt_full}{{21}{58}{La matrice $N\times N$ $QK^\top $: ogni cella contiene un confronto $q_i\cdot k_j$, calcolato simultaneamente con una singola moltiplicazione tra matrici \cite {jm3}.\relax }{figure.caption.29}{}}
\newlabel{fig:qkt_full@cref}{{[figure][21][]21}{[1][58][]58}}
\newlabel{eq:head_matrix}{{69}{58}{Self-attention in forma matriciale (una testa)}{equation.12.69}{}}
\newlabel{eq:head_matrix@cref}{{[equation][69][]69}{[1][58][]58}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.3.3}Mascheramento causale: eliminare il futuro}{58}{subsubsection.12.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Mascheramento causale di $QK^\top $: la parte triangolare superiore (con $j>i$) è impostata a $-\infty $, così la softmax annulla i contributi dei token futuri \blx@tocontentsinit {0}\cite {jm3}.\relax }}{59}{figure.caption.30}\protected@file@percent }
\newlabel{fig:qkt_masked}{{22}{59}{Mascheramento causale di $QK^\top $: la parte triangolare superiore (con $j>i$) è impostata a $-\infty $, così la softmax annulla i contributi dei token futuri \cite {jm3}.\relax }{figure.caption.30}{}}
\newlabel{fig:qkt_masked@cref}{{[figure][22][]22}{[1][58][]59}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.3.4}Schema completo per una testa (in parallelo)}{59}{subsubsection.12.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.3.5}Costo computazionale e dipendenza quadratica}{59}{subsubsection.12.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.3.6}Multi-head attention in parallelo}{59}{subsubsection.12.3.6}\protected@file@percent }
\newlabel{eq:QKV_per_head}{{70}{59}{Multi-head attention in parallelo}{equation.12.70}{}}
\newlabel{eq:QKV_per_head@cref}{{[equation][70][]70}{[1][59][]59}}
\newlabel{eq:multihead_parallel}{{71}{59}{Multi-head attention in parallelo}{equation.12.71}{}}
\newlabel{eq:multihead_parallel@cref}{{[equation][71][]71}{[1][59][]59}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Computazione parallela di una singola testa di self-attention: (i) proiezioni $Q,K,V$ da $X$; (ii) calcolo di $QK^\top $; (iii) maschera causale; (iv) (softmax non mostrata nello schema); (v) somma pesata tramite $V$ per ottenere l’output della testa \blx@tocontentsinit {0}\cite {jm3}.\relax }}{60}{figure.caption.31}\protected@file@percent }
\newlabel{fig:single_head_parallel}{{23}{60}{Computazione parallela di una singola testa di self-attention: (i) proiezioni $Q,K,V$ da $X$; (ii) calcolo di $QK^\top $; (iii) maschera causale; (iv) (softmax non mostrata nello schema); (v) somma pesata tramite $V$ per ottenere l’output della testa \cite {jm3}.\relax }{figure.caption.31}{}}
\newlabel{fig:single_head_parallel@cref}{{[figure][23][]23}{[1][59][]60}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.3.7}Il blocco Transformer in forma parallela}{60}{subsubsection.12.3.7}\protected@file@percent }
\newlabel{eq:block_parallel_O}{{72}{60}{Il blocco Transformer in forma parallela}{equation.12.72}{}}
\newlabel{eq:block_parallel_O@cref}{{[equation][72][]72}{[1][60][]60}}
\newlabel{eq:block_parallel_H}{{73}{60}{Il blocco Transformer in forma parallela}{equation.12.73}{}}
\newlabel{eq:block_parallel_H@cref}{{[equation][73][]73}{[1][60][]60}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.4}L'input del Transformer: embeddings di token e di posizione}{61}{subsection.12.4}\protected@file@percent }
\newlabel{subsec:input_token_position_embeddings}{{12.4}{61}{L'input del Transformer: embeddings di token e di posizione}{subsection.12.4}{}}
\newlabel{subsec:input_token_position_embeddings@cref}{{[subsection][4][12]12.4}{[1][60][]61}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.4.1}Token embeddings e matrice di embedding}{61}{subsubsection.12.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Selezione via one-hot (interpretazione equivalente).}{61}{section*.32}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Selezione dell’embedding del token $w_i$ a partire dalla matrice $E$ mediante un vettore one-hot: il prodotto \(\mathbf  {o}(w_i)^\top E\) restituisce la riga \(E[w_i]\) \blx@tocontentsinit {0}\cite {jm3}.\relax }}{62}{figure.caption.33}\protected@file@percent }
\newlabel{fig:select_single_embedding}{{24}{62}{Selezione dell’embedding del token $w_i$ a partire dalla matrice $E$ mediante un vettore one-hot: il prodotto \(\mathbf {o}(w_i)^\top E\) restituisce la riga \(E[w_i]\) \cite {jm3}.\relax }{figure.caption.33}{}}
\newlabel{fig:select_single_embedding@cref}{{[figure][24][]24}{[1][62][]62}}
\@writefile{toc}{\contentsline {paragraph}{Dalla sequenza alla matrice.}{62}{section*.34}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Selezione degli embeddings per un’intera sequenza: una matrice di one-hot \(O\) moltiplicata per \(E\) produce una matrice \(N\times d\) contenente gli embeddings lessicali della finestra di contesto \blx@tocontentsinit {0}\cite {jm3}.\relax }}{62}{figure.caption.35}\protected@file@percent }
\newlabel{fig:select_sequence_embedding}{{25}{62}{Selezione degli embeddings per un’intera sequenza: una matrice di one-hot \(O\) moltiplicata per \(E\) produce una matrice \(N\times d\) contenente gli embeddings lessicali della finestra di contesto \cite {jm3}.\relax }{figure.caption.35}{}}
\newlabel{fig:select_sequence_embedding@cref}{{[figure][25][]25}{[1][62][]62}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.4.2}Perché servono gli embeddings posizionali}{62}{subsubsection.12.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.4.3}Posizione assoluta e composizione dell’input}{63}{subsubsection.12.4.3}\protected@file@percent }
\newlabel{eq:composite_embedding}{{74}{63}{Posizione assoluta e composizione dell’input}{equation.12.74}{}}
\newlabel{eq:composite_embedding@cref}{{[equation][74][]74}{[1][63][]63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.5}Limiti della posizione assoluta e alternative: sinusoidale e posizione relativa}{63}{subsection.12.5}\protected@file@percent }
\newlabel{subsec:positional_alternatives}{{12.5}{63}{Limiti della posizione assoluta e alternative: sinusoidale e posizione relativa}{subsection.12.5}{}}
\newlabel{subsec:positional_alternatives@cref}{{[subsection][5][12]12.5}{[1][63][]63}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Composizione dell’input: l’embedding del token viene sommato all’embedding della posizione assoluta, producendo un vettore di input \(x_i\) in \(\mathbb  {R}^d\). L’insieme dei vettori forma la matrice \(X \in \mathbb  {R}^{N\times d}\) \blx@tocontentsinit {0}\cite {jm3}.\relax }}{64}{figure.caption.36}\protected@file@percent }
\newlabel{fig:token_plus_position}{{26}{64}{Composizione dell’input: l’embedding del token viene sommato all’embedding della posizione assoluta, producendo un vettore di input \(x_i\) in \(\mathbb {R}^d\). L’insieme dei vettori forma la matrice \(X \in \mathbb {R}^{N\times d}\) \cite {jm3}.\relax }{figure.caption.36}{}}
\newlabel{fig:token_plus_position@cref}{{[figure][26][]26}{[1][63][]64}}
\@writefile{toc}{\contentsline {paragraph}{Embeddings posizionali sinusoidali.}{64}{section*.37}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Posizione relativa.}{64}{section*.38}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.6}La \textit  {language modeling head}}{65}{subsection.12.6}\protected@file@percent }
\newlabel{subsec:lm_head}{{12.6}{65}{La \textit {language modeling head}}{subsection.12.6}{}}
\newlabel{subsec:lm_head@cref}{{[subsection][6][12]12.6}{[1][65][]65}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.6.1}Cosa entra e cosa esce: dal vettore $h_N^L$ alle probabilità sul vocabolario}{65}{subsubsection.12.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.6.2}Logits e livello di \textit  {unembedding}}{65}{subsubsection.12.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces La \textit  {language modeling head}: mappa l’output dell’ultimo token all’ultimo strato ($h_N^L$) in una distribuzione sul vocabolario tramite (i) un livello lineare (\emph  {unembedding}) e (ii) una softmax \blx@tocontentsinit {0}\cite {jm3}.\relax }}{66}{figure.caption.39}\protected@file@percent }
\newlabel{fig:lm_head_overview}{{27}{66}{La \textit {language modeling head}: mappa l’output dell’ultimo token all’ultimo strato ($h_N^L$) in una distribuzione sul vocabolario tramite (i) un livello lineare (\emph {unembedding}) e (ii) una softmax \cite {jm3}.\relax }{figure.caption.39}{}}
\newlabel{fig:lm_head_overview@cref}{{[figure][27][]27}{[1][65][]66}}
\newlabel{eq:logits_unembedding_general}{{75}{66}{Logits e livello di \textit {unembedding}}{equation.12.75}{}}
\newlabel{eq:logits_unembedding_general@cref}{{[equation][75][]75}{[1][66][]66}}
\@writefile{toc}{\contentsline {paragraph}{Weight tying: perché spesso $U=E^\top $.}{66}{section*.40}\protected@file@percent }
\newlabel{eq:logits_weight_tying}{{76}{66}{Weight tying: perché spesso $U=E^\top $}{equation.12.76}{}}
\newlabel{eq:logits_weight_tying@cref}{{[equation][76][]76}{[1][66][]66}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.6.3}Softmax: da logits a probabilità}{67}{subsubsection.12.6.3}\protected@file@percent }
\newlabel{eq:softmax_probs}{{77}{67}{Softmax: da logits a probabilità}{equation.12.77}{}}
\newlabel{eq:softmax_probs@cref}{{[equation][77][]77}{[1][66][]67}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.6.4}Dal modello alla generazione: scegliere il prossimo token}{67}{subsubsection.12.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.6.5}Visione d’insieme: un \textit  {decoder-only} che impila blocchi}{67}{subsubsection.12.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.7}Nota: \textit  {logit lens} e terminologia \textit  {decoder-only}}{67}{subsection.12.7}\protected@file@percent }
\newlabel{subsec:logit_lens_decoder_only}{{12.7}{67}{Nota: \textit {logit lens} e terminologia \textit {decoder-only}}{subsection.12.7}{}}
\newlabel{subsec:logit_lens_decoder_only@cref}{{[subsection][7][12]12.7}{[1][67][]67}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Un Transformer per language modeling (\textit  {decoder-only}): impila blocchi Transformer e usa la language modeling head per mappare \(h_i^L\) in una distribuzione sul prossimo token \(w_{i+1}\) \blx@tocontentsinit {0}\cite {jm3}.\relax }}{68}{figure.caption.41}\protected@file@percent }
\newlabel{fig:decoder_only_stack}{{28}{68}{Un Transformer per language modeling (\textit {decoder-only}): impila blocchi Transformer e usa la language modeling head per mappare \(h_i^L\) in una distribuzione sul prossimo token \(w_{i+1}\) \cite {jm3}.\relax }{figure.caption.41}{}}
\newlabel{fig:decoder_only_stack@cref}{{[figure][28][]28}{[1][67][]68}}
\@writefile{toc}{\contentsline {paragraph}{Nota terminologica: \textit  {decoder-only}.}{69}{section*.42}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13}Large Language Models}{69}{section.13}\protected@file@percent }
\newlabel{sec:llm}{{13}{69}{Large Language Models}{section.13}{}}
\newlabel{sec:llm@cref}{{[section][13][]13}{[1][69][]69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1}Large Language Models con Transformer: generazione condizionata}{70}{subsection.13.1}\protected@file@percent }
\newlabel{subsec:llm_with_transformers}{{13.1}{70}{Large Language Models con Transformer: generazione condizionata}{subsection.13.1}{}}
\newlabel{subsec:llm_with_transformers@cref}{{[subsection][1][13]13.1}{[1][70][]70}}
\@writefile{toc}{\contentsline {paragraph}{Perché “predire parole” è utile per tanti task.}{70}{section*.44}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Completamento autoregressivo left-to-right: i token generati vengono riaggiunti al contesto e diventano parte del prefisso per la predizione successiva.\relax }}{71}{figure.caption.43}\protected@file@percent }
\newlabel{fig:llm_completion}{{29}{71}{Completamento autoregressivo left-to-right: i token generati vengono riaggiunti al contesto e diventano parte del prefisso per la predizione successiva.\relax }{figure.caption.43}{}}
\newlabel{fig:llm_completion@cref}{{[figure][29][]29}{[1][70][]71}}
\@writefile{toc}{\contentsline {paragraph}{Esempio: riassunto come generazione condizionata.}{71}{section*.45}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Nota sul passo successivo (ponte verso BERT).}{71}{section*.48}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Esempio di testo e relativo riassunto (scenario tipico di summarization). Il compito può essere visto come generazione condizionata sul documento.\relax }}{72}{figure.caption.46}\protected@file@percent }
\newlabel{fig:summarization_example}{{30}{72}{Esempio di testo e relativo riassunto (scenario tipico di summarization). Il compito può essere visto come generazione condizionata sul documento.\relax }{figure.caption.46}{}}
\newlabel{fig:summarization_example@cref}{{[figure][30][]30}{[1][71][]72}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Riassunto tramite prompting: il token (o stringa) \texttt  {tl;dr} agisce da segnale che innesca la generazione di una sintesi, sfruttando la lunga finestra di contesto del Transformer.\relax }}{72}{figure.caption.47}\protected@file@percent }
\newlabel{fig:tldr_summarization}{{31}{72}{Riassunto tramite prompting: il token (o stringa) \texttt {tl;dr} agisce da segnale che innesca la generazione di una sintesi, sfruttando la lunga finestra di contesto del Transformer.\relax }{figure.caption.47}{}}
\newlabel{fig:tldr_summarization@cref}{{[figure][31][]31}{[1][71][]72}}
\@writefile{toc}{\contentsline {section}{\numberline {14}Sampling per la generazione con LLM}{73}{section.14}\protected@file@percent }
\newlabel{sec:llm-sampling}{{14}{73}{Sampling per la generazione con LLM}{section.14}{}}
\newlabel{sec:llm-sampling@cref}{{[section][14][]14}{[1][73][]73}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.1}Perché non basta il campionamento ``puro''}{73}{subsection.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2}Top-$k$ sampling}{74}{subsection.14.2}\protected@file@percent }
\newlabel{subsec:topk}{{14.2}{74}{Top-$k$ sampling}{subsection.14.2}{}}
\newlabel{subsec:topk@cref}{{[subsection][2][14]14.2}{[1][74][]74}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3}Top-$p$ (nucleus) sampling}{74}{subsection.14.3}\protected@file@percent }
\newlabel{subsec:topp}{{14.3}{74}{Top-$p$ (nucleus) sampling}{subsection.14.3}{}}
\newlabel{subsec:topp@cref}{{[subsection][3][14]14.3}{[1][74][]74}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4}Temperature sampling}{75}{subsection.14.4}\protected@file@percent }
\newlabel{subsec:temperature}{{14.4}{75}{Temperature sampling}{subsection.14.4}{}}
\newlabel{subsec:temperature@cref}{{[subsection][4][14]14.4}{[1][75][]75}}
\@writefile{toc}{\contentsline {section}{\numberline {15}Pretraining dei Large Language Models}{75}{section.15}\protected@file@percent }
\newlabel{sec:llm-pretraining}{{15}{75}{Pretraining dei Large Language Models}{section.15}{}}
\newlabel{sec:llm-pretraining@cref}{{[section][15][]15}{[1][75][]75}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.1}Setup, notazione e obiettivo di language modeling}{76}{subsection.15.1}\protected@file@percent }
\newlabel{subsec:llm-setup}{{15.1}{76}{Setup, notazione e obiettivo di language modeling}{subsection.15.1}{}}
\newlabel{subsec:llm-setup@cref}{{[subsection][1][15]15.1}{[1][76][]76}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.2}Self-supervision e funzione obiettivo}{76}{subsection.15.2}\protected@file@percent }
\newlabel{subsec:self-supervision-obj}{{15.2}{76}{Self-supervision e funzione obiettivo}{subsection.15.2}{}}
\newlabel{subsec:self-supervision-obj@cref}{{[subsection][2][15]15.2}{[1][76][]76}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3}Teacher forcing}{77}{subsection.15.3}\protected@file@percent }
\newlabel{subsec:teacher-forcing}{{15.3}{77}{Teacher forcing}{subsection.15.3}{}}
\newlabel{subsec:teacher-forcing@cref}{{[subsection][3][15]15.3}{[1][77][]77}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces Addestramento di un transformer come language model: per ogni posizione si predice il token successivo e si calcola una loss di cross-entropy; la loss totale è la media (o somma) sulle posizioni.\relax }}{78}{figure.caption.49}\protected@file@percent }
\newlabel{fig:train-lm}{{32}{78}{Addestramento di un transformer come language model: per ogni posizione si predice il token successivo e si calcola una loss di cross-entropy; la loss totale è la media (o somma) sulle posizioni.\relax }{figure.caption.49}{}}
\newlabel{fig:train-lm@cref}{{[figure][32][]32}{[1][77][]78}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4}Efficienza computazionale: parallelismo nei transformer}{78}{subsection.15.4}\protected@file@percent }
\newlabel{subsec:parallelism}{{15.4}{78}{Efficienza computazionale: parallelismo nei transformer}{subsection.15.4}{}}
\newlabel{subsec:parallelism@cref}{{[subsection][4][15]15.4}{[1][78][]78}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces Esempio di composizione di un grande corpus di pretraining (treemap): diverse sorgenti contribuiscono con proporzioni differenti.\relax }}{79}{figure.caption.50}\protected@file@percent }
\newlabel{fig:pile}{{33}{79}{Esempio di composizione di un grande corpus di pretraining (treemap): diverse sorgenti contribuiscono con proporzioni differenti.\relax }{figure.caption.50}{}}
\newlabel{fig:pile@cref}{{[figure][33][]33}{[1][79][]79}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.5}Dati di pretraining: fonti e filtraggio}{79}{subsection.15.5}\protected@file@percent }
\newlabel{subsec:data}{{15.5}{79}{Dati di pretraining: fonti e filtraggio}{subsection.15.5}{}}
\newlabel{subsec:data@cref}{{[subsection][5][15]15.5}{[1][79][]79}}
\@writefile{toc}{\contentsline {paragraph}{Filtri di qualità e sicurezza.}{80}{section*.51}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Aspetti etici e legali (panoramica).}{80}{section*.52}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.6}Dal pretraining all'adattamento: finetuning}{80}{subsection.15.6}\protected@file@percent }
\newlabel{subsec:finetune}{{15.6}{80}{Dal pretraining all'adattamento: finetuning}{subsection.15.6}{}}
\newlabel{subsec:finetune@cref}{{[subsection][6][15]15.6}{[1][80][]80}}
\@writefile{toc}{\contentsline {paragraph}{Tipi di adattamento (senza anticipare modelli specifici).}{80}{section*.54}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Schema concettuale: pretraining su grandi corpora $\rightarrow $ modello generale $\rightarrow $ adattamento su dati specifici (finetuning) $\rightarrow $ modello specializzato.\relax }}{81}{figure.caption.53}\protected@file@percent }
\newlabel{fig:pretrain-finetune}{{34}{81}{Schema concettuale: pretraining su grandi corpora $\rightarrow $ modello generale $\rightarrow $ adattamento su dati specifici (finetuning) $\rightarrow $ modello specializzato.\relax }{figure.caption.53}{}}
\newlabel{fig:pretrain-finetune@cref}{{[figure][34][]34}{[1][80][]81}}
\@writefile{toc}{\contentsline {paragraph}{Collegamento alle sezioni successive.}{81}{section*.55}\protected@file@percent }
\@setckpt{chapters/03_embeddings}{
\setcounter{page}{82}
\setcounter{equation}{78}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{section}{15}
\setcounter{subsection}{6}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{34}
\setcounter{table}{2}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{lstnumber}{1}
\setcounter{parentequation}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{89}
\setcounter{maxnames}{1}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{blx@maxsegment@0}{0}
\setcounter{blx@sectionciteorder@0}{89}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{Item}{35}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{93}
\setcounter{FancyVerbLine}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{23}
\setcounter{definition}{0}
\setcounter{lstlisting}{0}
\setcounter{section@level}{4}
}
