\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {3}Embeddings}{18}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Introduzione}{19}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}L'ipotesi distribuzionale}{19}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Ipotesi di Osgood}{20}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Embeddings}{21}{section.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Classificazione delle principali tipologie di embeddings trattate nel capitolo.\relax }}{22}{figure.caption.11}\protected@file@percent }
\newlabel{fig:classificazione_embeddings}{{8}{22}{Classificazione delle principali tipologie di embeddings trattate nel capitolo.\relax }{figure.caption.11}{}}
\newlabel{fig:classificazione_embeddings@cref}{{[figure][8][]8}{[1][21][]22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Embeddings count-based}{22}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}Matrice termine-documento}{22}{subsubsection.7.1.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Term-document matrix per quattro parole in quattro opere di Shakespeare. Ogni cella contiene il numero di occorrenze della parola (riga) nel documento (colonna). \blx@tocontentsinit {0}\cite {wang2024disentangledrepresentationlearning}\relax }}{23}{table.caption.12}\protected@file@percent }
\newlabel{tab:term_document_shakespeare}{{1}{23}{Term-document matrix per quattro parole in quattro opere di Shakespeare. Ogni cella contiene il numero di occorrenze della parola (riga) nel documento (colonna). \cite {wang2024disentangledrepresentationlearning}\relax }{table.caption.12}{}}
\newlabel{tab:term_document_shakespeare@cref}{{[table][1][]1}{[1][22][]23}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.2}Matrice termine-termine}{23}{subsubsection.7.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Estratto di una matrice termine--termine calcolata sul corpus Wikipedia. Ogni cella contiene il numero di co-occorrenze tra la parola target (riga) e la parola di contesto (colonna) all’interno di una finestra di contesto locale \blx@tocontentsinit {0}\cite {wang2024disentangledrepresentationlearning}.\relax }}{24}{table.caption.13}\protected@file@percent }
\newlabel{tab:term_term_wikipedia}{{2}{24}{Estratto di una matrice termine--termine calcolata sul corpus Wikipedia. Ogni cella contiene il numero di co-occorrenze tra la parola target (riga) e la parola di contesto (colonna) all’interno di una finestra di contesto locale \cite {wang2024disentangledrepresentationlearning}.\relax }{table.caption.13}{}}
\newlabel{tab:term_term_wikipedia@cref}{{[table][2][]2}{[1][24][]24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Riduzione dimensionale tramite SVD}{25}{subsection.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Cosine Similarity}{26}{subsection.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Word2Vec: un approccio predittivo}{27}{subsection.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.1}Il classificatore e la funzione sigmoide}{27}{subsubsection.7.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.2}Apprendimento e Negative Sampling}{28}{subsubsection.7.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.3}Perché due matrici? Il ruolo di $W$ e $C$}{28}{subsubsection.7.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Lo skip-gram apprende in totale due insiemi di embedding, uno per i target ($W$) e uno per i contesti ($C$), per un totale di $2|V|$ vettori. L’addestramento mira a massimizzare la probabilità che parole vicine nel testo abbiano vettori simili.\relax }}{29}{figure.caption.14}\protected@file@percent }
\newlabel{fig:skipgram_structure}{{9}{29}{Lo skip-gram apprende in totale due insiemi di embedding, uno per i target ($W$) e uno per i contesti ($C$), per un totale di $2|V|$ vettori. L’addestramento mira a massimizzare la probabilità che parole vicine nel testo abbiano vettori simili.\relax }{figure.caption.14}{}}
\newlabel{fig:skipgram_structure@cref}{{[figure][9][]9}{[1][29][]29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Proprietà semantiche degli embeddings}{29}{subsection.7.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Rappresentazione geometrica del modello del parallelogramma applicato all'analogia di genere.\relax }}{31}{figure.caption.15}\protected@file@percent }
\newlabel{fig:parallelogramma}{{10}{31}{Rappresentazione geometrica del modello del parallelogramma applicato all'analogia di genere.\relax }{figure.caption.15}{}}
\newlabel{fig:parallelogramma@cref}{{[figure][10][]10}{[1][30][]31}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Embeddings dinamici}{31}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Reti Neurali Ricorrenti}{32}{section.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Illustrazione del funzionamento di una RNN. Ad ogni passo l'informazione contestuale viene consegnata a quello successivo.\relax }}{33}{figure.caption.16}\protected@file@percent }
\newlabel{fig:rnn_flow}{{11}{33}{Illustrazione del funzionamento di una RNN. Ad ogni passo l'informazione contestuale viene consegnata a quello successivo.\relax }{figure.caption.16}{}}
\newlabel{fig:rnn_flow@cref}{{[figure][11][]11}{[1][33][]33}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Rappresentazione srotolata (unrolled) di una RNN. Le frecce orizzontali mostrano il passaggio dello stato nascosto attraverso il tempo (pesi $\mathbf  {U}$), mentre quelle verticali indicano l'elaborazione dell'input ($\mathbf  {W}$) e la generazione dell'output ($\mathbf  {V}$).\relax }}{34}{figure.caption.17}\protected@file@percent }
\newlabel{fig:rnn_unrolled}{{12}{34}{Rappresentazione srotolata (unrolled) di una RNN. Le frecce orizzontali mostrano il passaggio dello stato nascosto attraverso il tempo (pesi $\mathbf {U}$), mentre quelle verticali indicano l'elaborazione dell'input ($\mathbf {W}$) e la generazione dell'output ($\mathbf {V}$).\relax }{figure.caption.17}{}}
\newlabel{fig:rnn_unrolled@cref}{{[figure][12][]12}{[1][34][]34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}RNN come Language Models}{34}{subsection.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Generaizone di Embeddings tramite RNN}{35}{subsection.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}RNN Bidirezionali (Bi-RNN)}{36}{subsection.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Il problema del Gradiente Svanente}{36}{subsection.9.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}LSTM: Long Short-Term Memory}{37}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Meccanismi di Gating}{38}{subsection.10.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Rappresentazione di una singola unità LSTM come grafo computazionale. Gli input consistono nell'input attuale $x_t$, lo stato nascosto precedente $h_{t-1}$ e il contesto precedente $c_{t-1}$. Gli output sono il nuovo stato nascosto $h_t$ e il contesto aggiornato $c_t$ \blx@tocontentsinit {0}\cite {jm3}.\relax }}{38}{figure.caption.18}\protected@file@percent }
\newlabel{fig:lstm_unit}{{13}{38}{Rappresentazione di una singola unità LSTM come grafo computazionale. Gli input consistono nell'input attuale $x_t$, lo stato nascosto precedente $h_{t-1}$ e il contesto precedente $c_{t-1}$. Gli output sono il nuovo stato nascosto $h_t$ e il contesto aggiornato $c_t$ \cite {jm3}.\relax }{figure.caption.18}{}}
\newlabel{fig:lstm_unit@cref}{{[figure][13][]13}{[1][37][]38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Le equazioni del modello}{38}{subsection.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Modularità ed Embeddings}{39}{subsection.10.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Architettura Encoder-Decoder e limite del \textit  {bottleneck}}{39}{section.11}\protected@file@percent }
\newlabel{sec:encoder_decoder_rnn}{{11}{39}{Architettura Encoder-Decoder e limite del \textit {bottleneck}}{section.11}{}}
\newlabel{sec:encoder_decoder_rnn@cref}{{[section][11][]11}{[1][39][]39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1}Il problema \textit  {sequence-to-sequence}}{40}{subsection.11.1}\protected@file@percent }
\newlabel{eq:seq2seq_chain_rule}{{41}{40}{Il problema \textit {sequence-to-sequence}}{equation.11.41}{}}
\newlabel{eq:seq2seq_chain_rule@cref}{{[equation][41][]41}{[1][40][]40}}
\newlabel{eq:encoder_recurrence}{{42}{40}{Il problema \textit {sequence-to-sequence}}{equation.11.42}{}}
\newlabel{eq:encoder_recurrence@cref}{{[equation][42][]42}{[1][40][]40}}
\newlabel{eq:context_vector}{{43}{40}{Il problema \textit {sequence-to-sequence}}{equation.11.43}{}}
\newlabel{eq:context_vector@cref}{{[equation][43][]43}{[1][40][]40}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Schema formale dell'encoder--decoder ricorrente: l'encoder produce una sequenza di stati $h^{(e)}_1,\dots  ,h^{(e)}_n$ e il suo stato finale $h^{(e)}_n$ viene identificato con il vettore di contesto $c$, usato per inizializzare il decoder ($h^{(d)}_0$) e, nella variante mostrata, reso disponibile a ogni passo di decodifica. Questa dipendenza da un unico vettore di contesto anticipa il problema del \textit  {bottleneck} discusso in seguito. Adattata da \blx@tocontentsinit {0}\cite {jm3}.\relax }}{41}{figure.caption.19}\protected@file@percent }
\newlabel{fig:seq2seq_formal}{{14}{41}{Schema formale dell'encoder--decoder ricorrente: l'encoder produce una sequenza di stati $h^{(e)}_1,\dots ,h^{(e)}_n$ e il suo stato finale $h^{(e)}_n$ viene identificato con il vettore di contesto $c$, usato per inizializzare il decoder ($h^{(d)}_0$) e, nella variante mostrata, reso disponibile a ogni passo di decodifica. Questa dipendenza da un unico vettore di contesto anticipa il problema del \textit {bottleneck} discusso in seguito. Adattata da \cite {jm3}.\relax }{figure.caption.19}{}}
\newlabel{fig:seq2seq_formal@cref}{{[figure][14][]14}{[1][40][]41}}
\newlabel{eq:decoder_recurrence}{{44}{41}{Il problema \textit {sequence-to-sequence}}{equation.11.44}{}}
\newlabel{eq:decoder_recurrence@cref}{{[equation][44][]44}{[1][40][]41}}
\newlabel{eq:decoder_softmax}{{45}{41}{Il problema \textit {sequence-to-sequence}}{equation.11.45}{}}
\newlabel{eq:decoder_softmax@cref}{{[equation][45][]45}{[1][40][]41}}
\newlabel{eq:decoder_init}{{46}{41}{Il problema \textit {sequence-to-sequence}}{equation.11.46}{}}
\newlabel{eq:decoder_init@cref}{{[equation][46][]46}{[1][41][]41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2}Il limite del \textit  {bottleneck} informativo}{42}{subsection.11.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Schema Encoder--Decoder: quando il contesto $c$ coincide con il solo stato nascosto finale dell'encoder, tutta l'informazione della sequenza sorgente deve attraversare un \emph  {collo di bottiglia} rappresentazionale prima di essere utilizzata dal decoder.\relax }}{42}{figure.caption.20}\protected@file@percent }
\newlabel{fig:encoder_decoder_bottleneck}{{15}{42}{Schema Encoder--Decoder: quando il contesto $c$ coincide con il solo stato nascosto finale dell'encoder, tutta l'informazione della sequenza sorgente deve attraversare un \emph {collo di bottiglia} rappresentazionale prima di essere utilizzata dal decoder.\relax }{figure.caption.20}{}}
\newlabel{fig:encoder_decoder_bottleneck@cref}{{[figure][15][]15}{[1][42][]42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3}Soluzione al bottleneck: Meccanismo dell'attenzione}{43}{subsection.11.3}\protected@file@percent }
\newlabel{subsec:attention}{{11.3}{43}{Soluzione al bottleneck: Meccanismo dell'attenzione}{subsection.11.3}{}}
\newlabel{subsec:attention@cref}{{[subsection][3][11]11.3}{[1][43][]43}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Nel meccanismo di attenzione, a ciascun passo di decodifica $i$ il decoder utilizza un contesto \emph  {dinamico} $c_i$ (diverso per ogni token generato), calcolato come funzione di tutti gli stati nascosti dell'encoder. Adattata da \blx@tocontentsinit {0}\cite {jm3}.\relax }}{43}{figure.caption.21}\protected@file@percent }
\newlabel{fig:attention_dynamic_context}{{16}{43}{Nel meccanismo di attenzione, a ciascun passo di decodifica $i$ il decoder utilizza un contesto \emph {dinamico} $c_i$ (diverso per ogni token generato), calcolato come funzione di tutti gli stati nascosti dell'encoder. Adattata da \cite {jm3}.\relax }{figure.caption.21}{}}
\newlabel{fig:attention_dynamic_context@cref}{{[figure][16][]16}{[1][43][]43}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Schema encoder--decoder con attenzione, focalizzato sul calcolo di $c_i$. Per ogni stato precedente del decoder $h^{(d)}_{i-1}$ si calcola un punteggio di rilevanza rispetto a ciascuno stato dell'encoder $h^{(e)}_j$; i punteggi vengono normalizzati in pesi $\alpha _{ij}$ e usati per ottenere $c_i$ come somma pesata degli stati dell'encoder. Adattata da \blx@tocontentsinit {0}\cite {jm3}.\relax }}{44}{figure.caption.22}\protected@file@percent }
\newlabel{fig:attention_ci_computation}{{17}{44}{Schema encoder--decoder con attenzione, focalizzato sul calcolo di $c_i$. Per ogni stato precedente del decoder $h^{(d)}_{i-1}$ si calcola un punteggio di rilevanza rispetto a ciascuno stato dell'encoder $h^{(e)}_j$; i punteggi vengono normalizzati in pesi $\alpha _{ij}$ e usati per ottenere $c_i$ come somma pesata degli stati dell'encoder. Adattata da \cite {jm3}.\relax }{figure.caption.22}{}}
\newlabel{fig:attention_ci_computation@cref}{{[figure][17][]17}{[1][43][]44}}
\newlabel{eq:decoder_with_attention_jm}{{47}{44}{Soluzione al bottleneck: Meccanismo dell'attenzione}{equation.11.47}{}}
\newlabel{eq:decoder_with_attention_jm@cref}{{[equation][47][]47}{[1][44][]44}}
\newlabel{eq:attention_score_general}{{48}{45}{Soluzione al bottleneck: Meccanismo dell'attenzione}{equation.11.48}{}}
\newlabel{eq:attention_score_general@cref}{{[equation][48][]48}{[1][44][]45}}
\newlabel{eq:attention_dot_score}{{49}{45}{Soluzione al bottleneck: Meccanismo dell'attenzione}{equation.11.49}{}}
\newlabel{eq:attention_dot_score@cref}{{[equation][49][]49}{[1][45][]45}}
\newlabel{eq:attention_weights_softmax}{{50}{45}{Soluzione al bottleneck: Meccanismo dell'attenzione}{equation.11.50}{}}
\newlabel{eq:attention_weights_softmax@cref}{{[equation][50][]50}{[1][45][]45}}
\newlabel{eq:attention_context_weighted_sum}{{51}{45}{Soluzione al bottleneck: Meccanismo dell'attenzione}{equation.11.51}{}}
\newlabel{eq:attention_context_weighted_sum@cref}{{[equation][51][]51}{[1][45][]45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4}Verso i Transformer}{45}{subsection.11.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12}Il Transformer}{46}{section.12}\protected@file@percent }
\newlabel{sec:transformer}{{12}{46}{Il Transformer}{section.12}{}}
\newlabel{sec:transformer@cref}{{[section][12][]12}{[1][46][]46}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Schema di un Transformer causale (left-to-right) per language modeling. Ogni token in input viene codificato (embedding del token e della posizione), processato da una pila di blocchi Transformer, e infine proiettato tramite una testa di language modeling per predire il token successivo.\relax }}{46}{figure.caption.23}\protected@file@percent }
\newlabel{fig:transformer_overview}{{18}{46}{Schema di un Transformer causale (left-to-right) per language modeling. Ogni token in input viene codificato (embedding del token e della posizione), processato da una pila di blocchi Transformer, e infine proiettato tramite una testa di language modeling per predire il token successivo.\relax }{figure.caption.23}{}}
\newlabel{fig:transformer_overview@cref}{{[figure][18][]18}{[1][46][]46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1}Self-attention}{48}{subsection.12.1}\protected@file@percent }
\newlabel{subsec:self_attention}{{12.1}{48}{Self-attention}{subsection.12.1}{}}
\newlabel{subsec:self_attention@cref}{{[subsection][1][12]12.1}{[1][48][]48}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Esempio di distribuzione dei pesi di self-attention $\alpha $: per costruire una rappresentazione contestuale del token \emph  {it} in uno strato superiore, il modello attribuisce peso maggiore ad alcuni token precedenti particolarmente informativi (ad es.\ \emph  {chicken} e \emph  {road}). In quel punto della sequenza, infatti, la coreferenza del pronome non è ancora disambiguata; è quindi plausibile che la rappresentazione di \emph  {it} debba incorporare evidenza proveniente da entrambe le possibili entità.\relax }}{48}{figure.caption.24}\protected@file@percent }
\newlabel{fig:selfattn_alpha}{{19}{48}{Esempio di distribuzione dei pesi di self-attention $\alpha $: per costruire una rappresentazione contestuale del token \emph {it} in uno strato superiore, il modello attribuisce peso maggiore ad alcuni token precedenti particolarmente informativi (ad es.\ \emph {chicken} e \emph {road}). In quel punto della sequenza, infatti, la coreferenza del pronome non è ancora disambiguata; è quindi plausibile che la rappresentazione di \emph {it} debba incorporare evidenza proveniente da entrambe le possibili entità.\relax }{figure.caption.24}{}}
\newlabel{fig:selfattn_alpha@cref}{{[figure][19][]19}{[1][48][]48}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.1.1}Motivazione: dalle rappresentazioni statiche alle rappresentazioni contestuali}{48}{subsubsection.12.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Self-attention causale (left-to-right): in posizione $i$ il modello combina esclusivamente i token nelle posizioni $j\le i$ e, tramite mascheramento, non può incorporare informazione proveniente da posizioni future.\relax }}{49}{figure.caption.25}\protected@file@percent }
\newlabel{fig:selfattn_flow}{{20}{49}{Self-attention causale (left-to-right): in posizione $i$ il modello combina esclusivamente i token nelle posizioni $j\le i$ e, tramite mascheramento, non può incorporare informazione proveniente da posizioni future.\relax }{figure.caption.25}{}}
\newlabel{fig:selfattn_flow@cref}{{[figure][20][]20}{[1][48][]49}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.1.2}Self-attention causale: dominio informativo e vincolo di autoregressione}{50}{subsubsection.12.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.1.3}Intuizione: self-attention come combinazione pesata del contesto}{50}{subsubsection.12.1.3}\protected@file@percent }
\newlabel{eq:selfattn_weighted_sum}{{52}{50}{Intuizione: self-attention come combinazione pesata del contesto}{equation.12.52}{}}
\newlabel{eq:selfattn_weighted_sum@cref}{{[equation][52][]52}{[1][50][]50}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.1.4}Scaled dot-product attention: ruoli di query, key e value}{51}{subsubsection.12.1.4}\protected@file@percent }
\newlabel{eq:qkv}{{53}{51}{Scaled dot-product attention: ruoli di query, key e value}{equation.12.53}{}}
\newlabel{eq:qkv@cref}{{[equation][53][]53}{[1][51][]51}}
\@writefile{toc}{\contentsline {paragraph}{Punteggi di compatibilità (scores).}{51}{section*.26}\protected@file@percent }
\newlabel{eq:score_scaled}{{54}{51}{Punteggi di compatibilità (scores)}{equation.12.54}{}}
\newlabel{eq:score_scaled@cref}{{[equation][54][]54}{[1][51][]51}}
\@writefile{toc}{\contentsline {paragraph}{Normalizzazione tramite softmax e vincolo causale.}{52}{section*.27}\protected@file@percent }
\newlabel{eq:alpha_softmax}{{55}{52}{Normalizzazione tramite softmax e vincolo causale}{equation.12.55}{}}
\newlabel{eq:alpha_softmax@cref}{{[equation][55][]55}{[1][51][]52}}
\@writefile{toc}{\contentsline {paragraph}{Aggregazione dei value e proiezione in output.}{52}{section*.28}\protected@file@percent }
\newlabel{eq:head_output}{{56}{52}{Aggregazione dei value e proiezione in output}{equation.12.56}{}}
\newlabel{eq:head_output@cref}{{[equation][56][]56}{[1][52][]52}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.1.5}Forma matriciale e dimensioni (utile per l’implementazione)}{52}{subsubsection.12.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Calcolo di una singola testa di self-attention causale: (i) proiezioni in query/key/value; (ii) punteggi di compatibilità via dot-product scalato; (iii) normalizzazione in pesi $\alpha _{ij}$; (iv) somma pesata dei value per ottenere $\mathrm  {head}_i$; (v) proiezione finale per ottenere $a_i$ in dimensione del modello.\relax }}{53}{figure.caption.29}\protected@file@percent }
\newlabel{fig:selfattn_head}{{21}{53}{Calcolo di una singola testa di self-attention causale: (i) proiezioni in query/key/value; (ii) punteggi di compatibilità via dot-product scalato; (iii) normalizzazione in pesi $\alpha _{ij}$; (iv) somma pesata dei value per ottenere $\mathrm {head}_i$; (v) proiezione finale per ottenere $a_i$ in dimensione del modello.\relax }{figure.caption.29}{}}
\newlabel{fig:selfattn_head@cref}{{[figure][21][]21}{[1][52][]53}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.1.6}Multi-head attention: pluralità di criteri di selezione}{54}{subsubsection.12.1.6}\protected@file@percent }
\newlabel{eq:qkv_multihead}{{57}{54}{Multi-head attention: pluralità di criteri di selezione}{equation.12.57}{}}
\newlabel{eq:qkv_multihead@cref}{{[equation][57][]57}{[1][54][]54}}
\newlabel{eq:multihead_concat}{{58}{54}{Multi-head attention: pluralità di criteri di selezione}{equation.12.58}{}}
\newlabel{eq:multihead_concat@cref}{{[equation][58][]58}{[1][54][]54}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.1.7}Osservazione conclusiva: self-attention e contestualizzazione progressiva}{54}{subsubsection.12.1.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Multi-head attention: più teste calcolano in parallelo combinazioni pesate del contesto con parametri indipendenti; gli output vengono concatenati e proiettati per ottenere un vettore finale nella stessa dimensionalità dell’input.\relax }}{55}{figure.caption.30}\protected@file@percent }
\newlabel{fig:multihead}{{22}{55}{Multi-head attention: più teste calcolano in parallelo combinazioni pesate del contesto con parametri indipendenti; gli output vengono concatenati e proiettati per ottenere un vettore finale nella stessa dimensionalità dell’input.\relax }{figure.caption.30}{}}
\newlabel{fig:multihead@cref}{{[figure][22][]22}{[1][54][]55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2}Blocco Transformer}{55}{subsection.12.2}\protected@file@percent }
\newlabel{subsec:transformer_block}{{12.2}{55}{Blocco Transformer}{subsection.12.2}{}}
\newlabel{subsec:transformer_block@cref}{{[subsection][2][12]12.2}{[1][55][]55}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.2.1}Input del blocco e informazione di posizione}{56}{subsubsection.12.2.1}\protected@file@percent }
\newlabel{eq:token_plus_positional}{{59}{56}{Input del blocco e informazione di posizione}{equation.12.59}{}}
\newlabel{eq:token_plus_positional@cref}{{[equation][59][]59}{[1][56][]56}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.2.2}Residual stream: il flusso informativo del token}{56}{subsubsection.12.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Architettura di un blocco Transformer nella variante \emph  {prenorm} e interpretazione tramite \emph  {residual stream}: ciascun modulo legge dallo stream del token e aggiunge il proprio output allo stesso stream tramite una somma residua \blx@tocontentsinit {0}\cite {jm3}.\relax }}{57}{figure.caption.31}\protected@file@percent }
\newlabel{fig:transformer_block_residual_stream}{{23}{57}{Architettura di un blocco Transformer nella variante \emph {prenorm} e interpretazione tramite \emph {residual stream}: ciascun modulo legge dallo stream del token e aggiunge il proprio output allo stesso stream tramite una somma residua \cite {jm3}.\relax }{figure.caption.31}{}}
\newlabel{fig:transformer_block_residual_stream@cref}{{[figure][23][]23}{[1][56][]57}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.2.3}Perché la LayerNorm (motivazione)}{58}{subsubsection.12.2.3}\protected@file@percent }
\newlabel{eq:layernorm}{{61}{58}{Perché la LayerNorm (motivazione)}{equation.12.61}{}}
\newlabel{eq:layernorm@cref}{{[equation][61][]61}{[1][58][]58}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.2.4}Feedforward network (ruolo)}{58}{subsubsection.12.2.4}\protected@file@percent }
\newlabel{eq:ffn}{{62}{58}{Feedforward network (ruolo)}{equation.12.62}{}}
\newlabel{eq:ffn@cref}{{[equation][62][]62}{[1][58][]58}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.2.5}Equazioni del blocco (variante \emph  {prenorm})}{59}{subsubsection.12.2.5}\protected@file@percent }
\newlabel{eq:transformer_block_prenorm}{{68}{59}{Equazioni del blocco (variante \emph {prenorm})}{equation.12.68}{}}
\newlabel{eq:transformer_block_prenorm@cref}{{[equation][68][]68}{[1][59][]59}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.2.6}L’attenzione come “movimento” di informazione tra stream}{59}{subsubsection.12.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3}Parallelizzazione del calcolo con una singola matrice $X$}{59}{subsection.12.3}\protected@file@percent }
\newlabel{subsec:parallelizing_with_X}{{12.3}{59}{Parallelizzazione del calcolo con una singola matrice $X$}{subsection.12.3}{}}
\newlabel{subsec:parallelizing_with_X@cref}{{[subsection][3][12]12.3}{[1][59][]59}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Interpretazione della self-attention: una testa può spostare informazione dal residual stream del token $A$ a quello del token $B$, integrando in $B$ contenuto recuperato da altre posizioni \blx@tocontentsinit {0}\cite {jm3}.\relax }}{60}{figure.caption.32}\protected@file@percent }
\newlabel{fig:attention_moves_info}{{24}{60}{Interpretazione della self-attention: una testa può spostare informazione dal residual stream del token $A$ a quello del token $B$, integrando in $B$ contenuto recuperato da altre posizioni \cite {jm3}.\relax }{figure.caption.32}{}}
\newlabel{fig:attention_moves_info@cref}{{[figure][24][]24}{[1][59][]60}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.3.1}Impacchettare la sequenza in una matrice}{60}{subsubsection.12.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.3.2}Self-attention in forma matriciale (una testa)}{60}{subsubsection.12.3.2}\protected@file@percent }
\newlabel{eq:QKV_matrix}{{69}{60}{Self-attention in forma matriciale (una testa)}{equation.12.69}{}}
\newlabel{eq:QKV_matrix@cref}{{[equation][69][]69}{[1][60][]60}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces La matrice $N\times N$ $QK^\top $: ogni cella contiene un confronto $q_i\cdot k_j$, calcolato simultaneamente con una singola moltiplicazione tra matrici \blx@tocontentsinit {0}\cite {jm3}.\relax }}{61}{figure.caption.33}\protected@file@percent }
\newlabel{fig:qkt_full}{{25}{61}{La matrice $N\times N$ $QK^\top $: ogni cella contiene un confronto $q_i\cdot k_j$, calcolato simultaneamente con una singola moltiplicazione tra matrici \cite {jm3}.\relax }{figure.caption.33}{}}
\newlabel{fig:qkt_full@cref}{{[figure][25][]25}{[1][61][]61}}
\newlabel{eq:head_matrix}{{70}{61}{Self-attention in forma matriciale (una testa)}{equation.12.70}{}}
\newlabel{eq:head_matrix@cref}{{[equation][70][]70}{[1][61][]61}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.3.3}Mascheramento causale: eliminare il futuro}{61}{subsubsection.12.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Mascheramento causale di $QK^\top $: la parte triangolare superiore (con $j>i$) è impostata a $-\infty $, così la softmax annulla i contributi dei token futuri \blx@tocontentsinit {0}\cite {jm3}.\relax }}{62}{figure.caption.34}\protected@file@percent }
\newlabel{fig:qkt_masked}{{26}{62}{Mascheramento causale di $QK^\top $: la parte triangolare superiore (con $j>i$) è impostata a $-\infty $, così la softmax annulla i contributi dei token futuri \cite {jm3}.\relax }{figure.caption.34}{}}
\newlabel{fig:qkt_masked@cref}{{[figure][26][]26}{[1][61][]62}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.3.4}Schema completo per una testa (in parallelo)}{62}{subsubsection.12.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.3.5}Costo computazionale e dipendenza quadratica}{62}{subsubsection.12.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.3.6}Multi-head attention in parallelo}{62}{subsubsection.12.3.6}\protected@file@percent }
\newlabel{eq:QKV_per_head}{{71}{62}{Multi-head attention in parallelo}{equation.12.71}{}}
\newlabel{eq:QKV_per_head@cref}{{[equation][71][]71}{[1][62][]62}}
\newlabel{eq:multihead_parallel}{{72}{62}{Multi-head attention in parallelo}{equation.12.72}{}}
\newlabel{eq:multihead_parallel@cref}{{[equation][72][]72}{[1][62][]62}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Computazione parallela di una singola testa di self-attention: (i) proiezioni $Q,K,V$ da $X$; (ii) calcolo di $QK^\top $; (iii) maschera causale; (iv) (softmax non mostrata nello schema); (v) somma pesata tramite $V$ per ottenere l’output della testa \blx@tocontentsinit {0}\cite {jm3}.\relax }}{63}{figure.caption.35}\protected@file@percent }
\newlabel{fig:single_head_parallel}{{27}{63}{Computazione parallela di una singola testa di self-attention: (i) proiezioni $Q,K,V$ da $X$; (ii) calcolo di $QK^\top $; (iii) maschera causale; (iv) (softmax non mostrata nello schema); (v) somma pesata tramite $V$ per ottenere l’output della testa \cite {jm3}.\relax }{figure.caption.35}{}}
\newlabel{fig:single_head_parallel@cref}{{[figure][27][]27}{[1][62][]63}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.3.7}Il blocco Transformer in forma parallela}{63}{subsubsection.12.3.7}\protected@file@percent }
\newlabel{eq:block_parallel_O}{{73}{63}{Il blocco Transformer in forma parallela}{equation.12.73}{}}
\newlabel{eq:block_parallel_O@cref}{{[equation][73][]73}{[1][63][]63}}
\newlabel{eq:block_parallel_H}{{74}{63}{Il blocco Transformer in forma parallela}{equation.12.74}{}}
\newlabel{eq:block_parallel_H@cref}{{[equation][74][]74}{[1][63][]63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.4}L'input del Transformer: embeddings di token e di posizione}{64}{subsection.12.4}\protected@file@percent }
\newlabel{subsec:input_token_position_embeddings}{{12.4}{64}{L'input del Transformer: embeddings di token e di posizione}{subsection.12.4}{}}
\newlabel{subsec:input_token_position_embeddings@cref}{{[subsection][4][12]12.4}{[1][63][]64}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.4.1}Token embeddings e matrice di embedding}{64}{subsubsection.12.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Selezione via one-hot (interpretazione equivalente).}{64}{section*.36}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Selezione dell’embedding del token $w_i$ a partire dalla matrice $E$ mediante un vettore one-hot: il prodotto \(\mathbf  {o}(w_i)^\top E\) restituisce la riga \(E[w_i]\) \blx@tocontentsinit {0}\cite {jm3}.\relax }}{65}{figure.caption.37}\protected@file@percent }
\newlabel{fig:select_single_embedding}{{28}{65}{Selezione dell’embedding del token $w_i$ a partire dalla matrice $E$ mediante un vettore one-hot: il prodotto \(\mathbf {o}(w_i)^\top E\) restituisce la riga \(E[w_i]\) \cite {jm3}.\relax }{figure.caption.37}{}}
\newlabel{fig:select_single_embedding@cref}{{[figure][28][]28}{[1][65][]65}}
\@writefile{toc}{\contentsline {paragraph}{Dalla sequenza alla matrice.}{65}{section*.38}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Selezione degli embeddings per un’intera sequenza: una matrice di one-hot \(O\) moltiplicata per \(E\) produce una matrice \(N\times d\) contenente gli embeddings lessicali della finestra di contesto \blx@tocontentsinit {0}\cite {jm3}.\relax }}{65}{figure.caption.39}\protected@file@percent }
\newlabel{fig:select_sequence_embedding}{{29}{65}{Selezione degli embeddings per un’intera sequenza: una matrice di one-hot \(O\) moltiplicata per \(E\) produce una matrice \(N\times d\) contenente gli embeddings lessicali della finestra di contesto \cite {jm3}.\relax }{figure.caption.39}{}}
\newlabel{fig:select_sequence_embedding@cref}{{[figure][29][]29}{[1][65][]65}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.4.2}Perché servono gli embeddings posizionali}{65}{subsubsection.12.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.4.3}Posizione assoluta e composizione dell’input}{66}{subsubsection.12.4.3}\protected@file@percent }
\newlabel{eq:composite_embedding}{{75}{66}{Posizione assoluta e composizione dell’input}{equation.12.75}{}}
\newlabel{eq:composite_embedding@cref}{{[equation][75][]75}{[1][66][]66}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.5}Limiti della posizione assoluta e alternative: sinusoidale e posizione relativa}{66}{subsection.12.5}\protected@file@percent }
\newlabel{subsec:positional_alternatives}{{12.5}{66}{Limiti della posizione assoluta e alternative: sinusoidale e posizione relativa}{subsection.12.5}{}}
\newlabel{subsec:positional_alternatives@cref}{{[subsection][5][12]12.5}{[1][66][]66}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Composizione dell’input: l’embedding del token viene sommato all’embedding della posizione assoluta, producendo un vettore di input \(x_i\) in \(\mathbb  {R}^d\). L’insieme dei vettori forma la matrice \(X \in \mathbb  {R}^{N\times d}\) \blx@tocontentsinit {0}\cite {jm3}.\relax }}{67}{figure.caption.40}\protected@file@percent }
\newlabel{fig:token_plus_position}{{30}{67}{Composizione dell’input: l’embedding del token viene sommato all’embedding della posizione assoluta, producendo un vettore di input \(x_i\) in \(\mathbb {R}^d\). L’insieme dei vettori forma la matrice \(X \in \mathbb {R}^{N\times d}\) \cite {jm3}.\relax }{figure.caption.40}{}}
\newlabel{fig:token_plus_position@cref}{{[figure][30][]30}{[1][66][]67}}
\@writefile{toc}{\contentsline {paragraph}{Embeddings posizionali sinusoidali.}{67}{section*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Posizione relativa.}{67}{section*.42}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.6}La \textit  {language modeling head}}{68}{subsection.12.6}\protected@file@percent }
\newlabel{subsec:lm_head}{{12.6}{68}{La \textit {language modeling head}}{subsection.12.6}{}}
\newlabel{subsec:lm_head@cref}{{[subsection][6][12]12.6}{[1][68][]68}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.6.1}Cosa entra e cosa esce: dal vettore $h_N^L$ alle probabilità sul vocabolario}{68}{subsubsection.12.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.6.2}Logits e livello di \textit  {unembedding}}{68}{subsubsection.12.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces La \textit  {language modeling head}: mappa l’output dell’ultimo token all’ultimo strato ($h_N^L$) in una distribuzione sul vocabolario tramite (i) un livello lineare (\emph  {unembedding}) e (ii) una softmax \blx@tocontentsinit {0}\cite {jm3}.\relax }}{69}{figure.caption.43}\protected@file@percent }
\newlabel{fig:lm_head_overview}{{31}{69}{La \textit {language modeling head}: mappa l’output dell’ultimo token all’ultimo strato ($h_N^L$) in una distribuzione sul vocabolario tramite (i) un livello lineare (\emph {unembedding}) e (ii) una softmax \cite {jm3}.\relax }{figure.caption.43}{}}
\newlabel{fig:lm_head_overview@cref}{{[figure][31][]31}{[1][68][]69}}
\newlabel{eq:logits_unembedding_general}{{76}{69}{Logits e livello di \textit {unembedding}}{equation.12.76}{}}
\newlabel{eq:logits_unembedding_general@cref}{{[equation][76][]76}{[1][69][]69}}
\@writefile{toc}{\contentsline {paragraph}{Weight tying: perché spesso $U=E^\top $.}{69}{section*.44}\protected@file@percent }
\newlabel{eq:logits_weight_tying}{{77}{69}{Weight tying: perché spesso $U=E^\top $}{equation.12.77}{}}
\newlabel{eq:logits_weight_tying@cref}{{[equation][77][]77}{[1][69][]69}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.6.3}Softmax: da logits a probabilità}{70}{subsubsection.12.6.3}\protected@file@percent }
\newlabel{eq:softmax_probs}{{78}{70}{Softmax: da logits a probabilità}{equation.12.78}{}}
\newlabel{eq:softmax_probs@cref}{{[equation][78][]78}{[1][69][]70}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.6.4}Dal modello alla generazione: scegliere il prossimo token}{70}{subsubsection.12.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.6.5}Visione d’insieme: un \textit  {decoder-only} che impila blocchi}{70}{subsubsection.12.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.7}Nota: \textit  {logit lens} e terminologia \textit  {decoder-only}}{70}{subsection.12.7}\protected@file@percent }
\newlabel{subsec:logit_lens_decoder_only}{{12.7}{70}{Nota: \textit {logit lens} e terminologia \textit {decoder-only}}{subsection.12.7}{}}
\newlabel{subsec:logit_lens_decoder_only@cref}{{[subsection][7][12]12.7}{[1][70][]70}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces Un Transformer per language modeling (\textit  {decoder-only}): impila blocchi Transformer e usa la language modeling head per mappare \(h_i^L\) in una distribuzione sul prossimo token \(w_{i+1}\) \blx@tocontentsinit {0}\cite {jm3}.\relax }}{71}{figure.caption.45}\protected@file@percent }
\newlabel{fig:decoder_only_stack}{{32}{71}{Un Transformer per language modeling (\textit {decoder-only}): impila blocchi Transformer e usa la language modeling head per mappare \(h_i^L\) in una distribuzione sul prossimo token \(w_{i+1}\) \cite {jm3}.\relax }{figure.caption.45}{}}
\newlabel{fig:decoder_only_stack@cref}{{[figure][32][]32}{[1][70][]71}}
\@writefile{toc}{\contentsline {paragraph}{Nota terminologica: \textit  {decoder-only}.}{72}{section*.46}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13}Large Language Models}{72}{section.13}\protected@file@percent }
\newlabel{sec:llm}{{13}{72}{Large Language Models}{section.13}{}}
\newlabel{sec:llm@cref}{{[section][13][]13}{[1][72][]72}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1}Large Language Models con Transformer: generazione condizionata}{73}{subsection.13.1}\protected@file@percent }
\newlabel{subsec:llm_with_transformers}{{13.1}{73}{Large Language Models con Transformer: generazione condizionata}{subsection.13.1}{}}
\newlabel{subsec:llm_with_transformers@cref}{{[subsection][1][13]13.1}{[1][73][]73}}
\@writefile{toc}{\contentsline {paragraph}{Perché “predire parole” è utile per tanti task.}{73}{section*.48}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces Completamento autoregressivo left-to-right: i token generati vengono riaggiunti al contesto e diventano parte del prefisso per la predizione successiva.\relax }}{74}{figure.caption.47}\protected@file@percent }
\newlabel{fig:llm_completion}{{33}{74}{Completamento autoregressivo left-to-right: i token generati vengono riaggiunti al contesto e diventano parte del prefisso per la predizione successiva.\relax }{figure.caption.47}{}}
\newlabel{fig:llm_completion@cref}{{[figure][33][]33}{[1][73][]74}}
\@writefile{toc}{\contentsline {paragraph}{Esempio: riassunto come generazione condizionata.}{74}{section*.49}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Nota sul passo successivo (ponte verso BERT).}{74}{section*.52}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Esempio di testo e relativo riassunto (scenario tipico di summarization). Il compito può essere visto come generazione condizionata sul documento.\relax }}{75}{figure.caption.50}\protected@file@percent }
\newlabel{fig:summarization_example}{{34}{75}{Esempio di testo e relativo riassunto (scenario tipico di summarization). Il compito può essere visto come generazione condizionata sul documento.\relax }{figure.caption.50}{}}
\newlabel{fig:summarization_example@cref}{{[figure][34][]34}{[1][74][]75}}
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces Riassunto tramite prompting: il token (o stringa) \texttt  {tl;dr} agisce da segnale che innesca la generazione di una sintesi, sfruttando la lunga finestra di contesto del Transformer.\relax }}{75}{figure.caption.51}\protected@file@percent }
\newlabel{fig:tldr_summarization}{{35}{75}{Riassunto tramite prompting: il token (o stringa) \texttt {tl;dr} agisce da segnale che innesca la generazione di una sintesi, sfruttando la lunga finestra di contesto del Transformer.\relax }{figure.caption.51}{}}
\newlabel{fig:tldr_summarization@cref}{{[figure][35][]35}{[1][74][]75}}
\@writefile{toc}{\contentsline {section}{\numberline {14}Sampling per la generazione con LLM}{76}{section.14}\protected@file@percent }
\newlabel{sec:llm-sampling}{{14}{76}{Sampling per la generazione con LLM}{section.14}{}}
\newlabel{sec:llm-sampling@cref}{{[section][14][]14}{[1][76][]76}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.1}Perché non basta il campionamento ``puro''}{76}{subsection.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2}Top-$k$ sampling}{77}{subsection.14.2}\protected@file@percent }
\newlabel{subsec:topk}{{14.2}{77}{Top-$k$ sampling}{subsection.14.2}{}}
\newlabel{subsec:topk@cref}{{[subsection][2][14]14.2}{[1][77][]77}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3}Top-$p$ (nucleus) sampling}{77}{subsection.14.3}\protected@file@percent }
\newlabel{subsec:topp}{{14.3}{77}{Top-$p$ (nucleus) sampling}{subsection.14.3}{}}
\newlabel{subsec:topp@cref}{{[subsection][3][14]14.3}{[1][77][]77}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4}Temperature sampling}{78}{subsection.14.4}\protected@file@percent }
\newlabel{subsec:temperature}{{14.4}{78}{Temperature sampling}{subsection.14.4}{}}
\newlabel{subsec:temperature@cref}{{[subsection][4][14]14.4}{[1][78][]78}}
\@writefile{toc}{\contentsline {section}{\numberline {15}Pretraining dei Large Language Models}{78}{section.15}\protected@file@percent }
\newlabel{sec:llm-pretraining}{{15}{78}{Pretraining dei Large Language Models}{section.15}{}}
\newlabel{sec:llm-pretraining@cref}{{[section][15][]15}{[1][78][]78}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.1}Setup, notazione e obiettivo di language modeling}{79}{subsection.15.1}\protected@file@percent }
\newlabel{subsec:llm-setup}{{15.1}{79}{Setup, notazione e obiettivo di language modeling}{subsection.15.1}{}}
\newlabel{subsec:llm-setup@cref}{{[subsection][1][15]15.1}{[1][79][]79}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.2}Self-supervision e funzione obiettivo}{79}{subsection.15.2}\protected@file@percent }
\newlabel{subsec:self-supervision-obj}{{15.2}{79}{Self-supervision e funzione obiettivo}{subsection.15.2}{}}
\newlabel{subsec:self-supervision-obj@cref}{{[subsection][2][15]15.2}{[1][79][]79}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3}Teacher forcing}{80}{subsection.15.3}\protected@file@percent }
\newlabel{subsec:teacher-forcing}{{15.3}{80}{Teacher forcing}{subsection.15.3}{}}
\newlabel{subsec:teacher-forcing@cref}{{[subsection][3][15]15.3}{[1][80][]80}}
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces Addestramento di un transformer come language model: per ogni posizione si predice il token successivo e si calcola una loss di cross-entropy; la loss totale è la media (o somma) sulle posizioni.\relax }}{81}{figure.caption.53}\protected@file@percent }
\newlabel{fig:train-lm}{{36}{81}{Addestramento di un transformer come language model: per ogni posizione si predice il token successivo e si calcola una loss di cross-entropy; la loss totale è la media (o somma) sulle posizioni.\relax }{figure.caption.53}{}}
\newlabel{fig:train-lm@cref}{{[figure][36][]36}{[1][80][]81}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4}Efficienza computazionale: parallelismo nei transformer}{81}{subsection.15.4}\protected@file@percent }
\newlabel{subsec:parallelism}{{15.4}{81}{Efficienza computazionale: parallelismo nei transformer}{subsection.15.4}{}}
\newlabel{subsec:parallelism@cref}{{[subsection][4][15]15.4}{[1][81][]81}}
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces Esempio di composizione di un grande corpus di pretraining (treemap): diverse sorgenti contribuiscono con proporzioni differenti.\relax }}{82}{figure.caption.54}\protected@file@percent }
\newlabel{fig:pile}{{37}{82}{Esempio di composizione di un grande corpus di pretraining (treemap): diverse sorgenti contribuiscono con proporzioni differenti.\relax }{figure.caption.54}{}}
\newlabel{fig:pile@cref}{{[figure][37][]37}{[1][82][]82}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.5}Dati di pretraining: fonti e filtraggio}{82}{subsection.15.5}\protected@file@percent }
\newlabel{subsec:data}{{15.5}{82}{Dati di pretraining: fonti e filtraggio}{subsection.15.5}{}}
\newlabel{subsec:data@cref}{{[subsection][5][15]15.5}{[1][82][]82}}
\@writefile{toc}{\contentsline {paragraph}{Filtri di qualità e sicurezza.}{83}{section*.55}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Aspetti etici e legali (panoramica).}{83}{section*.56}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.6}Dal pretraining all'adattamento: finetuning}{83}{subsection.15.6}\protected@file@percent }
\newlabel{subsec:finetune}{{15.6}{83}{Dal pretraining all'adattamento: finetuning}{subsection.15.6}{}}
\newlabel{subsec:finetune@cref}{{[subsection][6][15]15.6}{[1][83][]83}}
\@writefile{toc}{\contentsline {paragraph}{Tipi di adattamento (senza anticipare modelli specifici).}{83}{section*.58}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces Schema concettuale: pretraining su grandi corpora $\rightarrow $ modello generale $\rightarrow $ adattamento su dati specifici (finetuning) $\rightarrow $ modello specializzato.\relax }}{84}{figure.caption.57}\protected@file@percent }
\newlabel{fig:pretrain-finetune}{{38}{84}{Schema concettuale: pretraining su grandi corpora $\rightarrow $ modello generale $\rightarrow $ adattamento su dati specifici (finetuning) $\rightarrow $ modello specializzato.\relax }{figure.caption.57}{}}
\newlabel{fig:pretrain-finetune@cref}{{[figure][38][]38}{[1][83][]84}}
\@writefile{toc}{\contentsline {paragraph}{Collegamento alle sezioni successive.}{84}{section*.59}\protected@file@percent }
\@setckpt{chapters/03_embeddings}{
\setcounter{page}{85}
\setcounter{equation}{79}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{section}{15}
\setcounter{subsection}{6}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{38}
\setcounter{table}{2}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{lstnumber}{1}
\setcounter{parentequation}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{89}
\setcounter{maxnames}{1}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{blx@maxsegment@0}{0}
\setcounter{blx@sectionciteorder@0}{89}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{Item}{35}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{92}
\setcounter{FancyVerbLine}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{23}
\setcounter{definition}{0}
\setcounter{lstlisting}{0}
\setcounter{section@level}{4}
}
