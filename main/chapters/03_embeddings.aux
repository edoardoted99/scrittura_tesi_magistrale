\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {3}Word Embeddings}{25}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Dalla semantica alla rappresentazione vettoriale}{26}{subsection.3.1}\protected@file@percent }
\newlabel{sec:semantics_to_vectors}{{3.1}{26}{Dalla semantica alla rappresentazione vettoriale}{subsection.3.1}{}}
\newlabel{sec:semantics_to_vectors@cref}{{[subsection][1][3]3.1}{[1][26][]26}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Simboli e significati}{26}{subsubsection.3.1.1}\protected@file@percent }
\newlabel{subsubsec:symbols_meanings}{{3.1.1}{26}{Simboli e significati}{subsubsection.3.1.1}{}}
\newlabel{subsubsec:symbols_meanings@cref}{{[subsubsection][1][3,1]3.1.1}{[1][26][]26}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Gli assi del linguaggio}{26}{subsubsection.3.1.2}\protected@file@percent }
\newlabel{subsubsec:linguistic_axes}{{3.1.2}{26}{Gli assi del linguaggio}{subsubsection.3.1.2}{}}
\newlabel{subsubsec:linguistic_axes@cref}{{[subsubsection][2][3,1]3.1.2}{[1][26][]26}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Rappresentazione del mapping tra lo spazio discreto dei simboli (Significanti) e lo spazio continuo dei vettori (Significati). L'obiettivo è apprendere una funzione $f$ tale che simboli diversi con significati simili vengano proiettati in vettori vicini nello spazio matematico.}}{27}{figure.caption.13}\protected@file@percent }
\newlabel{fig:signifier_signified_mapping}{{10}{27}{Rappresentazione del mapping tra lo spazio discreto dei simboli (Significanti) e lo spazio continuo dei vettori (Significati). L'obiettivo è apprendere una funzione $f$ tale che simboli diversi con significati simili vengano proiettati in vettori vicini nello spazio matematico}{figure.caption.13}{}}
\newlabel{fig:signifier_signified_mapping@cref}{{[figure][10][]10}{[1][26][]27}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}L'ipotesi distribuzionale}{27}{subsubsection.3.1.3}\protected@file@percent }
\newlabel{subsubsec:distributional_hypothesis}{{3.1.3}{27}{L'ipotesi distribuzionale}{subsubsection.3.1.3}{}}
\newlabel{subsubsec:distributional_hypothesis@cref}{{[subsubsection][3][3,1]3.1.3}{[1][27][]27}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Ferdinand de Saussure, fondatore della linguistica strutturale e teorico dei rapporti sintagmatici e associativi (paradigmatici) del linguaggio \blx@tocontentsinit {0}\cite {saussure_wikipedia_image}.}}{28}{figure.caption.14}\protected@file@percent }
\newlabel{fig:saussure}{{11}{28}{Ferdinand de Saussure, fondatore della linguistica strutturale e teorico dei rapporti sintagmatici e associativi (paradigmatici) del linguaggio \cite {saussure_wikipedia_image}}{figure.caption.14}{}}
\newlabel{fig:saussure@cref}{{[figure][11][]11}{[1][26][]28}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Rappresentazione degli assi del linguaggio. L'asse orizzontale mostra la sequenza lineare (sintagma), quello verticale le alternative possibili (paradigma).}}{29}{figure.caption.15}\protected@file@percent }
\newlabel{fig:linguistic_axes_final}{{12}{29}{Rappresentazione degli assi del linguaggio. L'asse orizzontale mostra la sequenza lineare (sintagma), quello verticale le alternative possibili (paradigma)}{figure.caption.15}{}}
\newlabel{fig:linguistic_axes_final@cref}{{[figure][12][]12}{[1][26][]29}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Ipotesi di Osgood: il significato come vettore}{30}{subsubsection.3.1.4}\protected@file@percent }
\newlabel{subsubsec:osgood_hypothesis}{{3.1.4}{30}{Ipotesi di Osgood: il significato come vettore}{subsubsection.3.1.4}{}}
\newlabel{subsubsec:osgood_hypothesis@cref}{{[subsubsection][4][3,1]3.1.4}{[1][30][]30}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.5}Verso i word embeddings}{31}{subsubsection.3.1.5}\protected@file@percent }
\newlabel{subsubsec:towards_embeddings}{{3.1.5}{31}{Verso i word embeddings}{subsubsection.3.1.5}{}}
\newlabel{subsubsec:towards_embeddings@cref}{{[subsubsection][5][3,1]3.1.5}{[1][30][]31}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Convergenza delle due ipotesi fondamentali negli embeddings moderni. L'ipotesi di Osgood fornisce il \emph  {formato} della rappresentazione (vettori numerici), mentre l'ipotesi distribuzionale indica \emph  {cosa} deve essere catturato (relazioni contestuali tra parole).}}{31}{figure.caption.16}\protected@file@percent }
\newlabel{fig:convergenza_ipotesi}{{13}{31}{Convergenza delle due ipotesi fondamentali negli embeddings moderni. L'ipotesi di Osgood fornisce il \emph {formato} della rappresentazione (vettori numerici), mentre l'ipotesi distribuzionale indica \emph {cosa} deve essere catturato (relazioni contestuali tra parole)}{figure.caption.16}{}}
\newlabel{fig:convergenza_ipotesi@cref}{{[figure][13][]13}{[1][30][]31}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Classificazione delle principali tipologie di embeddings trattate nel capitolo.}}{32}{figure.caption.17}\protected@file@percent }
\newlabel{fig:classificazione_embeddings}{{14}{32}{Classificazione delle principali tipologie di embeddings trattate nel capitolo}{figure.caption.17}{}}
\newlabel{fig:classificazione_embeddings@cref}{{[figure][14][]14}{[1][32][]32}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Embeddings statici}{33}{subsection.3.2}\protected@file@percent }
\newlabel{sec:static_embeddings}{{3.2}{33}{Embeddings statici}{subsection.3.2}{}}
\newlabel{sec:static_embeddings@cref}{{[subsection][2][3]3.2}{[1][32][]33}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Embeddings count-based}{33}{subsubsection.3.2.1}\protected@file@percent }
\newlabel{subsubsec:count_based}{{3.2.1}{33}{Embeddings count-based}{subsubsection.3.2.1}{}}
\newlabel{subsubsec:count_based@cref}{{[subsubsection][1][3,2]3.2.1}{[1][33][]33}{}{}{}}
\@writefile{toc}{\contentsline {paragraph}{Matrice termine-documento.}{33}{section*.18}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Term-document matrix per quattro parole in quattro opere di Shakespeare. Ogni cella contiene il numero di occorrenze della parola (riga) nel documento (colonna).}}{33}{table.caption.19}\protected@file@percent }
\newlabel{tab:term_document_shakespeare}{{1}{33}{Term-document matrix per quattro parole in quattro opere di Shakespeare. Ogni cella contiene il numero di occorrenze della parola (riga) nel documento (colonna)}{table.caption.19}{}}
\newlabel{tab:term_document_shakespeare@cref}{{[table][1][]1}{[1][33][]33}{}{}{}}
\@writefile{toc}{\contentsline {paragraph}{Matrice termine-termine.}{34}{section*.20}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Estratto di una matrice termine--termine calcolata sul corpus Wikipedia. Ogni cella contiene il numero di co-occorrenze tra la parola target (riga) e la parola di contesto (colonna) all'interno di una finestra di contesto locale.}}{34}{table.caption.21}\protected@file@percent }
\newlabel{tab:term_term_wikipedia}{{2}{34}{Estratto di una matrice termine--termine calcolata sul corpus Wikipedia. Ogni cella contiene il numero di co-occorrenze tra la parola target (riga) e la parola di contesto (colonna) all'interno di una finestra di contesto locale}{table.caption.21}{}}
\newlabel{tab:term_term_wikipedia@cref}{{[table][2][]2}{[1][34][]34}{}{}{}}
\@writefile{toc}{\contentsline {paragraph}{Riduzione dimensionale tramite SVD.}{35}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cosine similarity.}{36}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Word2Vec: un approccio predittivo}{36}{subsubsection.3.2.2}\protected@file@percent }
\newlabel{subsubsec:word2vec}{{3.2.2}{36}{Word2Vec: un approccio predittivo}{subsubsection.3.2.2}{}}
\newlabel{subsubsec:word2vec@cref}{{[subsubsection][2][3,2]3.2.2}{[1][36][]36}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Word2Vec come fusione tra teoria e implementazione. A sinistra, l'ipotesi di Osgood si traduce nella scelta di vettori continui e nel prodotto scalare come misura di similarità. A destra, l'ipotesi distribuzionale si concretizza nella finestra di contesto e nella modellazione di co-occorrenze. Word2Vec unifica elegantemente queste due linee, apprendendo vettori che massimizzano la predizione di co-occorrenze locali.}}{36}{figure.caption.24}\protected@file@percent }
\newlabel{fig:word2vec_teoria_implementazione}{{15}{36}{Word2Vec come fusione tra teoria e implementazione. A sinistra, l'ipotesi di Osgood si traduce nella scelta di vettori continui e nel prodotto scalare come misura di similarità. A destra, l'ipotesi distribuzionale si concretizza nella finestra di contesto e nella modellazione di co-occorrenze. Word2Vec unifica elegantemente queste due linee, apprendendo vettori che massimizzano la predizione di co-occorrenze locali}{figure.caption.24}{}}
\newlabel{fig:word2vec_teoria_implementazione@cref}{{[figure][15][]15}{[1][36][]36}{}{}{}}
\@writefile{toc}{\contentsline {paragraph}{Il classificatore e la funzione sigmoide.}{37}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Perché due matrici? Il ruolo di $W$ e $C$.}{38}{section*.26}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Lo skip-gram apprende in totale due insiemi di embedding, uno per i target ($W$) e uno per i contesti ($C$), per un totale di $2|V|$ vettori. L'addestramento mira a massimizzare la probabilità che parole vicine nel testo abbiano vettori simili.}}{38}{figure.caption.27}\protected@file@percent }
\newlabel{fig:skipgram_structure}{{16}{38}{Lo skip-gram apprende in totale due insiemi di embedding, uno per i target ($W$) e uno per i contesti ($C$), per un totale di $2|V|$ vettori. L'addestramento mira a massimizzare la probabilità che parole vicine nel testo abbiano vettori simili}{figure.caption.27}{}}
\newlabel{fig:skipgram_structure@cref}{{[figure][16][]16}{[1][38][]38}{}{}{}}
\@writefile{toc}{\contentsline {paragraph}{Finestra di contesto e precisione semantica.}{38}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Geometria dell'analogia: il modello del parallelogramma.}{39}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Limiti degli embeddings statici}{39}{subsubsection.3.2.3}\protected@file@percent }
\newlabel{subsubsec:static_limits}{{3.2.3}{39}{Limiti degli embeddings statici}{subsubsection.3.2.3}{}}
\newlabel{subsubsec:static_limits@cref}{{[subsubsection][3][3,2]3.2.3}{[1][39][]39}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Rappresentazione geometrica del modello del parallelogramma nello spazio latente.}}{40}{figure.caption.30}\protected@file@percent }
\newlabel{fig:parallelogramma}{{17}{40}{Rappresentazione geometrica del modello del parallelogramma nello spazio latente}{figure.caption.30}{}}
\newlabel{fig:parallelogramma@cref}{{[figure][17][]17}{[1][39][]40}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Embeddings dinamici (contestuali)}{41}{subsection.3.3}\protected@file@percent }
\newlabel{sec:dynamic_embeddings}{{3.3}{41}{Embeddings dinamici (contestuali)}{subsection.3.3}{}}
\newlabel{sec:dynamic_embeddings@cref}{{[subsection][3][3]3.3}{[1][41][]41}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}RNN: memoria sequenziale}{42}{subsubsection.3.3.1}\protected@file@percent }
\newlabel{subsubsec:rnn}{{3.3.1}{42}{RNN: memoria sequenziale}{subsubsection.3.3.1}{}}
\newlabel{subsubsec:rnn@cref}{{[subsubsection][1][3,3]3.3.1}{[1][42][]42}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Flusso di informazione in una RNN. Ogni stato nascosto $h_t$ integra l'input corrente $x_t$ con la memoria accumulata nello stato precedente $h_{t-1}$.}}{43}{figure.caption.31}\protected@file@percent }
\newlabel{fig:rnn_flow}{{18}{43}{Flusso di informazione in una RNN. Ogni stato nascosto $h_t$ integra l'input corrente $x_t$ con la memoria accumulata nello stato precedente $h_{t-1}$}{figure.caption.31}{}}
\newlabel{fig:rnn_flow@cref}{{[figure][18][]18}{[1][43][]43}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}LSTM: controllare il flusso di informazione}{44}{subsubsection.3.3.2}\protected@file@percent }
\newlabel{subsubsec:lstm}{{3.3.2}{44}{LSTM: controllare il flusso di informazione}{subsubsection.3.3.2}{}}
\newlabel{subsubsec:lstm@cref}{{[subsubsection][2][3,3]3.3.2}{[1][44][]44}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Schema semplificato di una cella LSTM. Il cell state (linea orizzontale) attraversa la cella subendo modifiche controllate dai tre gate, che decidono rispettivamente cosa dimenticare, cosa aggiungere, e cosa esporre come output.}}{45}{figure.caption.32}\protected@file@percent }
\newlabel{fig:lstm_gates}{{19}{45}{Schema semplificato di una cella LSTM. Il cell state (linea orizzontale) attraversa la cella subendo modifiche controllate dai tre gate, che decidono rispettivamente cosa dimenticare, cosa aggiungere, e cosa esporre come output}{figure.caption.32}{}}
\newlabel{fig:lstm_gates@cref}{{[figure][19][]19}{[1][45][]45}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Bidirezionalità}{46}{subsubsection.3.3.3}\protected@file@percent }
\newlabel{subsubsec:bidirectional}{{3.3.3}{46}{Bidirezionalità}{subsubsection.3.3.3}{}}
\newlabel{subsubsec:bidirectional@cref}{{[subsubsection][3][3,3]3.3.3}{[1][46][]46}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Architettura Bi-LSTM. La sequenza viene processata in entrambe le direzioni. L'embedding finale di ogni token è la concatenazione degli stati forward e backward, integrando così il contesto completo.}}{47}{figure.caption.33}\protected@file@percent }
\newlabel{fig:bilstm}{{20}{47}{Architettura Bi-LSTM. La sequenza viene processata in entrambe le direzioni. L'embedding finale di ogni token è la concatenazione degli stati forward e backward, integrando così il contesto completo}{figure.caption.33}{}}
\newlabel{fig:bilstm@cref}{{[figure][20][]20}{[1][47][]47}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Encoder-Decoder: trasformare sequenze}{48}{subsubsection.3.3.4}\protected@file@percent }
\newlabel{subsubsec:encoder_decoder}{{3.3.4}{48}{Encoder-Decoder: trasformare sequenze}{subsubsection.3.3.4}{}}
\newlabel{subsubsec:encoder_decoder@cref}{{[subsubsection][4][3,3]3.3.4}{[1][48][]48}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Architettura Encoder-Decoder. L'encoder comprime la sequenza sorgente nel context vector $c$. Il decoder genera la sequenza target condizionato su $c$, un token alla volta. Tutta l'informazione deve passare attraverso $c$: questo è il bottleneck.}}{49}{figure.caption.34}\protected@file@percent }
\newlabel{fig:encoder_decoder}{{21}{49}{Architettura Encoder-Decoder. L'encoder comprime la sequenza sorgente nel context vector $c$. Il decoder genera la sequenza target condizionato su $c$, un token alla volta. Tutta l'informazione deve passare attraverso $c$: questo è il bottleneck}{figure.caption.34}{}}
\newlabel{fig:encoder_decoder@cref}{{[figure][21][]21}{[1][49][]49}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.5}Attenzione: superare il bottleneck}{50}{subsubsection.3.3.5}\protected@file@percent }
\newlabel{subsubsec:attention}{{3.3.5}{50}{Attenzione: superare il bottleneck}{subsubsection.3.3.5}{}}
\newlabel{subsubsec:attention@cref}{{[subsubsection][5][3,3]3.3.5}{[1][50][]50}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Meccanismo di attenzione. Per generare ``gatto'', il decoder interroga tutti gli stati dell'encoder. I pesi $\alpha _{ij}$ determinano quanto ogni posizione contribuisce al context vector $c_i$. In questo esempio, l'attenzione si concentra su $h_2$ (``cat''), che contribuisce per l'80\%.}}{52}{figure.caption.35}\protected@file@percent }
\newlabel{fig:attention_mechanism}{{22}{52}{Meccanismo di attenzione. Per generare ``gatto'', il decoder interroga tutti gli stati dell'encoder. I pesi $\alpha _{ij}$ determinano quanto ogni posizione contribuisce al context vector $c_i$. In questo esempio, l'attenzione si concentra su $h_2$ (``cat''), che contribuisce per l'80\%}{figure.caption.35}{}}
\newlabel{fig:attention_mechanism@cref}{{[figure][22][]22}{[1][51][]52}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.6}Transformer: attenzione senza ricorrenza}{52}{subsubsection.3.3.6}\protected@file@percent }
\newlabel{subsubsec:transformer}{{3.3.6}{52}{Transformer: attenzione senza ricorrenza}{subsubsection.3.3.6}{}}
\newlabel{subsubsec:transformer@cref}{{[subsubsection][6][3,3]3.3.6}{[1][52][]52}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Self-attention: il token ``dorme'' assegna pesi di attenzione a tutte le posizioni della propria sequenza. Il peso maggiore va a ``gatto'' (linea spessa), catturando la dipendenza soggetto-verbo nonostante la distanza.}}{54}{figure.caption.36}\protected@file@percent }
\newlabel{fig:self_attention}{{23}{54}{Self-attention: il token ``dorme'' assegna pesi di attenzione a tutte le posizioni della propria sequenza. Il peso maggiore va a ``gatto'' (linea spessa), catturando la dipendenza soggetto-verbo nonostante la distanza}{figure.caption.36}{}}
\newlabel{fig:self_attention@cref}{{[figure][23][]23}{[1][53][]54}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.7}BERT: codifica bidirezionale}{55}{subsubsection.3.3.7}\protected@file@percent }
\newlabel{subsubsec:bert}{{3.3.7}{55}{BERT: codifica bidirezionale}{subsubsection.3.3.7}{}}
\newlabel{subsubsec:bert@cref}{{[subsubsection][7][3,3]3.3.7}{[1][54][]55}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Masked Language Modeling: il token mascherato viene predetto integrando informazione da tutto il contesto circostante—sia sinistro che destro.}}{56}{figure.caption.37}\protected@file@percent }
\newlabel{fig:mlm}{{24}{56}{Masked Language Modeling: il token mascherato viene predetto integrando informazione da tutto il contesto circostante—sia sinistro che destro}{figure.caption.37}{}}
\newlabel{fig:mlm@cref}{{[figure][24][]24}{[1][55][]56}{}{}{}}
\@writefile{toc}{\contentsline {paragraph}{Pre-training e fine-tuning.}{56}{section*.38}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Il problema dell'opacità.}{56}{section*.39}\protected@file@percent }
\@setckpt{chapters/03_embeddings}{
\setcounter{page}{58}
\setcounter{equation}{28}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{subsubsection}{7}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{24}
\setcounter{table}{2}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{lstnumber}{1}
\setcounter{parentequation}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{21}
\setcounter{maxnames}{1}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{section@level}{4}
\setcounter{Item}{71}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{48}
\setcounter{FancyVerbLine}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{24}
\setcounter{definition}{0}
\setcounter{lstlisting}{0}
}
