\section{Autoencoders e Sparse Autoencoders}
\label{sec:autoencoders}

Per comprendere il funzionamento di \textit{Prisma} e l’intero lavoro di tesi è fondamentale introdurre una particolare famiglia di reti neurali: gli \textbf{autoencoder}. Dietro questa architettura si celano numerosi concetti che risultano centrali per l’interpretazione delle rappresentazioni interne apprese dal modello.
Gli autoencoder sono modelli di apprendimento non supervisionato progettati per apprendere una funzione di ricostruzione dell’input. Essi sono caratterizzati dalla presenza di una strozzatura informativa (\textit{bottleneck}) che costringe il modello a comprimere i dati in una rappresentazione a dimensionalità inferiore, detta \textbf{rappresentazione latente}. L’idea alla base è quella di indurre il modello a catturare esclusivamente le caratteristiche più rilevanti dei dati, scartando ridondanze e rumore.
Si può immaginare un autoencoder come un’autostrada su cui transitano numerosi veicoli che trasportano informazione. In corrispondenza del casello, che rappresenta il bottleneck, solo una parte di questi veicoli viene lasciata passare. In modo analogo, l’autoencoder impara a selezionare e comprimere l’informazione necessaria alla ricostruzione dell’input.
Un esempio concreto è fornito da immagini in bianco e nero di dimensione $28 \times 28$, come quelle del dataset MNIST. Ogni immagine è rappresentabile come un vettore di 784 pixel, e lo spazio delle possibili configurazioni è quindi estremamente vasto. La maggior parte di tali configurazioni non possiede alcun significato semantico ed è assimilabile a rumore. In Figura~\ref{fig:noisy_images} sono mostrati esempi di immagini MNIST e delle corrispondenti versioni contaminate da rumore, che evidenziano come il rumore occupi una porzione rilevante dello spazio delle possibili immagini.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{pictures/cap2/noisy_examples.png}
    \caption{Esempi di immagini MNIST $28 \times 28$. Nella riga superiore sono mostrate immagini originali, mentre nella riga inferiore sono riportate versioni contaminate da rumore. La maggior parte delle possibili configurazioni pixel-wise non corrisponde a immagini semanticamente significative.}
    \label{fig:noisy_images}
\end{figure}
Addestrando un autoencoder esclusivamente su immagini dotate di struttura, il modello è incentivato ad apprendere una rappresentazione compatta che separi le componenti informative dal rumore. Questo processo è di natura \textbf{induttiva}: a partire da esempi specifici, il modello costruisce una rappresentazione generale del concetto di cifra.
Dal punto di vista architetturale, un autoencoder è composto da due moduli principali: un \textbf{encoder}, che mappa l’input nello spazio latente, e un \textbf{decoder}, che ricostruisce l’input a partire dalla rappresentazione latente. La Figura~\ref{fig:autoencoder_arch} illustra schematicamente questa struttura.
\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[
        scale=0.65,
        transform shape,
        node distance=1.2cm,
        every node/.style={font=\small}
    ]
        \node[draw, rectangle, minimum height=3cm, minimum width=0.8cm, fill=gray!20] (input) {Input $x$};
        \node[draw, trapezium, trapezium angle=70, shape border rotate=270, minimum height=1.6cm, fill=blue!10, right=of input] (enc) {Encoder};
        \node[draw, rectangle, minimum height=1cm, minimum width=0.8cm, fill=red!20, right=of enc] (latent) {$z \ll x$};
        \node[draw, trapezium, trapezium angle=70, shape border rotate=90, minimum height=1.6cm, fill=blue!10, right=of latent] (dec) {Decoder};
        \node[draw, rectangle, minimum height=3cm, minimum width=0.8cm, fill=gray!20, right=of dec] (output) {Output $\hat{x}$};

        \draw[->, thick] (input) -- (enc);
        \draw[->, thick] (enc) -- (latent);
        \draw[->, thick] (latent) -- (dec);
        \draw[->, thick] (dec) -- (output);
    \end{tikzpicture}
    \caption{Architettura di un autoencoder classico. Il bottleneck forza una compressione informativa che induce l’apprendimento di una rappresentazione latente compatta.}
    \label{fig:autoencoder_arch}
\end{figure}
Una volta addestrato, l’autoencoder è in grado di ricostruire fedelmente le immagini di input, pur operando una forte riduzione dimensionale. In Figura~\ref{fig:reconstructions} è mostrato un confronto tra immagini originali e ricostruzioni prodotte dal modello.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{pictures/cap2/reconstructions.png}
    \caption{Confronto tra immagini originali (riga superiore) e ricostruzioni prodotte dall’autoencoder (riga inferiore). Il modello preserva le strutture principali nonostante la compressione informativa.}
    \label{fig:reconstructions}
\end{figure}
Un aspetto cruciale degli autoencoder è la struttura dello spazio latente appreso. Proiettando le rappresentazioni latenti in uno spazio bidimensionale, come mostrato in Figura~\ref{fig:latent_space}, è possibile osservare l’emergere spontaneo di cluster associati alle diverse classi di cifre, pur in assenza di supervisione esplicita.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{pictures/cap2/latent_scatter.png}
    \caption{Proiezione bidimensionale dello spazio latente appreso dall’autoencoder. I punti sono colorati in base alla classe MNIST, evidenziando la presenza di cluster semanticamente coerenti.}
    \label{fig:latent_space}
\end{figure}
Un ulteriore modo per analizzare la rappresentazione latente consiste nel campionare sistematicamente lo spazio $z$ e osservare le immagini generate dal decoder. In Figura~\ref{fig:latent_manifold} è mostrato il risultato di tale campionamento. come solo una regione limitata dello spazio latente produca immagini riconoscibili, mentre ampie porzioni generano strutture prive di significato.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{pictures/cap2/manifold_grid.png}
    \caption{Campionamento del decoder su una griglia bidimensionale nello spazio latente. Solo una regione ristretta dello spazio genera immagini semanticamente significative, mentre le restanti producono rumore.}
    \label{fig:latent_manifold}
\end{figure}
Questa osservazione suggerisce che lo spazio latente appreso dall’autoencoder non sia densamente popolato da rappresentazioni valide, ma presenti una struttura complessa con regioni semanticamente rilevanti e regioni di vuoto. Si invita già ora il lettore a tenere a mente questo fatto, ovvero che non tutto lo spazio latente delle variabili latenti è sondato, ma solo alcune di queste esprimono un significato.
\subsubsection{Apprendimento non supervisionato}
Gli autoencoders sono modelli di apprendimento non supervisionato, in quanto
non richiedono etichette associate ai dati di input durante la fase di
addestramento. Consideriamo un dataset di addestramento
$S_T$ costituito da $M$ osservazioni non etichettate $\mathbf{x}_i$, con
$i = 1, \dots, M$:
\begin{equation}
    S_T = \{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_M\}.
\end{equation}
In generale, ciascuna osservazione appartiene allo spazio
$\mathbb{R}^n$, ovvero $\mathbf{x}_i \in \mathbb{R}^n$. L’obiettivo di un
autoencoder è apprendere una rappresentazione dei dati tale da permettere la
ricostruzione dell’input nel modo più accurato possibile, minimizzando una
misura dell’errore di ricostruzione. L’interesse verso questo tipo di modelli risiede nel fatto che la
rappresentazione latente appresa può essere utilizzata in numerose
applicazioni, come la riduzione della dimensionalità, l’estrazione di
caratteristiche, il denoising e l’anomaly detection. Una definizione formale
di autoencoder è la seguente.

\begin{notebox}
\textbf{Autoencoder} \textit{Un autoencoder è un tipo di algoritmo il cui
scopo principale è apprendere una rappresentazione dei dati, utilizzabile per
diverse applicazioni, imparando a ricostruire in modo sufficientemente
accurato un insieme di osservazioni di input}
\parencite{autoencodersbank2020autoencoders}.
\end{notebox}

\subsubsection{Encoder, decoder e spazio latente}

Un autoencoder è composto da due blocchi principali: un \textbf{encoder} e un
\textbf{decoder}. La struttura generale del modello è illustrata in
Figura~\ref{fig:autoencoder}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{pictures/autoencoder.png}
    \caption{Schema di funzionamento di un autoencoder \parencite{michelucci2022introductionautoencoders}}
    \label{fig:autoencoder}
\end{figure}

Nella maggior parte dei casi, l’encoder e il decoder sono implementati come
reti neurali. Seguendo l’impostazione descritta in
\parencite{michelucci2022introductionautoencoders}, l’encoder può essere
rappresentato come una funzione $g$, dipendente da un insieme di parametri
apprendibili, che associa a ciascun dato di input una rappresentazione nello
spazio latente:
\begin{equation}
    \mathbf{h}_i = g(\mathbf{x}_i).
\end{equation}
Qui $\mathbf{h}_i \in \mathbb{R}^q$ rappresenta il vettore delle
\textit{features latenti} ed è l’output del blocco di encoder quando la
funzione $g$ viene valutata sull’input $\mathbf{x}_i$. Ne consegue che
l’encoder realizza una mappatura del tipo
\begin{equation}
    g : \mathbb{R}^n \rightarrow \mathbb{R}^q.
\end{equation}

Il decoder ha il compito di ricostruire il dato originale a partire dalla
rappresentazione latente. L’output della rete, indicato con
$\hat{\mathbf{x}}_i$, può essere espresso tramite una seconda funzione
generica $f$:
\begin{equation}
    \hat{\mathbf{x}}_i = f(\mathbf{h}_i) = f(g(\mathbf{x}_i)),
\end{equation}
dove $\hat{\mathbf{x}}_i \in \mathbb{R}^n$ rappresenta la ricostruzione
dell’input $\mathbf{x}_i$.

\subsubsection{Funzione obiettivo e errore di ricostruzione}

L’addestramento di un autoencoder consiste nel determinare le funzioni
$g(\cdot)$ e $f(\cdot)$ tali da minimizzare una misura della discrepanza tra i
dati di input e le rispettive ricostruzioni. Formalmente, il problema di
ottimizzazione può essere espresso come
\begin{equation}
    \arg \min_{f,g}
    \left\langle
    \Delta\!\left(\mathbf{x}_i, f(g(\mathbf{x}_i))\right)
    \right\rangle,
\end{equation}
dove $\Delta$ indica una funzione di perdita che quantifica la differenza tra
l’input e l’output dell’autoencoder, mentre $\langle \cdot \rangle$ denota la
media su tutte le osservazioni del dataset di addestramento. Scelta quindi l'architettura dell'autoencoder, il problema di training sarà quello di minimarre un errore di ricostruzione tramite la minimizzazione della funzione di costo
\begin{equation}
    L = ||\mathbf{x}_i-\hat{\mathbf{x}_i}|| = ||\mathbf{x}_i-f(\mathbf{h}_i)||
\end{equation}

\subsection{Il problema dell’identità e la necessità di vincoli}

In assenza di vincoli sull’architettura o sulla funzione obiettivo, un
autoencoder dotato di capacità sufficiente può apprendere una semplice
funzione identità, ottenendo una ricostruzione perfetta ma priva di utilità
pratica. Per evitare questo comportamento degenerato, è comune introdurre
specifiche strategie di regolarizzazione, come la presenza di una
strozzatura dimensionale nello spazio latente oppure l’aggiunta di termini di
regolarizzazione alla funzione di costo.

\begin{notebox}
\textbf{Nota.} Un autoencoder efficace deve bilanciare due obiettivi
contrastanti: da un lato una ricostruzione sufficientemente accurata
dell’input, dall’altro l’apprendimento di una rappresentazione latente che
catturi le caratteristiche essenziali dei dati, evitando soluzioni banali come
l’identità.
\end{notebox}


\subsubsection{Bottleneck e riduzione della dimensionalità}

Al fine di evitare che l’autoencoder apprenda una banale funzione identità e di
favorire l’apprendimento di rappresentazioni astratte e informative dei dati,
una strategia comunemente adottata consiste nell’imporre una riduzione della
dimensionalità tra lo spazio di input e lo spazio latente. Tale configurazione
architetturale prende il nome di \textbf{bottleneck} o \textbf{strozzatura}. In un’architettura con bottleneck, la dimensione dello spazio latente $q$ è
strettamente inferiore alla dimensione dell’input $n$ ($q < n$). In queste
condizioni, l’encoder realizza una mappatura che comprime l’informazione
contenuta nei dati di ingresso:
\begin{equation}
    g : \mathbb{R}^n \rightarrow \mathbb{R}^q, \quad q < n,
    \label{eq:latent_dim_ae}
\end{equation}
costringendo il modello a selezionare e preservare esclusivamente le componenti
più rilevanti dell’input ai fini della ricostruzione. La presenza della strozzatura impedisce quindi una copia diretta dei dati e
spinge l’autoencoder a catturare strutture, correlazioni e regolarità latenti
presenti nel dataset. Al termine dell’addestramento, lo spazio latente
costituisce una rappresentazione compatta e astratta dei dati, che può essere
interpretata come una codifica delle caratteristiche essenziali dell’input e
utilizzata per compiti successivi quali riduzione della dimensionalità,
visualizzazione o analisi delle feature.

%%% qui metteremo alcuni esperimenti fatti con i dati del mnist succeccessivamente

\subsubsection{Introduzione di vincoli}

Oltre alla strozzatura architetturale, un ulteriore approccio per evitare che
l’autoencoder apprenda una semplice funzione identità consiste
nell’introduzione di vincoli aggiuntivi nella funzione obiettivo, tipicamente
sotto forma di termini di regolarizzazione. Tali vincoli agiscono limitando la
capacità espressiva del modello o penalizzando soluzioni considerate
indesiderabili, favorendo l’apprendimento di rappresentazioni latenti più
strutturate e informative.

In questo contesto, la funzione di costo dell’autoencoder non si limita più a
misurare esclusivamente l’errore di ricostruzione, ma include uno o più termini
addizionali che impongono specifiche proprietà alla rappresentazione latente o
ai parametri del modello. In forma generale, il problema di ottimizzazione può
essere scritto come
\begin{equation}
    \arg \min_{f,g}
    \left\langle
    \Delta\!\left(\mathbf{x}_i, f(g(\mathbf{x}_i))\right)
    \right\rangle
    + \lambda \, \Omega(g,f),
\end{equation}
dove $\Omega(g,f)$ rappresenta un termine di regolarizzazione e $\lambda > 0$
ne controlla l’importanza relativa rispetto all’errore di ricostruzione.
A seconda della scelta del termine di regolarizzazione, è possibile indurre
diverse proprietà nel modello. Ad esempio, la penalizzazione della norma dei
pesi limita la complessità della rete e migliora la capacità di
generalizzazione, mentre vincoli applicati direttamente allo spazio latente
possono favorire caratteristiche quali la \textbf{sparsità}, la robustezza al
rumore o la separazione delle feature. In particolare, l’introduzione di
vincoli di sparsità sulle attivazioni latenti costituisce il principio alla
base degli \textit{Sparse Autoencoders}, che verranno discussi nel seguito.

Un esempio comune di regolarizzazione consiste nell’introdurre un vincolo
direttamente sulle attivazioni dello spazio latente. In questo caso, la
funzione obiettivo dell’autoencoder assume la forma
\begin{equation}
    \arg \min_{f,g}
    \left\langle
    \Delta\!\left(\mathbf{x}_i, f(g(\mathbf{x}_i))\right)
    \right\rangle
    + \lambda \lVert \mathbf{h}_i \rVert_2^2,
\end{equation}
dove $\mathbf{h}_i = g(\mathbf{x}_i)$ denota il vettore delle attivazioni
latenti associate all’osservazione $\mathbf{x}_i$. Tale penalizzazione di tipo
$\ell_2$ scoraggia rappresentazioni latenti di grande norma, favorendo
codifiche più compatte e contribuendo alla stabilità del modello.

Un’alternativa è rappresentata dalla regolarizzazione di tipo $\ell_1$
applicata allo spazio latente:
\begin{equation}
    \arg \min_{f,g}
    \left\langle
    \Delta\!\left(\mathbf{x}_i, f(g(\mathbf{x}_i))\right)
    \right\rangle
    + \lambda \lVert \mathbf{h}_i \rVert_1.
\end{equation}
A differenza della norma $\ell_2$, la regolarizzazione $\ell_1$ tende a produrre
rappresentazioni sparse, in cui solo un numero limitato di componenti del
vettore latente risulta attivo per ciascun input. Questo comportamento
favorisce una decomposizione più interpretabile delle feature e costituisce il
principio alla base degli \textit{Sparse Autoencoders}, che verranno analizzati
nel seguito.

L’aggiunta di vincoli nella funzione obiettivo consente quindi di superare i
limiti degli autoencoders classici, guidando l’apprendimento verso soluzioni
non banali e semanticamente più significative, anche in assenza di una
riduzione esplicita della dimensionalità dello spazio latente.


\subsubsection{Relazioni con la PCA}

Dal momento che gli autoencoders possono essere utilizzati per la riduzione
della dimensionalità dei dati, è di interesse evidenziare la loro relazione con
il metodo delle \textit{Principal Component Analysis} (PCA). La PCA è una
tecnica di analisi statistica che consente di ridurre la dimensionalità di un
dataset preservando la maggior parte della varianza presente nei dati
originali, mediante una trasformazione lineare delle variabili.

Sia dato un dataset di $M$ osservazioni centrate
\begin{equation*}
    \mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_M \in \mathbb{R}^n,
    \qquad
    \frac{1}{M} \sum_{i=1}^{M} \mathbf{x}_i = 0.
\end{equation*}
Definendo la matrice dei dati
\begin{equation*}
    X =
    \begin{bmatrix}
    \mathbf{x}_1^\top \\
    \mathbf{x}_2^\top \\
    \vdots \\
    \mathbf{x}_M^\top
    \end{bmatrix}
    \in \mathbb{R}^{M \times n},
\end{equation*}
la matrice di covarianza empirica è data da
\begin{equation*}
    C = \frac{1}{M} X^\top X \in \mathbb{R}^{n \times n}.
\end{equation*}



L’obiettivo della PCA consiste nell’individuare una direzione unitaria
$\mathbf{w}_1 \in \mathbb{R}^n$ lungo la quale la proiezione dei dati presenti la
massima varianza. Indicando con $y_i = \mathbf{w}_1^\top \mathbf{x}_i$ la
proiezione dell’osservazione $\mathbf{x}_i$ lungo tale direzione, la varianza
dei dati proiettati può essere espressa come
\begin{equation*}
    \mathrm{Var}(X\mathbf{w}_1)
    =
    \frac{1}{M} \sum_{i=1}^{M}
    \left( \mathbf{w}_1^\top \mathbf{x}_i \right)^2,
\end{equation*}
dove si è utilizzato il fatto che i dati sono centrati, e quindi la media delle
proiezioni risulta nulla. Riscrivendo la precedente espressione in forma
matriciale si ottiene
\begin{equation*}
    \mathrm{Var}(X\mathbf{w}_1)
    =
    \mathbf{w}_1^\top
    \left(
        \frac{1}{M} \sum_{i=1}^{M} \mathbf{x}_i \mathbf{x}_i^\top
    \right)
    \mathbf{w}_1
    =
    \mathbf{w}_1^\top C \mathbf{w}_1.
\end{equation*}





Il problema della ricerca della direzione di massima varianza può quindi essere
formulato come il seguente problema di ottimizzazione vincolata:
\begin{equation}
    \max_{\|\mathbf{w}_1\|_2 = 1}
    \mathbf{w}_1^\top C \mathbf{w}_1.
\end{equation}

Tale problema può essere risolto mediante il metodo dei moltiplicatori di
Lagrange, introducendo la lagrangiana
\begin{equation*}
    L(\mathbf{w}, \lambda)
    =
    \mathbf{w}^\top C \mathbf{w}
    -
    \lambda (\mathbf{w}^\top \mathbf{w} - 1).
\end{equation*}
Imponendo la condizione di stazionarietà rispetto a $\mathbf{w}$ si ottiene
\begin{equation*}
    \nabla_{\mathbf{w}} L(\mathbf{w}, \lambda)
    =
    2 C \mathbf{w} - 2 \lambda \mathbf{w}
    =
    0,
\end{equation*}
da cui segue il problema agli autovalori
\begin{equation}
    C \mathbf{w} = \lambda \mathbf{w}.
\end{equation}

Le soluzioni ammissibili sono pertanto gli autovettori di $C$, mentre i
moltiplicatori di Lagrange coincidono con i corrispondenti autovalori. La
derivata della lagrangiana rispetto a $\lambda$ restituisce inoltre il vincolo
di normalizzazione
\begin{equation*}
    \mathbf{w}^\top \mathbf{w} = 1.
\end{equation*}

Sia $\mathbf{v}_k$ un autovettore unitario di $C$ associato all’autovalore
$\lambda_k$. Per tali vettori vale
\begin{equation*}
    \mathrm{Var}(X\mathbf{v}_k)
    =
    \mathbf{v}_k^\top C \mathbf{v}_k
    =
    \lambda_k,
\end{equation*}
ossia ciascun autovalore rappresenta la varianza dei dati lungo la
corrispondente direzione $\mathbf{v}_k$.

Ordinando gli autovalori in ordine decrescente
\begin{equation*}
    \lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_n \ge 0,
\end{equation*}
le direzioni associate agli autovalori maggiori individuano le componenti
principali del dataset. In particolare, la prima componente principale
$\mathbf{v}_1$ è la direzione di massima varianza, mentre le componenti
successive massimizzano la varianza residua sotto il vincolo di ortogonalità
rispetto alle precedenti.


\paragraph{Caso di un autoencoder lineare}

Si consideri un autoencoder costituito da un encoder e un decoder entrambi
lineari, addestrato su dati centrati. L’encoder realizza una mappatura del tipo
\begin{equation*}
    \mathbf{h} = W \mathbf{x},
\end{equation*}
dove $W \in \mathbb{R}^{q \times n}$ e $q < n$ è la dimensione dello spazio
latente. Il decoder ricostruisce l’input mediante
\begin{equation*}
    \hat{\mathbf{x}} = W^\top \mathbf{h} = W^\top W \mathbf{x},
\end{equation*}
dove, senza perdita di generalità, si è assunto che i pesi del decoder siano
vincolati a essere la trasposta di quelli dell’encoder.

L’addestramento dell’autoencoder consiste nel minimizzare l’errore quadratico
medio di ricostruzione:
\begin{equation}
    \mathcal{L}(W)
    =
    \frac{1}{M} \sum_{i=1}^{M}
    \left\|
        \mathbf{x}_i - W^\top W \mathbf{x}_i
    \right\|_2^2.
\end{equation}

Osservando che $W^\top W$ è una matrice simmetrica di rango al più $q$, tale
termine può essere interpretato come una proiezione lineare sul sottospazio
generato dalle righe di $W$. L’errore di ricostruzione misura quindi la distanza
tra ciascun dato e la sua proiezione su tale sottospazio.

Sfruttando l’ipotesi di dati centrati, la funzione di costo può essere riscritta
come
\begin{equation}
    \mathcal{L}(W)
    =
    \frac{1}{M} \sum_{i=1}^{M}
    \|\mathbf{x}_i\|_2^2
    -
    \frac{1}{M} \sum_{i=1}^{M}
    \|W \mathbf{x}_i\|_2^2,
\end{equation}
dove il primo termine è indipendente da $W$. Ne consegue che minimizzare
l’errore di ricostruzione equivale a massimizzare la quantità
\begin{equation}
    \frac{1}{M} \sum_{i=1}^{M}
    \|W \mathbf{x}_i\|_2^2.
\end{equation}

Indicando con $\mathbf{w}_1, \dots, \mathbf{w}_q$ le righe di $W$, si ottiene
\begin{equation}
    \frac{1}{M} \sum_{i=1}^{M}
    \|W \mathbf{x}_i\|_2^2
    =
    \sum_{j=1}^{q}
    \frac{1}{M} \sum_{i=1}^{M}
    (\mathbf{w}_j^\top \mathbf{x}_i)^2
    =
    \sum_{j=1}^{q}
    \mathrm{Var}(X \mathbf{w}_j),
\end{equation}
ossia la somma delle varianze dei dati proiettati lungo le direzioni
$\mathbf{w}_j$.

Pertanto, il problema di addestramento dell’autoencoder lineare equivale alla
ricerca di $q$ direzioni ortonormali che massimizzino la varianza totale dei
dati proiettati. Questo coincide esattamente con il problema risolto dalla PCA,
la cui soluzione è fornita dagli autovettori della matrice di covarianza
associati ai $q$ maggiori autovalori.

In particolare, il minimo della funzione di costo è ottenuto quando le righe di
$W$ coincidono (a meno di una trasformazione ortogonale) con gli autovettori
$\mathbf{v}_1, \dots, \mathbf{v}_q$ associati agli autovalori
$\lambda_1 \ge \dots \ge \lambda_q$. In tal caso vale
\begin{equation*}
    W^\top W = P_q,
\end{equation*}
dove $P_q$ denota il proiettore ortogonale sul sottospazio generato dalle prime
$q$ componenti principali.

Ne consegue che un autoencoder lineare, addestrato mediante minimizzazione
dell’errore quadratico medio, apprende lo stesso sottospazio individuato dalla
PCA. Le coordinate latenti possono differire da quelle ottenute tramite PCA per
una trasformazione ortogonale, ma lo spazio latente appreso coincide con lo
span delle prime $q$ componenti principali, mostrando come la PCA possa essere
interpretata come un caso particolare di autoencoder lineare.

\paragraph{Caso di un autoencoder non lineare}

Si consideri ora un autoencoder in cui almeno uno tra encoder e decoder è una
funzione non lineare. In particolare, si assuma un encoder del tipo
\begin{equation*}
    \mathbf{h} = g(\mathbf{x}) = \sigma(W \mathbf{x} + \mathbf{b}),
\end{equation*}
dove $\sigma(\cdot)$ è una funzione di attivazione non lineare applicata
elemento per elemento, mentre il decoder ricostruisce l’input mediante una
funzione generica
\begin{equation*}
    \hat{\mathbf{x}} = f(\mathbf{h}).
\end{equation*}

L’addestramento dell’autoencoder consiste ancora nella minimizzazione
dell’errore quadratico medio di ricostruzione:
\begin{equation}
    \mathcal{L}(f,g)
    =
    \frac{1}{M} \sum_{i=1}^{M}
    \left\|
        \mathbf{x}_i - f(g(\mathbf{x}_i))
    \right\|_2^2.
\end{equation}

A differenza del caso lineare, la mappatura complessiva
$\mathbf{x} \mapsto \hat{\mathbf{x}}$ non è più una proiezione lineare su un
sottospazio di dimensione ridotta. Di conseguenza, la funzione di costo non può
essere riscritta in termini di varianza proiettata, né ricondotta a un problema
agli autovalori della matrice di covarianza. In particolare, non è più possibile
esprimere l’errore di ricostruzione come differenza tra una quantità costante e
la varianza dei dati proiettati lungo un insieme di direzioni fisse.

L’autoencoder non lineare è quindi in grado di catturare strutture complesse e
non lineari presenti nel dataset, che non possono essere rappresentate in modo
efficace mediante una combinazione lineare di componenti principali rendendo possibile
l’apprendimento di rappresentazioni latenti più flessibili e adatte a dati che
giacciono approssimativamente su varietà non lineari.


\subsection{Interpretabilità delle feature latenti}

Uno degli obiettivi centrali nell’apprendimento di rappresentazioni è ottenere
codifiche latenti che non siano solamente utili per la ricostruzione dei dati,
ma anche interpretabili dal punto di vista umano. Nel contesto degli
autoencoders, tale interpretabilità è strettamente legata alla capacità del
modello di catturare e separare i fattori di variazione che governano la
generazione dei dati osservati.



\begin{notebox}
\textbf{Definizione (Fattori di variazione).}
Si definiscono \textit{fattori di variazione} le variabili latenti, generalmente
non osservabili, che parametrizzano il processo generativo dei dati e ne
determinano le principali modalità di cambiamento. Ciascun fattore di variazione
corrisponde a una dimensione semantica distinta secondo cui le osservazioni
possono variare, come ad esempio la forma, la posizione, l’orientamento, il
colore o la presenza di specifici oggetti. \parencite{wang2024disentangledrepresentationlearning}
\end{notebox}



L’introduzione di una strozzatura nello spazio latente o di vincoli di
regolarizzazione nella funzione obiettivo costringe l’autoencoder a comprimere
l’informazione contenuta nei dati di input, preservando principalmente gli
aspetti rilevanti ai fini della ricostruzione. In linea di principio, questo
processo può favorire l’apprendimento di rappresentazioni latenti che riflettono
i fattori di variazione sottostanti ai dati, anziché limitarsi a una
memorizzazione non strutturata delle osservazioni.

In uno scenario ideale, le componenti dello spazio latente risultano
semanticamente interpretabili: la variazione di una singola variabile latente
corrisponde a una modifica controllata e riconoscibile di un attributo
specifico dell’osservazione ricostruita. In tal caso, i valori quantitativi
assunti dalle feature latenti possono essere ricondotti a descrizioni
qualitative comprensibili, rendendo lo spazio latente non solo compatto, ma
anche concettualmente significativo.

Tuttavia, nella pratica, l’interpretabilità delle feature latenti non è
garantita. Gli autoencoders standard sono addestrati esclusivamente per
minimizzare l’errore di ricostruzione e tendono pertanto a organizzare lo spazio
latente in modo funzionale a tale obiettivo, senza alcuna esplicita pressione a
separare o strutturare semanticamente l’informazione. Di conseguenza, le
rappresentazioni apprese risultano spesso difficili da interpretare e
caratterizzate da una forte mescolanza dei fattori di variazione.

\subsubsection{Rappresentazioni latenti disentangled}

Una rappresentazione latente si dice \textit{disentangled} quando i diversi
fattori di variazione che descrivono i dati sono codificati in componenti
latenti distinte e, idealmente, statisticamente indipendenti. In una tale
rappresentazione, ciascuna variabile latente controlla un singolo fattore di
variazione, mentre risulta invariata rispetto agli altri.

In presenza di una rappresentazione disentangled, la manipolazione di una
singola dimensione dello spazio latente produce una variazione interpretabile e
localizzata nell’output ricostruito, senza influenzare gli altri attributi
dell’osservazione. Questa proprietà rende le rappresentazioni disentangled
particolarmente desiderabili in applicazioni quali l’analisi esplorativa dei
dati, il controllo generativo, la robustezza a variazioni spurie e il
trasferimento di conoscenza tra domini.

Nonostante il loro interesse teorico e pratico, le rappresentazioni
disentangled non emergono spontaneamente nell’addestramento di autoencoders
classici. La sola presenza di una strozzatura dimensionale non è sufficiente a
garantire la separazione dei fattori di variazione, e in molti casi il modello
apprende combinazioni complesse e non interpretabili di tali fattori, dando
luogo a rappresentazioni \textit{entangled}.

Per favorire l’apprendimento di rappresentazioni disentangled è quindi
necessario introdurre vincoli aggiuntivi o specifiche scelte architetturali e
di regolarizzazione. Tra queste rientrano l’imposizione di sparsità nello spazio
latente, la promozione dell’indipendenza statistica tra le feature, o
l’introduzione di termini di penalizzazione che incoraggino una separazione
esplicita dei fattori di variazione. Tali strategie costituiscono la base di
numerosi modelli avanzati, tra cui gli \textit{Sparse Autoencoders}, che verranno
analizzati nel seguito.

\subsection{Sparse Autoencoders}

Una possibile strategia per favorire l’apprendimento di rappresentazioni latenti
astratte, disentangled e interpretabili consiste nell’introdurre esplicitamente
vincoli di sparsità sulle attivazioni dello spazio latente. I modelli che adottano
questa impostazione prendono il nome di \textbf{Sparse Autoencoders}.

A differenza degli autoencoder classici con strozzatura (bottleneck), nei quali la
capacità di rappresentazione è limitata riducendo la dimensionalità dello spazio
latente, negli Sparse Autoencoders si abbandona tale vincolo architetturale a
favore di un vincolo di sparsità sulle attivazioni. In questo caso, lo spazio
latente può avere dimensione pari o superiore a quella dell’input, e l’encoder
realizza una mappatura del tipo
\begin{equation}
    g : \mathbb{R}^n \rightarrow \mathbb{R}^q, \quad q \ge n,
\end{equation}
richiedendo tuttavia che, per ciascun input, solo una frazione limitata delle
unità latenti risulti significativamente attiva.

\begin{figure}[htbp] 
    \centering 
    \includegraphics[width=0.5\linewidth]{pictures/SAE_diagram.png} 
    \caption{La figura mostra l'architettura di uno Sparse Autoencoder nel quale la dimensione dello stato latente è maggiore di quella di input.} 
    \label{fig:sparse_autoencoder} 
\end{figure}

L’idea centrale è che, pur disponendo di uno spazio latente ad alta dimensionalità,
il modello sia costretto a rappresentare ogni osservazione utilizzando un numero
ridotto di componenti. Questo comportamento induce una codifica selettiva, nella
quale le singole unità latenti tendono a rispondere a pattern o attributi
specifici dei dati, favorendo rappresentazioni più strutturate e potenzialmente
interpretabili.

Formalmente, dati un encoder $g_\theta$ e un decoder $f_\phi$, la funzione obiettivo
di uno Sparse Autoencoder può essere espressa come
\begin{equation}
    \mathcal{L}(f_\phi, g_\theta)
    =
    \left\langle
        \Delta\!\left(\mathbf{x}_i, f_\phi(g_\theta(\mathbf{x}_i))\right)
    \right\rangle
    +
    \lambda \, \mathcal{R}_\text{sparse}\big(g_\theta(\mathbf{x}_i)\big),
\end{equation}
dove $\mathcal{R}_\text{sparse}(\cdot)$ è un termine di regolarizzazione che impone
vincoli di sparsità sulle attivazioni latenti.

Una delle scelte più comuni consiste nell’applicare una penalizzazione di tipo
$\ell_1$ alle attivazioni latenti,
\begin{equation}
    \mathcal{R}_\text{sparse}(\mathbf{h}_i) = \lVert \mathbf{h}_i \rVert_1,
\end{equation}
che incoraggia soluzioni in cui molte componenti del vettore latente sono nulle o
prossime allo zero. In alternativa, è possibile imporre vincoli di tipo
\textit{top-$k$} (o $k$–sparsity), nei quali, per ciascun input, solo le $k$
attivazioni di maggiore ampiezza vengono mantenute, mentre tutte le altre sono
forzate a zero. Questo approccio impone una sparsità esplicita e controllata,
indipendente dalla scala delle attivazioni.

Sebbene la sparsità non garantisca in senso rigoroso una completa separazione
statistica dei fattori di variazione, essa introduce una forte pressione
strutturale sulla rappresentazione latente, riducendo la codifica diffusa
dell’informazione e favorendo l’emergere di feature più selettive e spesso
\textit{monosemantiche}. Per questo motivo, gli Sparse Autoencoders costituiscono
uno strumento particolarmente efficace per l’analisi e l’interpretazione di
rappresentazioni dense apprese da modelli complessi, oltre a rappresentare una
base concettuale naturale per lo studio del disentanglement.
