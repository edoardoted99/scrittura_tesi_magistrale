\section{Risultati sperimentali}
\label{sec:06_risultati}

\epigraph{
    In God we trust; all others must bring data.
}{W. Edwards Deming}

\newpage

\subsection{Setup sperimentale}
\label{subsec:setup}
Prima di presentare i risultati ottenuti con PRISMA, è necessario descrivere l'infrastruttura hardware e software su cui sono stati condotti gli esperimenti. La scelta della piattaforma non è stata casuale: l'addestramento di Sparse Autoencoder e l'esecuzione di Large Language Models per l'interpretazione automatica delle feature richiedono risorse computazionali significative, in particolare per quanto riguarda la memoria disponibile. Inoltre, la natura sensibile dei dati trattati—cartelle cliniche pediatriche—ha imposto vincoli stringenti sulla localizzazione dell'elaborazione.
\subsubsection{Infrastruttura hardware}
\label{subsubsec:hardware}
Gli esperimenti sono stati condotti su un \textbf{Mac Studio} equipaggiato con chip \textbf{Apple M3 Ultra} e \textbf{512 GB} di memoria unificata. La Tabella~\ref{tab:hardware_specs} riassume le specifiche tecniche della macchina.
\begin{table}[htbp]
    \centering
    \caption{Specifiche hardware della macchina utilizzata per gli esperimenti.}
    \label{tab:hardware_specs}
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{Componente} & \textbf{Specifica} \\
        \midrule
        Modello & Mac Studio (Mac15,14) \\
        Chip & Apple M3 Ultra \\
        Core CPU & 32 (24 performance + 8 efficiency) \\
        Core GPU & 80 \\
        Neural Engine & 32 core \\
        Memoria unificata & 512 GB \\
        Bandwidth memoria & 819 GB/s \\
        \bottomrule
    \end{tabular}
\end{table}
La scelta di questa piattaforma è motivata da due fattori: una caratteristica architetturale distintiva dei chip Apple Silicon e un requisito di privacy imposto dalla natura dei dati.
\paragraph{La Unified Memory Architecture.}
A differenza delle architetture tradizionali, dove CPU e GPU dispongono di memorie separate (RAM di sistema e VRAM dedicata), nei chip Apple Silicon tutti i componenti computazionali—CPU, GPU e Neural Engine—condividono lo stesso pool di memoria fisica. Questa architettura, denominata \textbf{Unified Memory Architecture} (UMA), elimina un collo di bottiglia fondamentale nel deep learning: il trasferimento dati tra CPU e GPU.

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[
        scale=0.95,
        block/.style={draw, rectangle, rounded corners=4pt, minimum height=1.4cm, minimum width=2.8cm, font=\small, fill=#1, drop shadow={opacity=0.15}},
        memory/.style={draw, rectangle, rounded corners=6pt, minimum height=1.6cm, minimum width=11cm, font=\small\bfseries, fill=green!15, drop shadow={opacity=0.2}},
        arrow/.style={->, thick, gray!60},
        biarrow/.style={<->, very thick, green!50!black}
    ]
        
        % Contenitore chip (sfondo)
        \fill[gray!8, rounded corners=10pt] (-6, -0.5) rectangle (6, 4.2);
        \draw[thick, rounded corners=10pt, gray!40] (-6, -0.5) rectangle (6, 4.2);
        
        % Titolo chip
        \node[font=\normalsize\bfseries, gray!70] at (0, 3.7) {Apple M3 Ultra SoC};
        
        % CPU block
        \node[block=blue!25] (cpu) at (-3.5, 1.9) {
            \begin{tabular}{c}
                \textbf{CPU} \\
                \scriptsize 24 Performance \\
                \scriptsize 8 Efficiency
            \end{tabular}
        };
        
        % GPU block
        \node[block=orange!25] (gpu) at (0, 1.9) {
            \begin{tabular}{c}
                \textbf{GPU} \\
                \scriptsize 80 cores \\
                \scriptsize Ray Tracing
            \end{tabular}
        };
        
        % Neural Engine block
        \node[block=violet!25] (ne) at (3.5, 1.9) {
            \begin{tabular}{c}
                \textbf{Neural Engine} \\
                \scriptsize 32 cores \\
                \scriptsize 31 TOPS
            \end{tabular}
        };
        
        % Memory controller (barra)
        \fill[gray!20, rounded corners=3pt] (-5, 0) rectangle (5, 0.5);
        \draw[gray!50, rounded corners=3pt] (-5, 0) rectangle (5, 0.5);
        \node[font=\scriptsize\bfseries, gray!70] at (0, 0.25) {Memory Fabric · 819 GB/s};
        
        % Frecce dai componenti al memory controller
        \draw[arrow, line width=1.2pt] (cpu.south) -- ++(0, -0.35);
        \draw[arrow, line width=1.2pt] (gpu.south) -- ++(0, -0.35);
        \draw[arrow, line width=1.2pt] (ne.south) -- ++(0, -0.35);
        
        % Unified Memory (sotto)
        \node[memory] (mem) at (0, -1.7) {Unified Memory — 512 GB LPDDR5};
        
        % Freccia bidirezionale memory controller <-> memoria
        \draw[biarrow, line width=2pt] (0, -0.1) -- (0, -0.9);
        
        % Annotazioni software (fuori dal chip)
        \node[font=\scriptsize, blue!60!black, align=center] (ann1) at (-3.5, -3.5) {PyTorch MPS\\SAE Training};
        \draw[->, blue!50, thick, dashed] (ann1.north) -- (-3.5, -2.5);
        
        \node[font=\scriptsize, orange!70!black, align=center] (ann2) at (0, -3.5) {BERT\\Embeddings};
        \draw[->, orange!50, thick, dashed] (ann2.north) -- (0, -2.5);
        
        \node[font=\scriptsize, violet!60!black, align=center] (ann3) at (3.5, -3.5) {Ollama\\LLM Interpreter};
        \draw[->, violet!50, thick, dashed] (ann3.north) -- (3.5, -2.5);
        
        % Nota zero-copy
        \node[draw, rounded corners=3pt, fill=yellow!10, font=\scriptsize, inner sep=4pt] at (0, -4.5) {Zero-copy: tutti i componenti accedono agli stessi dati senza trasferimenti};
        
    \end{tikzpicture}
    \caption{Architettura del chip Apple M3 Ultra con Unified Memory Architecture (UMA). I tre componenti computazionali—CPU, GPU e Neural Engine—condividono lo stesso pool di 512 GB di memoria LPDDR5, accessibile attraverso un memory fabric con bandwidth aggregato di 819 GB/s. Il \textbf{Neural Engine} è un acceleratore hardware dedicato alle operazioni di machine learning (moltiplicazioni matriciali, convoluzioni), capace di 31 trilioni di operazioni al secondo (TOPS); viene sfruttato da Ollama per accelerare l'inferenza dei Large Language Models utilizzati come interpreti delle feature. L'architettura elimina i trasferimenti CPU-GPU tipici dei sistemi con memorie separate, abilitando workflow che alternano preprocessing (CPU), training (GPU) e inferenza LLM (Neural Engine) sugli stessi dati.}
    \label{fig:m3_ultra_architecture}
\end{figure}
\paragraph{Il collo di bottiglia nelle architetture tradizionali.}
Per comprendere il vantaggio della UMA, è utile considerare come funzionano le architetture convenzionali. In un sistema con CPU e GPU discrete, i due processori dispongono di memorie fisicamente separate: la RAM di sistema (accessibile alla CPU) e la VRAM (accessibile alla GPU). Quando si addestra una rete neurale, i dati devono essere copiati dalla RAM alla VRAM prima di ogni operazione su GPU, e i risultati devono essere copiati indietro per l'elaborazione su CPU.
Il canale di comunicazione tra CPU e GPU è il bus \textbf{PCIe} (Peripheral Component Interconnect Express). Anche nelle configurazioni più recenti, il bandwidth di questo canale è ordini di grandezza inferiore a quello della memoria locale:
\begin{table}[htbp]
    \centering
    \caption{Confronto del memory bandwidth tra architetture. Il bandwidth misura quanti dati possono essere trasferiti al secondo tra memoria e processori—un fattore critico per workload memory-bound come l'inferenza di LLM.}
    \label{tab:bandwidth_comparison}
    \begin{tabular}{@{}lrl@{}}
        \toprule
        \textbf{Interfaccia} & \textbf{Bandwidth} & \textbf{Note} \\
        \midrule
        PCIe 4.0 x16 & 32 GB/s & Standard attuale per GPU \\
        PCIe 5.0 x16 & 64 GB/s & Ultima generazione \\
        \midrule
        NVIDIA RTX 4090 (VRAM) & 1.008 GB/s & Memoria locale GPU \\
        NVIDIA A100 80GB (VRAM) & 2.039 GB/s & GPU professionale \\
        \midrule
        \textbf{Apple M3 Ultra (UMA)} & \textbf{819 GB/s} & Memoria unificata \\
        \bottomrule
    \end{tabular}
\end{table}
Il problema emerge chiaramente: anche se la VRAM di una GPU NVIDIA ha bandwidth elevatissimo (1-2 TB/s), i dati devono prima \textit{attraversare} il bus PCIe, che è 15-30 volte più lento. Per workload che richiedono frequenti trasferimenti CPU-GPU—come l'addestramento di autoencoder con preprocessing su CPU—questo diventa il collo di bottiglia dominante.
\begin{notebox}
\textbf{Analogia: il magazzino e la porta}\\[0.5em]
Il memory bandwidth può essere compreso con un'analogia. Immaginiamo la memoria come un magazzino e i processori (CPU/GPU) come operai che devono lavorare i materiali:
\begin{itemize}
    \item La \textbf{capacità della memoria} (512 GB) corrisponde alla dimensione del magazzino
    \item Il \textbf{memory bandwidth} (819 GB/s) corrisponde alla larghezza della porta
\end{itemize}
Un magazzino enorme con una porta stretta costringe gli operai ad aspettare i materiali. Nelle architetture tradizionali, la ``porta'' tra CPU e GPU (il bus PCIe) è molto più stretta della ``porta'' verso le rispettive memorie locali. Con la UMA, CPU e GPU condividono lo stesso magazzino e la stessa porta—larga 819 GB/s.
\end{notebox}
Con la Unified Memory Architecture, questo collo di bottiglia scompare. Un tensore allocato in memoria è immediatamente accessibile sia dalla CPU che dalla GPU, senza alcuna copia (\textit{zero-copy}). Il bandwidth di 819 GB/s—25 volte superiore a PCIe 4.0—è disponibile per \textit{tutti} i componenti simultaneamente.
\paragraph{Superamento dei limiti della VRAM.}

Il secondo vantaggio della UMA riguarda la capacità. Le GPU consumer più potenti (NVIDIA RTX 4090) dispongono di 24 GB di VRAM; le GPU professionali (NVIDIA A100) arrivano a 80 GB. Con 512 GB di memoria unificata, il Mac Studio può caricare modelli che eccederebbero la VRAM di qualsiasi GPU singola.
Questo è particolarmente rilevante per i Large Language Models. DeepSeek-V3, con 671 miliardi di parametri, richiede circa 400 GB di memoria anche in formato quantizzato\footnote{La quantizzazione riduce la precisione dei pesi (ad esempio da 16 bit a 4 bit) per diminuire l'occupazione di memoria. Una trattazione dettagliata è riportata in Appendice~\ref{app:quantization}.}. Sul Mac Studio utilizzato, è possibile eseguire questo modello interamente in memoria, senza ricorrere a tecniche di \textit{offloading} su disco o a cluster multi-GPU—opzioni che introdurrebbero latenza significativa e complessità architetturale.
\paragraph{Metal Performance Shaders (MPS).}
PyTorch supporta l'accelerazione GPU su Apple Silicon attraverso il backend MPS (\texttt{torch.device("mps")}). Questo permette di sfruttare gli 80 core GPU del M3 Ultra per l'addestramento degli Sparse Autoencoder, ottenendo prestazioni significativamente superiori rispetto all'esecuzione su sola CPU.
\paragraph{Requisiti di privacy: elaborazione locale dei dati clinici.}
Oltre ai vantaggi prestazionali, la scelta di un'infrastruttura locale è stata dettata da un requisito non negoziabile: la \textbf{privacy dei dati}. Il corpus utilizzato per gli esperimenti è costituito da cartelle cliniche pediatriche che, sebbene anonimizzate, contengono informazioni sensibili che non possono essere trasmesse a servizi cloud esterni.
L'utilizzo di API cloud per l'interpretazione delle feature—ad esempio, inviando i testi attivanti a GPT-4 o Claude—avrebbe comportato il trasferimento di dati clinici su server di terze parti, una pratica incompatibile con le normative sulla protezione dei dati sanitari e con i principi etici della ricerca medica. La disponibilità di 512 GB di memoria unificata ha permesso di eseguire \textit{localmente} modelli linguistici di grande scala (DeepSeek, Llama, Qwen) attraverso Ollama, garantendo che nessun dato clinico lasciasse mai la macchina.
Questa configurazione—elaborazione interamente on-premise con hardware sufficientemente potente da eseguire LLM di frontiera—rappresenta attualmente una soluzione rara nel panorama del deep learning, dove la maggior parte dei workflow assume l'accesso a risorse cloud. Il Mac Studio con M3 Ultra e 512 GB di memoria è una delle poche piattaforme consumer che rende possibile questo approccio.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estrazione e organizzazione delle feature}
\label{subsec:feature_extraction_results}

\subsubsection{Feature families nel dominio clinico}
\label{subsubsec:clinical_features}

L'applicazione di PRISMA al dataset Pedianet ha prodotto risultati qualitativamente promettenti. Le feature estratte dallo Sparse Autoencoder mostrano una chiara organizzazione semantica, con raggruppamenti che riflettono le categorie concettuali naturali del dominio pediatrico.

L'analisi delle feature families (Sezione~\ref{subsubsec:feature_families}) ha rivelato strutture gerarchiche coerenti con la tassonomia medica. Sono emerse famiglie corrispondenti a:
\begin{itemize}
    \item \textbf{Sintomatologia}: feature parent come ``sintomi respiratori'' con children ``tosse'', ``dispnea'', ``rinorrea''
    \item \textbf{Patologie infettive}: feature parent come ``infezioni'' con children ``otite'', ``faringite'', ``gastroenterite''
    \item \textbf{Parametri clinici}: feature parent come ``esame obiettivo'' con children ``temperatura'', ``frequenza cardiaca'', ``saturazione''
    \item \textbf{Interventi terapeutici}: feature parent come ``terapia farmacologica'' con children ``antibiotici'', ``antipiretici'', ``aerosol''
\end{itemize}

La struttura a stella tipica delle feature families---un parent generale connesso a children specifici che raramente co-occorrono tra loro---riflette la logica del ragionamento clinico: un paziente può presentare ``sintomi respiratori'' (parent attivo) manifestati come tosse \textit{oppure} dispnea (un solo child attivo), ma raramente entrambi con uguale prominenza.

\subsubsection{Feature families nel dominio scientifico}
\label{subsubsec:scientific_features}

Sul dataset arXiv, le feature estratte mostrano un'organizzazione altrettanto coerente, con famiglie che mappano le aree disciplinari e le metodologie della ricerca scientifica. Esempi di famiglie identificate:
\begin{itemize}
    \item \textbf{Machine Learning}: parent ``ottimizzazione'' con children ``gradient descent'', ``Adam'', ``learning rate scheduling''
    \item \textbf{Fisica delle particelle}: parent ``modello standard'' con children ``bosone di Higgs'', ``quark'', ``simmetrie di gauge''
    \item \textbf{Astrofisica}: parent ``oggetti compatti'' con children ``buchi neri'', ``stelle di neutroni'', ``pulsar''
\end{itemize}

Questi risultati sono consistenti con quelli riportati da O'Neill et al.~\parencite{oneill2024disentangling}, confermando che la metodologia degli Sparse Autoencoder produce feature semanticamente interpretabili anche quando applicata con modelli di embedding e infrastrutture diverse.

\begin{notebox}
\textbf{Consistenza cross-dominio}\\
Un risultato notevole è la \textit{consistenza strutturale} delle feature families tra domini completamente diversi (clinico vs.\ scientifico). In entrambi i casi emergono:
\begin{itemize}
    \item Gerarchie a due livelli (parent $\to$ children)
    \item Topologia a stella (children raramente connessi tra loro)
    \item Corrispondenza con tassonomie domain-specific preesistenti
\end{itemize}
Questa consistenza suggerisce che l'organizzazione gerarchica non è un artefatto del dominio, ma una proprietà emergente del modo in cui i modelli linguistici rappresentano i concetti.
\end{notebox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Analisi dell'Effective Rank}
\label{subsec:effective_rank_results}

La sezione~\ref{subsubsec:effective_rank} ha introdotto l'Effective Rank come metrica per quantificare la dimensionalità effettiva dello spazio delle attivazioni. In questa sezione presentiamo i risultati sperimentali ottenuti sul dataset di 50.000 abstract, variando sistematicamente l'expansion factor $\rho$.

\subsubsection{Setup sperimentale}
\label{subsubsec:er_setup}

Per ogni valore di expansion factor $\rho \in \{0.125, 1, 2, 3, 4, 6.5, 13, 26, 52\}$, sono stati addestrati due Sparse Autoencoder:
\begin{enumerate}
    \item \textbf{SAE su dati reali}: addestrato sugli embedding dei 50.000 abstract
    \item \textbf{SAE su dati casuali} (ipotesi nulla): addestrato su vettori generati uniformemente in $\mathbb{R}^d$, privi di struttura semantica
\end{enumerate}

Per ciascun SAE è stata calcolata la matrice di attivazione $H \in \mathbb{R}^{N \times n}$ e il corrispondente Effective Rank secondo l'Equazione~\ref{eq:effective_rank}. Il confronto tra le due condizioni permette di isolare il contributo della struttura semantica alla compressione delle rappresentazioni.

\subsubsection{Il problema del limite degli autovalori}
\label{subsubsec:eigenvalue_limit}

Il calcolo dell'Effective Rank richiede la decomposizione ai valori singolari (SVD) della matrice $H$. Il numero massimo di valori singolari non nulli è limitato da $\min(N, n)$, dove $N$ è il numero di documenti e $n$ la dimensionalità dello spazio latente.

Nel nostro setup con $N = 50.000$ documenti e embedding di dimensione $d = 768$, per expansion factor elevati ($\rho > 13$) la dimensionalità latente $n = \rho \cdot d$ supera $N$. In questo regime, l'Effective Rank non può superare $N$, indipendentemente dalla struttura intrinseca dei dati.

Questo fenomeno è documentato nella letteratura sull'Effective Rank~\parencite{roy2007effective}: quando il numero di campioni è inferiore alla dimensionalità, la matrice è \textit{rank-deficient} e l'entropia dello spettro singolare satura al valore $\log(\min(N,n))$.

Per verificare empiricamente questo effetto, abbiamo condotto due serie di esperimenti:
\begin{enumerate}
    \item \textbf{Limite a 10.000 autovalori}: SVD troncata ai primi 10.000 valori singolari
    \item \textbf{Limite a 40.000 autovalori}: SVD troncata ai primi 40.000 valori singolari
\end{enumerate}

\subsubsection{Risultati: Effective Rank vs.\ Expansion Factor}
\label{subsubsec:er_results}

La Figura~\ref{fig:er_10000} mostra i risultati con limite a 10.000 autovalori. Si osserva:
\begin{itemize}
    \item L'Effective Rank su dati casuali (curva arancione) cresce quasi linearmente con l'expansion factor, come atteso per dati privi di struttura
    \item L'Effective Rank su dati reali (curva blu) cresce più lentamente, evidenziando la compressione dovuta alla struttura semantica
    \item Entrambe le curve raggiungono un \textbf{plateau} intorno a 8.000--10.000, corrispondente al limite artificiale imposto dalla SVD troncata
    \item Il Semantic Compression Ratio (SCR) cresce fino a $\sim$37\%, poi \textbf{decresce} quando entrambe le curve saturano al limite
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{pictures/cap6/erank_ds11_ksing10000_ksparse16_analysis_1.png}
    \caption{Effective Rank in funzione dell'expansion factor, con limite a 10.000 autovalori. \textbf{Sopra}: confronto tra dati reali (blu) e ipotesi nulla (arancione). Il plateau oltre $\rho \approx 26$ è un artefatto del limite computazionale. \textbf{Sotto}: Semantic Compression Ratio. Il calo dopo il picco è dovuto alla saturazione di entrambe le curve al limite degli autovalori, non a una reale diminuzione della compressione semantica.}
    \label{fig:er_10000}
\end{figure}

La Figura~\ref{fig:er_40000} mostra i risultati con limite esteso a 40.000 autovalori. I risultati sono qualitativamente diversi:
\begin{itemize}
    \item Il plateau scompare: l'Effective Rank continua a crescere per entrambe le condizioni
    \item Il gap tra dati reali e casuali \textbf{aumenta} con l'expansion factor
    \item Il Semantic Compression Ratio cresce \textbf{monotonicamente}, raggiungendo circa il 60\% per $\rho = 52$
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{pictures/cap6/erank_ds11_ksing40000_ksparse16_analysis_1.png}
    \caption{Effective Rank in funzione dell'expansion factor, con limite a 40.000 autovalori. \textbf{Sopra}: senza il limite artificiale, l'Effective Rank continua a crescere. Il gap tra le due condizioni aumenta progressivamente. \textbf{Sotto}: il Semantic Compression Ratio cresce monotonicamente fino al 60\%, indicando che la struttura semantica comprime sempre più le rappresentazioni all'aumentare della capacità.}
    \label{fig:er_40000}
\end{figure}

\subsubsection{Interpretazione dei risultati}
\label{subsubsec:er_interpretation}

I risultati dell'analisi dell'Effective Rank supportano due conclusioni principali.

\paragraph{La struttura semantica comprime le rappresentazioni.}
Il gap persistente tra dati reali e casuali---quantificato dal Semantic Compression Ratio---dimostra che i dati linguistici reali occupano una varietà di dimensionalità inferiore rispetto a dati casuali nella stessa configurazione architettonica. Questa compressione non è un artefatto del vincolo Top-K (presente in entrambe le condizioni), ma riflette le correlazioni intrinseche del dominio semantico: concetti correlati (febbre $\leftrightarrow$ infezione, gradiente $\leftrightarrow$ ottimizzazione) tendono a co-attivarsi, riducendo la dimensionalità effettiva.

\paragraph{L'SCR crescente è consistente con il feature splitting.}
L'aumento monotono del Semantic Compression Ratio con l'expansion factor (Figura~\ref{fig:er_40000}) è coerente con il fenomeno del feature splitting descritto nella Sezione~\ref{subsubsec:feature_splitting}. All'aumentare della capacità, le feature generali si scindono in sotto-feature più specifiche. Ma queste sotto-feature, pur essendo distinte, rimangono \textit{correlate}: appartengono alla stessa famiglia semantica, si attivano su sottoinsiemi sovrapposti di documenti, e le loro direzioni decoder formano angoli non ortogonali.

In altre parole: il feature splitting aumenta il numero di feature \textit{nominali} ($n$), ma non aumenta proporzionalmente la dimensionalità \textit{effettiva} (ER). Le nuove feature emergenti non sono indipendenti dalle precedenti---sono loro specializzazioni. Questo spiega perché l'SCR \textit{aumenta} con l'expansion factor: più feature, ma meno indipendenza relativa.

\begin{notebox}
\textbf{Il paradosso della capacità, rivisitato}\\
Nel Capitolo~\ref{sec:04_disentangling_dense_embeddings_with_sparse_autoencoders} abbiamo introdotto il paradosso della capacità: come può BERT rappresentare decine di migliaia di concetti con soli 768 neuroni?

L'analisi dell'Effective Rank suggerisce una risposta parziale: quei concetti non sono \textit{indipendenti}. La struttura semantica del linguaggio---le relazioni tassonomiche, le co-occorrenze, le analogie---introduce correlazioni che permettono di ``comprimere'' molti concetti in poche dimensioni effettive.

Lo Sparse Autoencoder non elimina queste correlazioni: le rende \textit{esplicite}, mappandole in famiglie e gerarchie interpretabili.
\end{notebox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Discussione e limitazioni}
\label{subsec:discussion}

\subsubsection{Punti di forza}
\label{subsubsec:strengths}

I risultati presentati dimostrano che PRISMA è in grado di:
\begin{enumerate}
    \item \textbf{Estrarre feature interpretabili} da embedding densi, in domini diversi (clinico e scientifico)
    \item \textbf{Identificare strutture gerarchiche} (feature families) coerenti con le tassonomie di dominio
    \item \textbf{Quantificare la compressione semantica} attraverso l'Effective Rank, distinguendo il contributo della struttura dei dati da quello dell'architettura
    \item \textbf{Operare su hardware consumer} (Mac con memoria unificata), senza richiedere cluster GPU dedicati
\end{enumerate}

La consistenza dei risultati tra domini diversi e la corrispondenza con i risultati della letteratura~\parencite{oneill2024disentangling} suggeriscono che le metodologie implementate sono robuste e generalizzabili.

\subsubsection{Limitazioni}
\label{subsubsec:limitations}

Riconosciamo diverse limitazioni del presente lavoro.

\paragraph{Validazione quantitativa limitata.}
L'interpretabilità delle feature è stata valutata principalmente in modo qualitativo, attraverso l'ispezione manuale delle etichette prodotte dall'Interpreter LLM. Una validazione rigorosa richiederebbe:
\begin{itemize}
    \item Confronto con annotazioni umane indipendenti
    \item Valutazione su task downstream (classificazione, retrieval)
    \item Metriche formali di monosematicità (come proposto da O'Neill et al.)
\end{itemize}

\paragraph{Dipendenza dal modello di embedding.}
Le feature estratte dipendono criticamente dal modello di embedding utilizzato. Un modello pre-addestrato su dati diversi, o con architettura diversa, potrebbe produrre feature qualitativamente differenti. Non abbiamo condotto studi sistematici di ablazione su questo aspetto.

\paragraph{Scalabilità dell'interpretazione automatica.}
L'interpretazione automatica tramite LLM locale (via Ollama) richiede tempo significativo per corpus di grandi dimensioni. Per il dataset arXiv con milioni di documenti, l'interpretazione completa di tutte le feature richiede diverse ore di elaborazione.

\paragraph{Effective Rank come proxy.}
L'Effective Rank è una metrica globale che non cattura la struttura fine delle correlazioni. Due matrici con lo stesso ER potrebbero avere strutture di correlazione molto diverse. Metriche complementari (come l'analisi della struttura dei cluster nello spazio latente) potrebbero fornire informazioni aggiuntive.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Conclusioni}
\label{subsec:conclusions}

Questa tesi ha presentato PRISMA, un sistema per l'analisi interpretabile degli embedding testuali basato su Sparse Autoencoder. Il lavoro ha affrontato il problema della \textit{superposition}---la codifica di molti concetti in poche dimensioni attraverso direzioni non ortogonali---proponendo una metodologia per ``scomporre'' le rappresentazioni dense in feature sparse e monosemantiche.

I contributi principali sono:
\begin{enumerate}
    \item \textbf{Implementazione completa} di una pipeline per l'addestramento di SAE, l'interpretazione automatica delle feature, e l'analisi delle strutture gerarchiche (feature families)
    
    \item \textbf{Applicazione al dominio clinico}, dimostrando che le metodologie sviluppate per la ricerca sull'interpretabilità dei LLM possono essere trasferite a contesti applicativi dove la trasparenza è un requisito
    
    \item \textbf{Analisi dell'Effective Rank} come metrica per quantificare la compressione semantica, con risultati che supportano l'ipotesi che il feature splitting aumenti il numero di feature senza aumentare proporzionalmente la dimensionalità effettiva
    
    \item \textbf{Validazione cross-dominio}, mostrando consistenza strutturale tra feature families estratte da corpora clinici e scientifici
\end{enumerate}

Il nome PRISMA---scelto per l'analogia con il prisma ottico che scompone la luce bianca nelle sue componenti spettrali---si è rivelato appropriato: lo Sparse Autoencoder non aggiunge informazione agli embedding, ma la rende \textit{visibile}, separando ciò che era mescolato in una rappresentazione opaca.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sviluppi futuri}
\label{subsec:future_work}

I risultati ottenuti aprono diverse direzioni di ricerca. Presentiamo qui una prospettiva teorica che guiderà i prossimi sviluppi.

\subsubsection{Verso una teoria degli invarianti semantici}
\label{subsubsec:semantic_invariants}

Un'osservazione ricorrente in questo lavoro è la \textit{consistenza} delle strutture estratte: le feature families hanno topologie simili in domini diversi, le gerarchie riflettono tassonomie preesistenti, l'Effective Rank mostra andamenti regolari. Questa consistenza suggerisce l'esistenza di strutture che si preservano attraverso rappresentazioni diverse.

Proponiamo di formalizzare questa intuizione prendendo ispirazione dalla fisica teorica. Prima di Einstein, i fisici cercavano un sistema di riferimento privilegiato---l'\textit{etere}---rispetto al quale misurare il moto assoluto. La relatività ha capovolto la prospettiva: non esiste un riferimento privilegiato, ma esistono \textbf{invarianti}---quantità che restano identiche in tutti i sistemi di riferimento. L'intervallo spaziotemporale $ds^2 = c^2 dt^2 - dx^2 - dy^2 - dz^2$ è reale proprio perché è invariante.

Analogamente, nel dominio semantico potremmo smettere di cercare la rappresentazione ``vera'' (densa? sparsa? umana?) e concentrarci sugli \textbf{invarianti semantici}---strutture che si preservano attraverso tutte le rappresentazioni del significato.

\begin{table}[htbp]
    \centering
    \caption{Parallelo tra fisica relativistica e semantica.}
    \label{tab:relativity_analogy}
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{Relatività} & \textbf{Semantica} \\
        \midrule
        Non esiste sistema di riferimento privilegiato & Non esiste rappresentazione semantica privilegiata \\
        Tutti i sistemi inerziali sono equivalenti & Denso, sparso, umano sono equivalenti \\
        Cercare l'etere era un errore & Cercare la rappresentazione ``vera'' è un errore \\
        Le leggi fisiche sono invarianti & Le leggi semantiche sono invarianti \\
        L'intervallo $ds^2$ è reale & Gli invarianti semantici sono reali \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Candidati invarianti}
\label{subsubsec:candidate_invariants}

Se gli invarianti semantici esistono, quali potrebbero essere? Proponiamo alcuni candidati da investigare:

\begin{enumerate}
    \item \textbf{Relazioni di analogia.} Se la relazione ``re : uomo = regina : donna'' vale nella rappresentazione densa, in quella sparsa, \textit{e} nei giudizi umani, allora è un invariante.
    
    \item \textbf{Strutture gerarchiche.} Se ``cane è un mammifero'' si preserva in tutte le rappresentazioni (come inclusione, vicinanza, o implicazione), è un invariante.
    
    \item \textbf{Ordini di similarità.} Se $A$ è più simile a $B$ che a $C$ in tutte le rappresentazioni:
    \begin{equation}
        d(A,B) < d(A,C) \quad \forall \text{ rappresentazioni}
    \end{equation}
    
    \item \textbf{Struttura delle feature families.} Se la topologia a stella (parent $\to$ children) emerge indipendentemente dal dominio, dal modello di embedding, e dalla capacità del SAE, potrebbe essere un invariante strutturale.
\end{enumerate}

\subsubsection{Programma di ricerca}
\label{subsubsec:research_program}

Il programma di ricerca che proponiamo si articola in quattro fasi:

\begin{enumerate}
    \item \textbf{Identificazione empirica}: estendere l'analisi a più domini, più modelli di embedding, più lingue, cercando strutture che si preservano
    
    \item \textbf{Formalizzazione matematica}: sviluppare un framework formale per descrivere gli invarianti (teoria delle categorie? topologia algebrica? teoria dei gruppi?)
    
    \item \textbf{Validazione con dati umani}: confrontare le strutture estratte dai modelli con giudizi umani (similarità, categorizzazione, inferenza)
    
    \item \textbf{Predizioni falsificabili}: derivare predizioni non ovvie dagli invarianti ipotizzati e testarle empiricamente
\end{enumerate}

Se gli invarianti semantici esistono e sono formalizzabili, avremmo trovato qualcosa di analogo alle leggi della fisica per il dominio del significato. La domanda non sarebbe più ``cos'è il significato?'' ma ``cosa resta invariante quando cambiamo modo di rappresentare il significato?''

\begin{notebox}
\textbf{La tesi centrale degli sviluppi futuri}\\
Gli invarianti semantici---strutture che si preservano attraverso tutte le rappresentazioni del significato---sono ontologicamente fondamentali. Le rappresentazioni particolari (densa, sparsa, umana, linguistica) sono proiezioni di questi invarianti, così come le coordinate $(x, y, z, t)$ sono proiezioni dell'intervallo spaziotemporale invariante.

Se questa tesi è corretta, lo Sparse Autoencoder non è solo uno strumento di interpretabilità: è un \textit{microscopio} per osservare gli invarianti semantici---la struttura profonda del significato.
\end{notebox}