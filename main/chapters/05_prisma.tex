\section{Prisma}

\section{Introduzione}
Prisma è un’applicazione nata con l’obiettivo di rendere interpretabili gli \textit{embeddings} testuali, generalizzando questo processo a qualsiasi insieme di documenti. Gli \textit{embeddings}, pur essendo rappresentazioni ricche dal punto di vista semantico, tendono a condensare molteplici concetti in uno spazio denso e difficilmente esplorabile, rendendo complessa l’analisi del loro contenuto.

L’idea alla base di Prisma richiama il comportamento di un prisma ottico: così come un fascio di luce bianca viene scomposto nelle sue componenti cromatiche fondamentali, allo stesso modo un \textit{embedding} semanticamente denso può essere ``rifratto'' nei concetti che lo compongono. Il software si propone di decomporre queste rappresentazioni compatte in dimensioni concettuali interpretabili, fornendo una visione strutturata del contenuto semantico. Un obiettivo centrale è l’accessibilità: Prisma mira ad abbassare la barriera d’ingresso per utenti non specialisti, fornendo un’interfaccia intuitiva per esplorare rappresentazioni semantiche complesse.

\section{Architettura}
L'applicazione è sviluppata in Python utilizzando il framework Django come nucleo centrale per la gestione della logica di business e dell'interfaccia web. Il sistema si appoggia a un database SQLite locale per la persistenza dei dati e interagisce con il framework Ollama per l'esecuzione dei modelli di linguaggio (LLM) necessari all'interpretazione.

\begin{figure}[!ht]
\centering
\begin{tikzpicture}[
    font=\sffamily\small,
    node distance=1.2cm and 1.5cm,
    % Stili semplificati
    core/.style={rectangle, draw, fill=blue!5, thick, rounded corners, minimum width=10cm, minimum height=6cm},
    module/.style={rectangle, draw, fill=white, thick, rounded corners, text width=2.5cm, align=center, minimum height=3cm},
    db/.style={cylinder, draw, shape border rotate=90, fill=orange!20, text width=1.5cm, align=center, minimum height=1.5cm},
    ext/.style={rectangle, draw, fill=red!10, thick, rounded corners, text width=2.5cm, align=center, minimum height=1cm},
    line/.style={draw, -{Stealth[scale=1.2]}, thick},
    both/.style={draw, {Stealth[scale=1.2]}-{Stealth[scale=1.2]}, thick}
]

    % Django Core Container
    \node [core] (django) {};
    \node[anchor=north, yshift=-0.2cm] at (django.north) {\textbf{Django Core}};

    % Moduli interni (senza sottotesto)
    \node [module, xshift=-3.2cm, yshift=-0.3cm] (emb) at (django.center) {\textbf{Generazione Embedding}};
    \node [module, yshift=-0.3cm] (sae) at (django.center) {\textbf{Training SAE}};
    \node [module, xshift=3.2cm, yshift=-0.3cm] (interp) at (django.center) {\textbf{Interpretazione}};

    % Elementi Esterni
    \node [db, right=of django, xshift=0.3cm] (sqlite) {Database\\(SQLite)};
    \node [ext, below=of interp, yshift=-0.5cm] (ollama) {Ollama API};

    % Connessioni Flusso
    \path [line] (emb) -- (sae);
    \path [line] (sae) -- (interp);
    
    % Connessioni Infrastruttura
    \path [both] (django.east) -- (sqlite);
    \path [both] (interp.south) -- (ollama);

\end{tikzpicture}
\caption{Schema semplificato dell'architettura di Prisma.}
\label{fig:architettura_semplice}
\end{figure}

\section{Generazione Embeddings}
Il flusso operativo inizia con la creazione di un \texttt{Dataset} e la successiva trasformazione dei documenti in vettori numerici. L'applicazione permette di selezionare diversi modelli di codifica, tra cui modelli specializzati per il dominio clinico (come \textit{medbit}) o modelli multilingua performanti (come la famiglia \textit{GTE}).

\subsection{Gestione di documenti lunghi: strategia \textit{chunk-and-average}}

Un aspetto strutturale del sistema Prisma riguarda la gestione di documenti testuali la cui lunghezza eccede il limite massimo di input dei modelli Transformer utilizzati per la generazione di embeddings.
In particolare, molti modelli della famiglia BERT-like accettano in input una sequenza di lunghezza massima pari a
\[
L_{\max} = 512 \ \text{token}.
\]
Di conseguenza, un documento con lunghezza superiore a $L_{\max}$ non può essere processato integralmente in un singolo forward pass. 
Una soluzione na{\"i}ve consiste nella troncatura (\textit{truncation}) del testo, che mantiene solo i primi $L_{\max}$ token;
tuttavia, questa scelta può eliminare porzioni informative rilevanti (ad esempio conclusioni, diagnosi o follow-up riportati in coda al documento). Per preservare l'intero contenuto informativo, Prisma implementa una strategia di segmentazione denominata \textit{chunk-and-average},
che consente di codificare testi arbitrariamente lunghi producendo un singolo embedding globale per documento.

\paragraph{Tokenizzazione e vincoli di lunghezza}
Dato un documento testuale $D$, esso viene prima trasformato in una sequenza di token tramite un tokenizer associato al modello scelto. Indichiamo tale sequenza come
\[
T = (t_1, t_2, \dots, t_N),
\]
dove $N$ è il numero totale di token ottenuti dalla tokenizzazione e i token $t_i$ possono rappresentare parole intere o sotto-parole (subword), a seconda del vocabolario del modello.
I modelli Transformer richiedono tipicamente l'aggiunta di token speciali che delimitano la sequenza e ne abilitano alcune funzionalità interne. Ad esempio, nei modelli tipo BERT è comune inserire un token di classificazione \texttt{[CLS]} all'inizio e un token di separazione \texttt{[SEP]} alla fine. Indichiamo con $N_{\text{special}}$ il numero di token speciali necessari. La lunghezza massima effettivamente disponibile per il contenuto testuale (cioè per i token provenienti dal documento) è quindi
\[
W_{\text{eff}} = L_{\max} - N_{\text{special}}.
\]

\paragraph{Segmentazione in chunk contigui}
Se $N \le W_{\text{eff}}$, il documento può essere processato direttamente. Se invece $N > W_{\text{eff}}$, la sequenza $T$ viene suddivisa in $K$ segmenti contigui (\textit{chunks}) ciascuno di lunghezza al più $W_{\text{eff}}$:
\[
C_k = (t_{(k-1)W_{\text{eff}} + 1}, \dots, t_{\min(kW_{\text{eff}},\, N)}),
\qquad k = 1,\dots,K,
\]
dove
\[
K = \left\lceil \frac{N}{W_{\text{eff}}} \right\rceil.
\]
Ciascun chunk viene poi confezionato come input valido per il modello aggiungendo i token speciali richiesti (ad esempio \texttt{[CLS]} e \texttt{[SEP]}), in modo che la lunghezza totale dell'input sia $\le L_{\max}$.

\paragraph{Embedding per chunk e concetto di pooling}
Sia $f_{\theta}$ il modello Transformer selezionato (con parametri $\theta$). Dato un chunk $C_k$ composto da $m_k$ token (dopo l'aggiunta dei token speciali), il forward pass del modello produce una sequenza di vettori nascosti (hidden states), uno per ciascun token:
\[
f_{\theta}(C_k) = (\mathbf{h}^{(k)}_1, \mathbf{h}^{(k)}_2, \dots, \mathbf{h}^{(k)}_{m_k}),
\qquad \mathbf{h}^{(k)}_j \in \mathbb{R}^{d},
\]
dove $d$ è la dimensionalità interna della rappresentazione (hidden size) del modello. Per ottenere un singolo vettore che rappresenti l'intero chunk, Prisma applica un'operazione di \textit{pooling}. Con il termine pooling si intende una funzione di aggregazione che combina la sequenza di vettori token-level in un unico vettore a livello di segmento (segment-level). Nel caso più comune utilizzato in Prisma, si adotta il \textit{mean pooling}, ovvero la media aritmetica dei vettori dei token (tipicamente escludendo eventuali token speciali, a seconda dell'implementazione):
\[
\mathbf{e}_k = \text{MeanPool}\bigl(f_{\theta}(C_k)\bigr)
= \frac{1}{m_k} \sum_{j=1}^{m_k} \mathbf{h}^{(k)}_j,
\qquad \mathbf{e}_k \in \mathbb{R}^{d}.
\]
Intuitivamente, il mean pooling produce un vettore che riassume il contenuto semantico del chunk distribuendo il contributo su tutti i token che lo compongono, senza privilegiare una posizione specifica della sequenza.

\paragraph{Aggregazione a livello documento}
Una volta ottenuti gli embedding dei singoli chunk $\mathbf{e}_1,\dots,\mathbf{e}_K$, Prisma costruisce un embedding globale per il documento $D$ combinandoli tramite media aritmetica:
\[
\mathbf{v}_D = \frac{1}{K}\sum_{k=1}^{K} \mathbf{e}_k,
\qquad \mathbf{v}_D \in \mathbb{R}^{d}.
\]
Questa scelta consente di ottenere una rappresentazione unica e confrontabile tra documenti, anche quando le loro lunghezze originali sono molto diverse. Inoltre, la procedura garantisce che tutte le parti del testo contribuiscano alla rappresentazione finale, evitando la perdita sistematica di informazione tipica della troncatura.

\section{Training SAE}
Una volta generati gli \textit{embeddings}, il sistema procede alla loro scomposizione tramite il modulo \textit{Sparse Autoencoders} (SAE). In questa fase, l'utente configura un'istanza di \texttt{SAERun} definendo parametri quali l'\textit{expansion factor} (per determinare la dimensione del livello latente) e la \textit{k-sparsity} (per regolare il numero di neuroni attivi contemporaneamente).

Il training produce un modello capace di ricostruire l'input densificato attraverso una rappresentazione sparsa. Al termine, Prisma genera automaticamente analisi visive, tra cui mappe di calore della similarità delle matrici e della co-occorrenza, che permettono di valutare statisticamente la qualità delle feature estratte prima dell'interpretazione semantica.

\section{Interpretazione}
L'ultima fase del processo trasforma le feature matematiche in concetti comprensibili. Il modulo \textit{Explorer} utilizza le attivazioni più significative di ogni feature per estrarre ``evidenze'' testuali dai documenti originali.

Queste evidenze vengono inviate all'API locale di Ollama, che genera un'etichetta (\textit{label}) e una descrizione testuale per ogni feature. Prisma supporta la gestione di interpretazioni multiple per la stessa feature e permette di organizzare i concetti in \texttt{FeatureFamily}, ricostruendo una gerarchia semantica che va dai concetti generali a quelli più specifici.



\section{Esperimenti}
In questo lavoro di tesi sono stati condotti tre esperimenti principali. 
Il primo è stato chiamato Pedianet e riguarda dati medici che verranno in seguito presentati e descritti, il secondo basato su un dataset di abstracts di paper scientifici generale, il terzo invece riguarda un dataset di papers scientifici di PubMed. 
Verranno di seguito descritti e verranno presentati i risultati ottenuti. 

\section{Pedianet}


Il progetto Pedianet è una rete italiana di pediatria di famiglia fondata nel 1998, con l’obiettivo di raccogliere, strutturare e analizzare i dati clinici pediatrici derivanti dalla normale attività ambulatoriale dei pediatri di libera scelta (PLS). 
L’iniziativa nasce per colmare una lacuna storica nella ricerca clinica pediatrica, ovvero la scarsa disponibilità di dati real-world relativi alla popolazione infantile. 
A differenza di altri database europei che includono anche adulti, Pedianet è uno dei pochi progetti esclusivamente focalizzati sull’età pediatrica, risultando così un archivio unico nel panorama della ricerca biomedica europea. 
La rete è composta da oltre 400 pediatri distribuiti su tutto il territorio italiano, che utilizzano la stessa piattaforma elettronica (Junior Bit) per la gestione delle cartelle cliniche dei pazienti. 
Circa 3700 pediatri utilizzano questo software, e sono potenziali candidati ad aderire alla rete. Tutti i dati raccolti vengono trasmessi tramite connessione protetta a un server centrale situato a Padova,
dove vengono validati, standardizzati e archiviati. I dati raccolti sono nella forma di cartelle cliniche pediatriche, ma presentano il problema di non essere strutturati, ovvero non organizzati e non seprati in classi.
Con la nascita dei large language models questi dati hanno assunto un valore completamente nuovo perché se correttammente lavorati e organizzati, dal momento che vanno a ricoprire un dominio molto specifico e verticale, si prestano benissimo a funzionare come dati di addestramento nel campo di modelli di NLP.
Un'altra motivazione per cui è necessario intervenire con strumenti di NLP è quello di svolgere task di classificaione per destinare questi dati al mondo della ricerca.
Per esempio si vorrebbe raccogliere tutte quelle cartelle che hanno visto casi di \textit{bronchiolite}, ma non bastano metodi di regulary expression dal momento che ci sono cartelle che invece presentano casi di \textit{sospetta} bronchiolite. Si proverà quindi ad intervenire con la metodologia presentata nei precedenti capitoli per processare questi dati.
Ecco il testo della sottosezione in formato paragrafo unico:

\subsection{Scelta del modello di embedding} 
Per la generazione delle rappresentazioni vettoriali è stato selezionato il modello MedBIT \cite{BUONOCORE2023104431}, progettato per colmare il divario tecnologico nelle lingue meno fornite di risorse biomediche come l’italiano. Nel lavoro di riferimento, gli autori mettono a confronto due approcci distinti: BioBIT, basato sulla traduzione automatica neurale di milioni di abstract di PubMed per privilegiare la quantità dei dati , e MedBIT, che raffina tale conoscenza attraverso l’uso di un corpus di alta qualità composto da libri di testo medici scritti nativamente in italiano. La scelta di MedBIT per il progetto Pedianet risulta ottimale innanzitutto per la sua natività linguistica; poiché i dati di Pedianet sono redatti interamente in italiano dai medici di base , il modello garantisce una rappresentazione più naturale e sintatticamente corretta rispetto ai sistemi basati su traduzioni automatiche, riducendo le ambiguità semantiche. Queste caratteristiche rendono MedBIT lo strumento più affidabile per catturare le delicate sfumature cliniche presenti nei diari dei pediatri italiani, permettendo di distinguere efficacemente tra diagnosi accertate e semplici sospetti.


\subsection{Esperimento} 
Per la conduzione dell'esperimento sono state processate circa 200.000 cartelle cliniche provenienti dal database Pedianet. Durante la fase preliminare di analisi, è emersa una criticità significativa legata alla lunghezza dei testi: una porzione rilevante delle note cliniche superava il limite di lunghezza massima ($L_{max}$) imposto dall'architettura del modello MedBIT (tipicamente 512 tokens). La troncatura del testo (truncation) avrebbe comportato la perdita di informazioni potenzialmente cruciali situate nella parte finale delle note cliniche, compromettendo la qualità della rappresentazione semantica. Per ovviare a questo problema senza perdere contenuto informativo, è stata adottata una strategia di \textit{chunk-and-average}.
Formalmente, dato un documento $D$ costituito da una sequenza di token $T = {t_1, t_2, \dots, t_N}$, dove $N > L_{max}$, il testo viene suddiviso in $K$ segmenti (chunks) contigui $C_1, C_2, \dots, C_K$. La lunghezza effettiva utilizzabile per ogni chunk è definita come $W_{eff} = L_{max} - N_{special}$, dove $N_{special}$ rappresenta il numero di token speciali richiesti dal modello (es. \texttt{[CLS]} e \texttt{[SEP]}).
Il documento viene quindi partizionato in modo tale che ogni chunk $C_k$ contenga una sottosequenza di token di lunghezza al più $W_{eff}$: $$ C_k = {t_{i}, \dots, t_{i + W_{eff} - 1}} $$
Per ogni segmento $k$, viene calcolato un embedding denso $\mathbf{e}k \in \mathbb{R}^d$ attraverso il modello $f\theta$: $$ \mathbf{e}k = \text{Pooling}(f\theta(C_k)) $$ dove la funzione di pooling (nel nostro caso, \textit{mean pooling}) aggrega gli stati nascosti dell'ultimo layer del modello per ottenere una singola rappresentazione vettoriale per il segmento.
Infine, la rappresentazione vettoriale globale dell'intero documento clinico $\mathbf{v}_D$ è ottenuta calcolando la media aritmetica degli embedding dei singoli segmenti. Questo approccio permette di condensare l'informazione distribuita lungo tutto il testo in un unico punto nello spazio latente: $$ \mathbf{v}D = \frac{1}{K} \sum{k=1}^{K} \mathbf{e}_k $$
Questa metodologia garantisce che ogni parte della cartella clinica contribuisca equamente alla rappresentazione finale, preservando dettagli sintomatologici o diagnostici che potrebbero trovarsi in qualsiasi punto della narrazione medica. A valle di questo processo di codifica, i vettori risultanti sono stati utilizzati per addestrare gli autoencoder sparsi (SAE) oggetto dello studio.

\section{Abstracts}
