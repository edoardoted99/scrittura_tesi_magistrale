\section{Introduzione}
\label{sec:01_introduzione}

\epigraph{
    The question of whether a computer can think is no more interesting than the question of whether a submarine can swim.
}{Edsger W. Dijkstra}

\newpage

\subsection{Il problema: rappresentazioni potenti ma opache}
\label{subsec:01_problema}

Una delle capacità più straordinarie delle reti neurali è quella di costruirsi autonomamente una rappresentazione del mondo. Senza che nessuno le dica come organizzare l'informazione, una rete impara a codificare i dati in uno \textbf{spazio latente}---uno spazio geometrico interno in cui ogni punto corrisponde a una configurazione del mondo esterno. Questa capacità non è programmata: emerge spontaneamente dall'addestramento, come conseguenza del tentativo della rete di risolvere un compito.

Quando questo principio viene applicato al linguaggio, il risultato è notevole. I modelli linguistici moderni---da Word2Vec~\parencite{mikolov2013efficientestimationwordrepresentations} a BERT~\parencite{devlin2019bert}, da GPT a LLaMA---imparano a rappresentare parole, frasi e interi documenti come \textbf{vettori} in spazi ad alta dimensionalità, gli \textit{embedding}. In questi spazi, le relazioni semantiche diventano relazioni geometriche: parole simili occupano regioni vicine, analogie si traducono in parallelismi vettoriali, e il significato di una frase emerge dalla posizione del suo vettore rispetto a tutti gli altri. L'idea che il significato possa essere catturato dalla geometria---che ``re'' stia a ``uomo'' come ``regina'' sta a ``donna'' non in senso metaforico, ma come operazione aritmetica su vettori---è una delle intuizioni più profonde dell'elaborazione del linguaggio naturale.

Ma questa straordinaria capacità rappresentativa ha un costo: la perdita di \textbf{interpretabilità}. Gli spazi latenti dei modelli linguistici sono densi e opachi. Ogni neurone partecipa alla codifica di molti concetti diversi, e ogni concetto è distribuito su molti neuroni. Non esiste, in generale, un neurone che si attivi esclusivamente in presenza di ``colore rosso'' o ``muso di cane'': il significato è codificato in pattern di attivazione distribuiti, illeggibili per un osservatore umano.

Questa opacità non è solo un inconveniente tecnico---è un problema di \textbf{fiducia}. Man mano che i modelli linguistici vengono impiegati in contesti ad alta posta---dalla diagnosi medica alla consulenza legale, dalla moderazione dei contenuti alle decisioni finanziarie---diventa sempre più urgente comprendere \textit{come} un modello arriva alle sue risposte. Un sistema che produce output corretti ma incomprensibili è un sistema di cui non ci si può fidare completamente. Il problema dell'interpretabilità non è accessorio rispetto alla capacità del modello: ne è il complemento necessario.

Una risposta teorica a questa opacità è la \textbf{Superposition Hypothesis}~\parencite{elhage2022toy}: le reti neurali sfruttano il fatto che molti concetti sono mutuamente esclusivi---``meccanica quantistica'' raramente co-occorre con ``torta al cioccolato''---per rappresentare un numero di concetti superiore al numero di neuroni disponibili. È una strategia efficiente di compressione, ma genera \textit{polisemanticità}: ogni neurone codifica più concetti sovrapposti, e la rete diventa una black box.

La domanda che motiva questo lavoro è dunque la seguente: se la superposition comprime molti concetti in pochi neuroni, è possibile costruire uno strumento che \textbf{decomprime} le rappresentazioni, proiettandole in uno spazio dove ogni direzione corrisponde a un concetto interpretabile? E se sì, possiamo non solo leggere cosa il modello ``sa'', ma anche osservare come organizza la conoscenza---quali concetti associa, quali gerarchie costruisce, come struttura il significato?

A tale scopo è stato sviluppato \textbf{PRISMA} (\textit{Projection of Representations for Interpretability via Sparse Monosemantic Autoencoders}): un metodo e un applicativo che, analogamente a un prisma ottico che scompone la luce bianca nelle sue componenti cromatiche, scompone le rappresentazioni dense dei modelli linguistici nei concetti atomici che le costituiscono. Lo strumento centrale di PRISMA è lo \textbf{Sparse Autoencoder} (SAE): una rete neurale che proietta gli embedding densi in uno spazio latente ad alta dimensionalità, imponendo un vincolo di sparsità che forza ogni input ad attivare solo poche direzioni---ciascuna corrispondente a un concetto identificabile.

Prima di descrivere PRISMA nel dettaglio, è necessario collocare questo lavoro nel contesto della ricerca recente sull'interpretabilità meccanicistica e sugli Sparse Autoencoder, per chiarire quali problemi aperti affronta e in che modo si distingue dagli approcci esistenti.

\subsection{Stato dell'arte e posizionamento del lavoro}
\label{subsec:01_stato_dell_arte}

Il problema dell'interpretabilità delle reti neurali ha generato nell'ultimo decennio un campo di ricerca vasto e in rapida espansione. Questa sezione ne ripercorre i filoni principali, concentrandosi sugli sviluppi più direttamente rilevanti per il presente lavoro: l'interpretabilità meccanicistica, la Superposition Hypothesis, gli Sparse Autoencoder come strumento di analisi, e la questione della convergenza delle rappresentazioni.

\subsubsection{Interpretabilità meccanicistica: dai circuiti alle feature}
\label{subsubsec:01_mech_interp}

L'interpretabilità meccanicistica (\textit{mechanistic interpretability}) è il programma di ricerca che si propone di comprendere le reti neurali non trattandole come black box, ma \textit{reverse-engineerandone} i meccanismi interni---analogamente a come un ingegnere potrebbe studiare un circuito elettronico sconosciuto analizzandone i componenti e le connessioni. Le radici di questo approccio risalgono al lavoro sulla visualizzazione delle feature nelle reti convoluzionali: Olah et al.~\parencite{olah2017feature} hanno mostrato che i neuroni delle CNN addestrate su immagini sviluppano rappresentazioni visivamente interpretabili---rilevatori di bordi, texture, pattern ricorrenti---organizzate in una gerarchia crescente di complessità. Questo risultato ha suggerito che le reti non memorizzano i dati in modo opaco, ma costruiscono rappresentazioni strutturate e, almeno in parte, leggibili.

Il passo successivo è stato il \textit{framework dei circuiti}~\parencite{olah2020zoom}: anziché studiare neuroni isolati, si analizzano sottografi della rete---gruppi di neuroni connessi che implementano una funzione computazionale identificabile. In questo quadro, una rete neurale è vista come una composizione di circuiti specializzati, ciascuno responsabile di un aspetto specifico del compito. Questo approccio, inizialmente sviluppato per le CNN, è stato successivamente esteso ai Transformer~\parencite{elhage2021mathematical}, dove l'architettura basata su attention head e strati residuali offre una struttura naturale per l'analisi modulare.

Due risultati hanno reso particolarmente concreto il programma dell'interpretabilità meccanicistica nei modelli linguistici. Il primo è la scoperta degli \textit{induction head}~\parencite{olsson2022context}: coppie di attention head che implementano un algoritmo di copia contestuale, responsabile di una parte significativa dell'apprendimento in-context dei Transformer. Il secondo è il fenomeno della \textit{grokking}~\parencite{nanda2023progress}, in cui reti addestrate su aritmetica modulare passano dalla memorizzazione alla generalizzazione, sviluppando internamente algoritmi basati su rappresentazioni di Fourier---un caso in cui il reverse-engineering ha rivelato non solo \textit{cosa} la rete sa, ma \textit{come} lo computa.

Tuttavia, l'analisi per circuiti incontra un limite fondamentale quando si cerca di scalare a modelli di grandi dimensioni: la \textbf{polisemanticità}. I neuroni individuali dei LLM non corrispondono, in generale, a concetti singoli, rendendo l'identificazione dei circuiti ambigua. Questo problema ha motivato la ricerca di metodi capaci di estrarre le unità semantiche fondamentali---le \textit{feature}---come passo preliminare all'analisi dei circuiti.

\subsubsection{La Superposition Hypothesis e il suo fondamento teorico}
\label{subsubsec:01_superposition}

La polisemanticità non è un difetto accidentale delle reti neurali, ma una conseguenza prevedibile della loro architettura. La \textbf{Superposition Hypothesis}, formalizzata da Elhage et al.~\parencite{elhage2022toy}, propone che le reti rappresentino più feature di quante siano le dimensioni disponibili, sfruttando la sparsità statistica dei concetti: poiché la maggior parte delle feature è inattiva per un dato input, la rete può assegnare a ciascuna una direzione quasi-ortogonale nello spazio delle attivazioni, tollerando una piccola interferenza tra feature raramente co-attive.

Il lavoro di Elhage et al. ha formalizzato questa intuizione attraverso modelli giocattolo (\textit{toy models}), dimostrando che la superposition emerge spontaneamente quando il numero di feature rilevanti supera la dimensione dello spazio latente e le feature sono sufficientemente sparse. I toy model mostrano inoltre che la geometria della superposition è governata da un trade-off tra \textit{fedeltà della rappresentazione} e \textit{interferenza}: la rete impara a privilegiare le feature più importanti e più sparse, allocando loro direzioni più prossime all'ortogonalità.

Questo risultato ha implicazioni profonde per l'interpretabilità: se le unità semantiche fondamentali non sono i neuroni ma le \textit{direzioni} nello spazio delle attivazioni, allora l'analisi neurone per neurone è insufficiente per principio. Servono strumenti capaci di identificare queste direzioni privilegiate---ed è precisamente il ruolo che gli Sparse Autoencoder sono stati chiamati a svolgere.

\subsubsection{Sparse Autoencoder per l'interpretabilità dei LLM}
\label{subsubsec:01_sae_interp}

L'applicazione degli Sparse Autoencoder all'interpretabilità dei modelli linguistici rappresenta uno dei filoni più attivi della ricerca recente. L'idea fondamentale è utilizzare un SAE per proiettare le attivazioni dense di un LLM in uno spazio sovradimensionato (\textit{overcomplete}) e sparso, dove le direzioni apprese corrispondano alle feature monosemantiche che la superposition ha mescolato.

Il lavoro fondativo in questa direzione è \textit{Towards Monosemanticity} di Bricken et al.~\parencite{bricken2023monosemanticity}, in cui gli autori hanno addestrato SAE sulle attivazioni di un Transformer a un solo strato, estraendo feature che rispondono a concetti interpretabili e specifici---dal codice Python alle citazioni bibliche, da specifici pattern sintattici a concetti semantici ben definiti. Questo lavoro ha dimostrato per la prima volta che la decomposizione sparsa delle attivazioni dei LLM produce unità semantiche coerenti e monosemantiche, confermando empiricamente le predizioni della Superposition Hypothesis.

In parallelo, Cunningham et al.~\parencite{cunningham2023sparse} hanno proposto un approccio analogo, dimostrando che gli SAE possono recuperare feature interpretabili anche quando applicati a modelli più grandi, e introducendo metriche per valutare la qualità delle feature estratte. I risultati dei due gruppi, ottenuti indipendentemente, hanno consolidato gli SAE come strumento principale per la decomposizione delle rappresentazioni nei LLM.

Il successivo \textit{Scaling Monosemanticity}~\parencite{templeton2024scaling} ha esteso l'approccio a Claude 3 Sonnet, un LLM di scala industriale, mostrando che gli SAE scalano ai modelli di frontiera: le feature estratte includono concetti astratti di alto livello---come la fiducia, il pericolo, o specifici bias---e possono essere utilizzate per il \textit{steering} del comportamento del modello, attivando o disattivando selettivamente singole feature.

Sul fronte architetturale, la ricerca ha esplorato diverse varianti del vincolo di sparsità. Il lavoro originale di Bricken et al. utilizza una penalità L1 nella funzione di perdita, che tuttavia soffre del problema del \textit{shrinkage}: la penalità comprime sistematicamente verso lo zero le attivazioni delle feature, introducendo un bias nella ricostruzione. Per superare questo limite, Rajamanoharan et al.~\parencite{rajamanoharan2024improving} hanno proposto i \textbf{Gated SAE}, che separano la decisione di attivazione (gate binario) dall'ampiezza dell'attivazione, eliminando il conflitto tra sparsità e fedeltà della ricostruzione. Gli stessi autori hanno successivamente introdotto i \textbf{JumpReLU SAE}~\parencite{rajamanoharan2024jumping}, che utilizzano una funzione di attivazione con soglia appresa per ottenere lo stesso beneficio con un'architettura più semplice. In parallelo, Gao et al.~\parencite{gao2024scaling} hanno proposto il vincolo \textbf{Top-K}, che garantisce un livello di sparsità esatto fissando il numero di feature attive per input---l'approccio adottato anche nel presente lavoro. Questa varietà di architetture riflette un problema aperto: non esiste ancora consenso su quale vincolo di sparsità sia ottimale, e la scelta dipende dal trade-off specifico tra fedeltà della ricostruzione, interpretabilità delle feature, e costo computazionale.

Un aspetto critico emerso dalla letteratura è il problema dei \textbf{dead latents}---feature che cessano di attivarsi durante l'addestramento, riducendo la capacità effettiva del SAE. Bricken et al.~\parencite{bricken2023monosemanticity} hanno proposto il \textit{resampling} periodico dei neuroni morti, mentre Gao et al.~\parencite{gao2024scaling} hanno introdotto una \textit{auxiliary loss} che incentiva l'uso delle feature meno attive. PRISMA adotta quest'ultimo approccio, come descritto nel Capitolo~\ref{sec:05_prisma}.

\subsubsection{Interpretazione automatica delle feature}
\label{subsubsec:01_auto_interp}

L'estrazione di migliaia di feature monosemantiche pone un problema pratico: come assegnare a ciascuna un'etichetta semantica? L'ispezione manuale non scala, e l'interpretabilità di un SAE con decine di migliaia di feature richiede metodi automatici.

Il lavoro pionieristico in questa direzione è quello di Bills et al.~\parencite{bills2023language}, in cui GPT-4 viene utilizzato per generare descrizioni in linguaggio naturale dei neuroni di GPT-2, sulla base dei pattern di attivazione osservati. L'approccio---far spiegare un modello linguistico \textit{da} un altro modello linguistico---si è rivelato sorprendentemente efficace, producendo etichette che correlano significativamente con il comportamento reale dei neuroni.

Questo paradigma è stato rapidamente adottato dalla comunità: Bricken et al.~\parencite{bricken2023monosemanticity} e Templeton et al.~\parencite{templeton2024scaling} utilizzano Claude per etichettare le feature estratte dai loro SAE, mentre altri lavori hanno esplorato varianti del protocollo, come l'uso di punteggi di simulazione per validare le etichette generate. Il campo sta convergendo verso un approccio standard in cui l'etichettatura automatica tramite LLM è considerata un componente essenziale della pipeline di interpretabilità.

PRISMA si inserisce in questa linea ma con una scelta distintiva: l'intero processo di etichettatura è eseguito da un LLM locale (Gemma 3 27B~\parencite{gemma3report}), senza trasmissione di dati a servizi esterni. Questa scelta, motivata da esigenze di \textbf{privacy}---in particolare per il dominio clinico, dove i dati sono soggetti a stringenti vincoli normativi---dimostra che l'interpretazione automatica è realizzabile anche in contesti in cui l'accesso ad API commerciali non è praticabile.

\subsubsection{Approcci alternativi all'interpretabilità}
\label{subsubsec:01_approcci_alternativi}

Gli Sparse Autoencoder non sono l'unico strumento per sondare le rappresentazioni interne dei modelli linguistici. È utile collocarli nel panorama più ampio dei metodi di interpretabilità, per comprenderne vantaggi e limiti specifici.

Il \textbf{probing}~\parencite{alain2017understanding, belinkov2017neural} consiste nell'addestrare un classificatore lineare (o shallow) sopra le rappresentazioni interne di un modello, per verificare se una determinata proprietà linguistica---parte del discorso, struttura sintattica, ruolo semantico---è codificata in modo accessibile nello spazio delle attivazioni. I risultati del probing hanno dimostrato che i Transformer codificano una ricchezza sorprendente di informazione linguistica strutturata nei loro strati intermedi. Tuttavia, il probing richiede di formulare in anticipo l'ipotesi da testare: può dire \textit{se} un'informazione è presente, ma non \textit{quali} informazioni la rete ha scelto di rappresentare. Gli SAE, al contrario, operano in modo non supervisionato, lasciando che le feature emergano dai dati.

I \textbf{concept-based methods} seguono un approccio diverso: anziché sondare proprietà predefinite, cercano di identificare direzioni nello spazio delle attivazioni che corrispondano a concetti umani. Kim et al.~\parencite{kim2018interpretability} hanno proposto \textbf{TCAV} (\textit{Testing with Concept Activation Vectors}), che utilizza esempi positivi e negativi di un concetto per identificarne la direzione nello spazio delle attivazioni e misurarne l'influenza sulla predizione. Questo approccio richiede però un dataset etichettato per ogni concetto di interesse, limitandone la scalabilità.

L'\textbf{analisi delle attention head}~\parencite{clark2019does} sfrutta la struttura dei Transformer per interpretare il modello attraverso i pattern di attenzione: quali token il modello ``guarda'' quando produce un output. Sebbene intuitiva, questa analisi è stata criticata per il fatto che i pattern di attenzione non sempre riflettono il flusso causale dell'informazione~\parencite{jain2019attention}, e non si estende facilmente alle rappresentazioni dense degli strati intermedi.

Rispetto a tutti questi approcci, gli SAE offrono un vantaggio distintivo: estraggono un \textit{dizionario completo} di feature in modo non supervisionato, senza richiedere ipotesi preliminari su cosa cercare. Il costo di questa generalità è la necessità di validare a posteriori l'interpretabilità delle feature estratte---un problema che l'etichettatura automatica tramite LLM affronta ma non risolve completamente.

\subsubsection{Convergenza delle rappresentazioni e la Platonic Representation Hypothesis}
\label{subsubsec:01_platonic}

Un filone di ricerca apparentemente distinto, ma profondamente connesso al tema dell'interpretabilità, riguarda la \textit{convergenza delle rappresentazioni} tra modelli diversi. Huh et al.~\parencite{huh2024platonicrepresentationhypothesis} hanno proposto la \textbf{Platonic Representation Hypothesis}: modelli diversi, addestrati su dati e modalità diverse (testo, immagini, audio), tendono a convergere verso la stessa struttura relazionale nello spazio delle rappresentazioni. In altri termini, le distanze e le relazioni tra concetti si preservano attraverso modelli radicalmente diversi, suggerendo l'esistenza di una struttura semantica sottostante---una ``realtà platonica''---di cui ogni modello è una proiezione.

Le evidenze empiriche a sostegno di questa ipotesi provengono da studi di \textit{kernel alignment} tra le matrici di similarità di modelli diversi: la correlazione tra le rappresentazioni cresce con la scala dei modelli e con la quantità di dati, come se modelli più capaci convergessero più fedelmente verso la stessa struttura. Questo risultato è coerente con la visione secondo cui i modelli non inventano struttura arbitraria, ma scoprono regolarità statistiche del mondo che, al crescere della scala, approssimano sempre meglio una struttura condivisa.

La connessione con il presente lavoro è diretta: se le rappresentazioni dense di modelli diversi convergono verso la stessa struttura, allora le feature monosemantiche estratte da un SAE non dovrebbero essere artefatti del modello specifico, ma riflettere proprietà del dominio. In questo quadro, il Semantic Compression Ratio introdotto in questa tesi---che misura quanto la struttura semantica riduce i gradi di libertà delle rappresentazioni---potrebbe costituire un \textit{invariante}: una quantità che si preserva attraverso modelli diversi, catturando una proprietà intrinseca del dominio e non della rappresentazione. Questa prospettiva è sviluppata nel Capitolo~\ref{sec:06_risultati}.

\subsubsection{Posizionamento di PRISMA}
\label{subsubsec:01_posizionamento}

Il panorama delineato nelle sezioni precedenti evidenzia un campo in rapida evoluzione, in cui gli Sparse Autoencoder si sono affermati come strumento principale per la decomposizione delle rappresentazioni nei LLM. I lavori esistenti condividono tuttavia un focus comune: l'applicazione degli SAE alle \textbf{attivazioni intermedie} dei Transformer---i vettori residuali o le uscite degli strati MLP---con l'obiettivo di comprendere i meccanismi computazionali interni del modello.

PRISMA si distingue per un obiettivo diverso e complementare: applicare gli SAE agli \textbf{embedding} prodotti dai modelli linguistici---le rappresentazioni vettoriali di documenti nello spazio semantico---per analizzare non il meccanismo interno del modello, ma la \textbf{struttura del dominio} che il modello ha appreso. In questa prospettiva, l'embedding non è un punto intermedio di una computazione da decifrare, ma una \textit{mappa} del significato da esplorare: il SAE diventa uno strumento di analisi del dominio, non solo del modello.

Questa differenza di prospettiva genera contributi specifici che, per quanto a conoscenza dell'autore, non trovano riscontro diretto nella letteratura esistente:

\begin{enumerate}
    \item \textbf{Feature families gerarchiche}: le feature estratte da PRISMA si organizzano spontaneamente in famiglie parent-children coerenti con le tassonomie dei domini di applicazione, una struttura che i lavori precedenti non hanno analizzato sistematicamente.

    \item \textbf{Semantic Compression Ratio}: una metrica quantitativa, basata sull'Effective Rank della matrice delle attivazioni, che misura il grado in cui la struttura semantica vincola le rappresentazioni---fornendo una misura operativa della ``comprimibilità'' di un dominio.

    \item \textbf{Applicazione multi-dominio}: PRISMA è stato applicato sia a un corpus scientifico (arXiv) sia a dati clinici (Pedianet), dimostrando la generalità del metodo su domini con caratteristiche radicalmente diverse.

    \item \textbf{Privacy by design}: l'intera pipeline---dall'addestramento del SAE all'etichettatura delle feature tramite LLM locale---opera senza trasmissione di dati a servizi esterni, un requisito essenziale per il dominio clinico.
\end{enumerate}

La Tabella~\ref{tab:confronto_sae} riassume il posizionamento di PRISMA rispetto ai lavori più direttamente confrontabili.

\begin{table}[ht]
\centering
\small
\caption{Confronto tra i principali lavori sugli SAE per l'interpretabilità e PRISMA.}
\label{tab:confronto_sae}
\begin{tabular}{lcccc}
\toprule
 & \textbf{Target} & \textbf{Sparsità} & \textbf{Interp. auto.} & \textbf{Struttura feature} \\
\midrule
Bricken et al.~\parencite{bricken2023monosemanticity} & Residual stream & L1 & Claude & --- \\
Cunningham et al.~\parencite{cunningham2023sparse} & Residual stream & L1 & --- & --- \\
Templeton et al.~\parencite{templeton2024scaling} & Residual stream & L1 & Claude & --- \\
Gao et al.~\parencite{gao2024scaling} & Residual stream & Top-K & --- & --- \\
Rajamanoharan et al.~\parencite{rajamanoharan2024jumping} & Residual stream & JumpReLU & --- & --- \\
\textbf{PRISMA} & \textbf{Embedding} & \textbf{Top-K} & \textbf{LLM locale} & \textbf{Famiglie + SCR} \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Contributi del lavoro e struttura della tesi}
\label{subsec:01_contributi_struttura}

Alla luce del panorama delineato nella sezione precedente, il presente lavoro offre i seguenti contributi.

Sul piano \textbf{metodologico}, è stato sviluppato PRISMA, un applicativo che implementa l'intera pipeline di analisi: dall'addestramento degli Sparse Autoencoder sugli embedding di un corpus, all'estrazione e all'etichettatura automatica delle feature monosemantiche tramite LLM locale, fino all'analisi quantitativa della struttura delle rappresentazioni sparse. L'architettura adotta un vincolo di sparsità Top-K~\parencite{gao2024scaling} e una auxiliary loss per il recupero dei dead latents, ed è stata progettata per operare interamente in locale, senza trasmissione di dati a servizi esterni.

Sul piano \textbf{empirico}, PRISMA è stato applicato a due domini radicalmente diversi: un corpus di circa 2,5 milioni di abstract scientifici estratti da arXiv e un dataset di cartelle cliniche pediatriche dal network Pedianet. In entrambi i casi, il SAE ha estratto migliaia di feature monosemantiche---concetti atomici interpretabili che un esperto di dominio riconosce immediatamente---organizzate spontaneamente in famiglie gerarchiche coerenti con le tassonomie dei rispettivi domini. La consistenza di questa organizzazione tra domini con caratteristiche tanto diverse suggerisce che la struttura gerarchica non è un artefatto del metodo, ma una proprietà delle rappresentazioni analizzate.

Sul piano \textbf{teorico-quantitativo}, questo lavoro introduce il \textbf{Semantic Compression Ratio} (SCR), una metrica basata sull'Effective Rank della matrice delle attivazioni, che misura il grado in cui la struttura semantica dei dati vincola le rappresentazioni apprese. I risultati mostrano che questa compressione è sistematica, cresce monotonicamente con l'expansion factor del SAE, e raggiunge il 59,9\% per $\rho = 64$: la semantica si manifesta come \textbf{riduzione dei gradi di libertà} nello spazio dei concetti estratti, poiché le feature non sono indipendenti ma vincolate da relazioni di co-attivazione che riflettono la struttura del dominio. L'SCR fornisce inoltre un possibile ponte verso la Platonic Representation Hypothesis~\parencite{huh2024platonicrepresentationhypothesis}: se la compressione semantica è una proprietà del dominio e non del modello, l'SCR potrebbe costituire un invariante misurabile attraverso rappresentazioni diverse.

\subsubsection*{Struttura della tesi}

La tesi è organizzata in sei capitoli, ciascuno dei quali sviluppa un tassello necessario alla costruzione dell'argomento complessivo.

Il \textbf{Capitolo~\ref{sec:02_autoencoders}} introduce gli \textit{Autoencoders}: reti neurali capaci di costruirsi autonomamente una rappresentazione compressa del mondo, codificando l'informazione in uno spazio latente. Il capitolo esplora come diversi tipi di vincoli---architetturali, probabilistici, di sparsità---determinino le proprietà dello spazio latente, preparando il terreno per gli Sparse Autoencoder.

Il \textbf{Capitolo~\ref{sec:03_word_embeddings}} applica questo principio al linguaggio, mostrando come le macchine imparino a rappresentare parole e frasi come vettori in uno spazio geometrico. Il percorso va dalle prime intuizioni sulla semantica distribuzionale fino ai Transformer e ai modelli contestuali come BERT, tracciando l'evoluzione degli \textit{embedding} testuali.

Il \textbf{Capitolo~\ref{sec:04_disentangling_dense_embeddings_with_sparse_autoencoders}} affronta il problema centrale: queste rappresentazioni, per quanto potenti, non sono necessariamente interpretabili dall'uomo. Il capitolo formalizza il problema del \textit{disentanglement}---l'allineamento tra lo spazio latente della macchina e le categorie concettuali umane---e introduce la Superposition Hypothesis come spiegazione dell'opacità delle reti.

Il \textbf{Capitolo~\ref{sec:05_prisma}} presenta PRISMA, il metodo e l'applicativo sviluppato per affrontare questo problema. Il capitolo descrive l'architettura dello Sparse Autoencoder, la funzione di perdita con vincolo Top-K, il sistema di interpretazione automatica delle feature tramite LLM, e le metriche di analisi della struttura delle feature (famiglie semantiche, feature splitting, Effective Rank).

Il \textbf{Capitolo~\ref{sec:06_risultati}} riporta i risultati sperimentali. Dopo aver descritto l'infrastruttura hardware e le motivazioni di privacy che hanno guidato la scelta di un'elaborazione interamente locale, il capitolo presenta le feature estratte dai due domini (scientifico e clinico), analizza la struttura delle famiglie semantiche, e quantifica la compressione semantica tramite l'Effective Rank e il Semantic Compression Ratio. Il capitolo si chiude con una riflessione sugli sviluppi futuri, collegando i risultati ottenuti alla Platonic Representation Hypothesis e alla ricerca di invarianti semantici.