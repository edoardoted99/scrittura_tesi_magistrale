\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {2}Autoencoders}{13}{section.2}\protected@file@percent }
\newlabel{sec:autoencoders}{{2}{13}{Autoencoders}{section.2}{}}
\newlabel{sec:autoencoders@cref}{{[section][2][]2}{[1][13][]13}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Richard P. Feynman, premio Nobel per la Fisica nel 1965. Immagine tratta da Wikipedia~\blx@tocontentsinit {0}\cite {feynman_wikipedia}.\relax }}{14}{figure.caption.4}\protected@file@percent }
\newlabel{fig:feynman}{{1}{14}{Richard P. Feynman, premio Nobel per la Fisica nel 1965. Immagine tratta da Wikipedia~\cite {feynman_wikipedia}.\relax }{figure.caption.4}{}}
\newlabel{fig:feynman@cref}{{[figure][1][]1}{[1][14][]14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Definizione e formulazione generale}{15}{subsection.2.1}\protected@file@percent }
\newlabel{subsec:ae_definition}{{2.1}{15}{Definizione e formulazione generale}{subsection.2.1}{}}
\newlabel{subsec:ae_definition@cref}{{[subsection][1][2]2.1}{[1][14][]15}}
\newlabel{eq:ae_objective}{{4}{15}{Definizione e formulazione generale}{equation.2.4}{}}
\newlabel{eq:ae_objective@cref}{{[equation][4][]4}{[1][15][]15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Il problema dell'identità e la necessità di vincoli}{15}{subsection.2.2}\protected@file@percent }
\newlabel{subsec:identity_problem}{{2.2}{15}{Il problema dell'identità e la necessità di vincoli}{subsection.2.2}{}}
\newlabel{subsec:identity_problem@cref}{{[subsection][2][2]2.2}{[1][15][]15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Introduzione dei vincoli: architettura e regolarizzazione}{16}{subsection.2.3}\protected@file@percent }
\newlabel{subsec:constraints_overview}{{2.3}{16}{Introduzione dei vincoli: architettura e regolarizzazione}{subsection.2.3}{}}
\newlabel{subsec:constraints_overview@cref}{{[subsection][3][2]2.3}{[1][16][]16}}
\newlabel{eq:ae_regularized_objective}{{5}{16}{Introduzione dei vincoli: architettura e regolarizzazione}{equation.2.5}{}}
\newlabel{eq:ae_regularized_objective@cref}{{[equation][5][]5}{[1][16][]16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Vincoli architetturali: bottleneck e riduzione della dimensionalità}{16}{subsubsection.2.3.1}\protected@file@percent }
\newlabel{subsubsec:bottleneck}{{2.3.1}{16}{Vincoli architetturali: bottleneck e riduzione della dimensionalità}{subsubsection.2.3.1}{}}
\newlabel{subsubsec:bottleneck@cref}{{[subsubsection][1][2,3]2.3.1}{[1][16][]16}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Architettura di un autoencoder classico. Il bottleneck forza una compressione informativa che induce l’apprendimento di una rappresentazione latente compatta in uno spazio di dimensione $q<n$.\relax }}{17}{figure.caption.5}\protected@file@percent }
\newlabel{fig:autoencoder_arch}{{2}{17}{Architettura di un autoencoder classico. Il bottleneck forza una compressione informativa che induce l’apprendimento di una rappresentazione latente compatta in uno spazio di dimensione $q<n$.\relax }{figure.caption.5}{}}
\newlabel{fig:autoencoder_arch@cref}{{[figure][2][]2}{[1][16][]17}}
\newlabel{eq:ae_bottleneck}{{6}{17}{Vincoli architetturali: bottleneck e riduzione della dimensionalità}{equation.2.6}{}}
\newlabel{eq:ae_bottleneck@cref}{{[equation][6][]6}{[1][17][]17}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Confronto tra immagini originali (riga superiore) e ricostruzioni prodotte dall’autoencoder (riga inferiore). Il modello preserva le strutture principali nonostante la compressione informativa.\relax }}{17}{figure.caption.6}\protected@file@percent }
\newlabel{fig:reconstructions}{{3}{17}{Confronto tra immagini originali (riga superiore) e ricostruzioni prodotte dall’autoencoder (riga inferiore). Il modello preserva le strutture principali nonostante la compressione informativa.\relax }{figure.caption.6}{}}
\newlabel{fig:reconstructions@cref}{{[figure][3][]3}{[1][17][]17}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Campionamento del decoder su una griglia bidimensionale nello spazio latente. Solo una regione ristretta dello spazio genera immagini semanticamente significative, mentre le restanti producono rumore. La semantica emerge solo in una regione limitata dello spazio latente. \relax }}{18}{figure.caption.7}\protected@file@percent }
\newlabel{fig:latent_manifold}{{4}{18}{Campionamento del decoder su una griglia bidimensionale nello spazio latente. Solo una regione ristretta dello spazio genera immagini semanticamente significative, mentre le restanti producono rumore. La semantica emerge solo in una regione limitata dello spazio latente. \relax }{figure.caption.7}{}}
\newlabel{fig:latent_manifold@cref}{{[figure][4][]4}{[1][17][]18}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Proiezione bidimensionale dello spazio latente appreso dall’autoencoder. I punti sono colorati in base alla classe MNIST, evidenziando la presenza di cluster semanticamente coerenti.\relax }}{19}{figure.caption.8}\protected@file@percent }
\newlabel{fig:latent_space}{{5}{19}{Proiezione bidimensionale dello spazio latente appreso dall’autoencoder. I punti sono colorati in base alla classe MNIST, evidenziando la presenza di cluster semanticamente coerenti.\relax }{figure.caption.8}{}}
\newlabel{fig:latent_space@cref}{{[figure][5][]5}{[1][17][]19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Vincoli nella funzione obiettivo: regolarizzazione dello spazio latente}{20}{subsubsection.2.3.2}\protected@file@percent }
\newlabel{subsubsec:latent_regularization}{{2.3.2}{20}{Vincoli nella funzione obiettivo: regolarizzazione dello spazio latente}{subsubsection.2.3.2}{}}
\newlabel{subsubsec:latent_regularization@cref}{{[subsubsection][2][2,3]2.3.2}{[1][20][]20}}
\newlabel{eq:ae_latent_regularized_objective}{{7}{20}{Vincoli nella funzione obiettivo: regolarizzazione dello spazio latente}{equation.2.7}{}}
\newlabel{eq:ae_latent_regularized_objective@cref}{{[equation][7][]7}{[1][20][]20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}La semantica emerge dai vincoli}{21}{subsection.2.4}\protected@file@percent }
\newlabel{sec:semantics_from_constraints}{{2.4}{21}{La semantica emerge dai vincoli}{subsection.2.4}{}}
\newlabel{sec:semantics_from_constraints@cref}{{[subsection][4][2]2.4}{[1][21][]21}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Il manifold dei dati e le regioni vuote}{21}{subsubsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Analogia fisica: lo spazio delle fasi}{22}{subsubsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.3}Implicazioni per il design di autoencoder}{23}{subsubsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Interpretabilità e Disentanglement}{23}{subsection.2.5}\protected@file@percent }
\newlabel{sec:interpretabilita_disentanglement}{{2.5}{23}{Interpretabilità e Disentanglement}{subsection.2.5}{}}
\newlabel{sec:interpretabilita_disentanglement@cref}{{[subsection][5][2]2.5}{[1][23][]23}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Analogia tra fisica, semantica e risultati sperimentali. (A-B) Il vincolo energetico costringe il pendolo su un'orbita specifica; gli stati esterni sono impossibili. (C-D) Il modello apprende la semantica vincolando il manifold dei dati in una regione densa dello spazio latente. (E-F) Risultati sperimentali su MNIST: il decoder genera transizioni coerenti esplorando il manifold appreso (E), mentre i dati di test si clusterizzano rispettando la struttura imposta (F).\relax }}{24}{figure.caption.9}\protected@file@percent }
\newlabel{fig:physics_semantics_final}{{6}{24}{Analogia tra fisica, semantica e risultati sperimentali. (A-B) Il vincolo energetico costringe il pendolo su un'orbita specifica; gli stati esterni sono impossibili. (C-D) Il modello apprende la semantica vincolando il manifold dei dati in una regione densa dello spazio latente. (E-F) Risultati sperimentali su MNIST: il decoder genera transizioni coerenti esplorando il manifold appreso (E), mentre i dati di test si clusterizzano rispettando la struttura imposta (F).\relax }{figure.caption.9}{}}
\newlabel{fig:physics_semantics_final@cref}{{[figure][6][]6}{[1][23][]24}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}Fattori di variazione e rappresentazioni entangled}{25}{subsubsection.2.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}Definizione di Disentanglement}{26}{subsubsection.2.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.3}Esempio: InfoGAN su MNIST}{26}{subsubsection.2.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Esempio di Disentanglement su MNIST (InfoGAN). Ogni riga mostra l'effetto della variazione di una singola variabile latente mantenendo fisse le altre: è possibile controllare separatamente il tipo di cifra, l'inclinazione e lo spessore \blx@tocontentsinit {0}\parencite {chen2016infoganinterpretablerepresentationlearning}.\relax }}{27}{figure.caption.10}\protected@file@percent }
\newlabel{fig:mnist_disenangled_info_gan}{{7}{27}{Esempio di Disentanglement su MNIST (InfoGAN). Ogni riga mostra l'effetto della variazione di una singola variabile latente mantenendo fisse le altre: è possibile controllare separatamente il tipo di cifra, l'inclinazione e lo spessore \parencite {chen2016infoganinterpretablerepresentationlearning}.\relax }{figure.caption.10}{}}
\newlabel{fig:mnist_disenangled_info_gan@cref}{{[figure][7][]7}{[1][26][]27}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.4}Perché il disentanglement non emerge spontaneamente}{28}{subsubsection.2.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Regolarizzare lo spazio latente: i $\beta $-VAE}{29}{subsection.2.6}\protected@file@percent }
\newlabel{sec:beta_vae}{{2.6}{29}{Regolarizzare lo spazio latente: i \texorpdfstring {$\beta $}{beta}-VAE}{subsection.2.6}{}}
\newlabel{sec:beta_vae@cref}{{[subsection][6][2]2.6}{[1][29][]29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Dai vincoli architetturali ai vincoli probabilistici}{29}{subsubsection.2.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}Formulazione matematica}{29}{subsubsection.2.6.2}\protected@file@percent }
\newlabel{eq:vae_loss}{{12}{30}{Formulazione matematica}{equation.2.12}{}}
\newlabel{eq:vae_loss@cref}{{[equation][12][]12}{[1][30][]30}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.3}Effetto geometrico del vincolo probabilistico}{30}{subsubsection.2.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.4}Il trade-off tra ricostruzione e regolarizzazione}{30}{subsubsection.2.6.4}\protected@file@percent }
\newlabel{eq:beta_vae_loss}{{13}{30}{Il trade-off tra ricostruzione e regolarizzazione}{equation.2.13}{}}
\newlabel{eq:beta_vae_loss@cref}{{[equation][13][]13}{[1][30][]30}}
\newlabel{fig:ae_latent_instability}{{8a}{31}{Spazio latente AE classico: geometria irregolare e sensibile al training.\relax }{figure.caption.11}{}}
\newlabel{fig:ae_latent_instability@cref}{{[subfigure][1][8]8a}{[1][30][]31}}
\newlabel{sub@fig:ae_latent_instability}{{a}{31}{Spazio latente AE classico: geometria irregolare e sensibile al training.\relax }{figure.caption.11}{}}
\newlabel{sub@fig:ae_latent_instability@cref}{{[subfigure][1][8]8a}{[1][30][]31}}
\newlabel{fig:vae_latent_stability}{{8b}{31}{Spazio latente VAE: la regolarizzazione induce una distribuzione compatta.\relax }{figure.caption.11}{}}
\newlabel{fig:vae_latent_stability@cref}{{[subfigure][2][8]8b}{[1][30][]31}}
\newlabel{sub@fig:vae_latent_stability}{{b}{31}{Spazio latente VAE: la regolarizzazione induce una distribuzione compatta.\relax }{figure.caption.11}{}}
\newlabel{sub@fig:vae_latent_stability@cref}{{[subfigure][2][8]8b}{[1][30][]31}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Confronto tra spazi latenti. L'introduzione del vincolo (prior) nel VAE costringe i dati a organizzarsi in una struttura geometrica regolare, prerequisito per la semantica \blx@tocontentsinit {0}\parencite {gordon2020machine}.\relax }}{31}{figure.caption.11}\protected@file@percent }
\newlabel{fig:ae_vs_vae_latent}{{8}{31}{Confronto tra spazi latenti. L'introduzione del vincolo (prior) nel VAE costringe i dati a organizzarsi in una struttura geometrica regolare, prerequisito per la semantica \parencite {gordon2020machine}.\relax }{figure.caption.11}{}}
\newlabel{fig:ae_vs_vae_latent@cref}{{[figure][8][]8}{[1][30][]31}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.5}$\beta $-VAE e disentanglement}{32}{subsubsection.2.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.6}Limiti dei vincoli probabilistici}{32}{subsubsection.2.6.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Sparse Autoencoders}{33}{subsection.2.7}\protected@file@percent }
\newlabel{sec:sparse_autoencoders}{{2.7}{33}{Sparse Autoencoders}{subsection.2.7}{}}
\newlabel{sec:sparse_autoencoders@cref}{{[subsection][7][2]2.7}{[1][32][]33}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.1}Dall'undercomplete all'overcomplete: inversione del paradigma}{33}{subsubsection.2.7.1}\protected@file@percent }
\newlabel{eq:sae_overcomplete}{{14}{33}{Dall'undercomplete all'overcomplete: inversione del paradigma}{equation.2.14}{}}
\newlabel{eq:sae_overcomplete@cref}{{[equation][14][]14}{[1][33][]33}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Architettura di uno Sparse Autoencoder. A differenza dell'autoencoder classico (Fig.~\ref  {fig:autoencoder_arch}), lo spazio latente ha dimensionalità $q > n$, ma solo poche unità (evidenziate in blu) sono attive per ciascun input.\relax }}{33}{figure.caption.12}\protected@file@percent }
\newlabel{fig:sparse_autoencoder_arch}{{9}{33}{Architettura di uno Sparse Autoencoder. A differenza dell'autoencoder classico (Fig.~\ref {fig:autoencoder_arch}), lo spazio latente ha dimensionalità $q > n$, ma solo poche unità (evidenziate in blu) sono attive per ciascun input.\relax }{figure.caption.12}{}}
\newlabel{fig:sparse_autoencoder_arch@cref}{{[figure][9][]9}{[1][33][]33}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.2}Formulazione matematica}{34}{subsubsection.2.7.2}\protected@file@percent }
\newlabel{eq:sae_loss}{{15}{34}{Formulazione matematica}{equation.2.15}{}}
\newlabel{eq:sae_loss@cref}{{[equation][15][]15}{[1][34][]34}}
\@writefile{toc}{\contentsline {paragraph}{Regolarizzazione $\ell _1$.}{34}{section*.13}\protected@file@percent }
\newlabel{eq:l1_sparsity}{{16}{34}{Regolarizzazione $\ell _1$}{equation.2.16}{}}
\newlabel{eq:l1_sparsity@cref}{{[equation][16][]16}{[1][34][]34}}
\@writefile{toc}{\contentsline {paragraph}{Vincolo top-$k$.}{34}{section*.14}\protected@file@percent }
\newlabel{eq:topk_sparsity}{{17}{34}{Vincolo top-$k$}{equation.2.17}{}}
\newlabel{eq:topk_sparsity@cref}{{[equation][17][]17}{[1][34][]34}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.3}Interpretazione geometrica e confronto con i vincoli probabilistici}{35}{subsubsection.2.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.4}Perché la sparsità favorisce l'interpretabilità}{35}{subsubsection.2.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.5}Conclusioni e prospettive}{35}{subsubsection.2.7.5}\protected@file@percent }
\@setckpt{chapters/02_autoencoders}{
\setcounter{page}{37}
\setcounter{equation}{17}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{section}{2}
\setcounter{subsection}{7}
\setcounter{subsubsection}{5}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{9}
\setcounter{table}{1}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{lstnumber}{1}
\setcounter{parentequation}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{46}
\setcounter{maxnames}{1}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{blx@maxsegment@0}{0}
\setcounter{blx@sectionciteorder@0}{44}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{Item}{48}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{39}
\setcounter{FancyVerbLine}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{2}
\setcounter{definition}{0}
\setcounter{lstlisting}{0}
\setcounter{section@level}{3}
}
