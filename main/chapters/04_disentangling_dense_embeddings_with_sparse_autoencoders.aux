\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {4}Il problema del Disentanglement}{70}{section.4}\protected@file@percent }
\newlabel{sec:04_disentangling_dense_embeddings_with_sparse_autoencoders}{{4}{70}{Il problema del Disentanglement}{section.4}{}}
\newlabel{sec:04_disentangling_dense_embeddings_with_sparse_autoencoders@cref}{{[section][4][]4}{[1][70][]70}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Il mondo ideale: rappresentazioni disentangled}{71}{subsection.4.1}\protected@file@percent }
\newlabel{subsec:mondo_ideale}{{4.1}{71}{Il mondo ideale: rappresentazioni disentangled}{subsection.4.1}{}}
\newlabel{subsec:mondo_ideale@cref}{{[subsection][1][4]4.1}{[1][71][]71}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Il trade-off tra capacità predittiva e interpretabilità. Le leggi fisiche classiche sono interpretabili ma limitate nella complessità dei fenomeni che descrivono. Le reti neurali raggiungono capacità predittive elevate, ma producono rappresentazioni opache. L'obiettivo ideale è il punto di convergenza in alto a destra: modelli che uniscono la potenza predittiva alla trasparenza semantica.\relax }}{71}{figure.caption.42}\protected@file@percent }
\newlabel{fig:tradeoff_prediction_interpretability}{{25}{71}{Il trade-off tra capacità predittiva e interpretabilità. Le leggi fisiche classiche sono interpretabili ma limitate nella complessità dei fenomeni che descrivono. Le reti neurali raggiungono capacità predittive elevate, ma producono rappresentazioni opache. L'obiettivo ideale è il punto di convergenza in alto a destra: modelli che uniscono la potenza predittiva alla trasparenza semantica.\relax }{figure.caption.42}{}}
\newlabel{fig:tradeoff_prediction_interpretability@cref}{{[figure][25][]25}{[1][71][]71}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}La rete ideale}{72}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Due spazi vettoriali}{72}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Definizione di disentanglement}{73}{subsubsection.4.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Il paradosso della capacità}{74}{subsection.4.2}\protected@file@percent }
\newlabel{subsec:paradosso_capacita}{{4.2}{74}{Il paradosso della capacità}{subsection.4.2}{}}
\newlabel{subsec:paradosso_capacita@cref}{{[subsection][2][4]4.2}{[1][74][]74}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Confronto tra rappresentazione entangled e disentangled. In alto: rappresentazione entangled con più feature che neuroni—ogni neurone partecipa alla codifica di più concetti, e ogni concetto è distribuito su più neuroni. In basso: rappresentazione disentangled con corrispondenza biunivoca tra neuroni e feature—ogni neurone codifica esattamente un concetto.\relax }}{75}{figure.caption.43}\protected@file@percent }
\newlabel{fig:entangled_vs_disentangled_vert}{{26}{75}{Confronto tra rappresentazione entangled e disentangled. In alto: rappresentazione entangled con più feature che neuroni—ogni neurone partecipa alla codifica di più concetti, e ogni concetto è distribuito su più neuroni. In basso: rappresentazione disentangled con corrispondenza biunivoca tra neuroni e feature—ogni neurone codifica esattamente un concetto.\relax }{figure.caption.43}{}}
\newlabel{fig:entangled_vs_disentangled_vert@cref}{{[figure][26][]26}{[1][74][]75}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Neuroni e feature: una distinzione cruciale}{76}{subsection.4.3}\protected@file@percent }
\newlabel{subsec:neuroni_feature}{{4.3}{76}{Neuroni e feature: una distinzione cruciale}{subsection.4.3}{}}
\newlabel{subsec:neuroni_feature@cref}{{[subsection][3][4]4.3}{[1][76][]76}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}L'analogia geografica}{76}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Analogia geografica tra neuroni e feature. Gli assi cardinali (Nord, Est) rappresentano i neuroni: le direzioni di riferimento del sistema. La direzione Nord-Nord-Est rappresenta una feature: non coincide con nessun asse, ma è definita come loro combinazione ($100 \text  { km Nord} + 50 \text  { km Est} = 112 \text  { km NNE}$). In uno spazio bidimensionale esistono solo 2 assi, ma infinite direzioni.\relax }}{77}{figure.caption.44}\protected@file@percent }
\newlabel{fig:geographic_analogy}{{27}{77}{Analogia geografica tra neuroni e feature. Gli assi cardinali (Nord, Est) rappresentano i neuroni: le direzioni di riferimento del sistema. La direzione Nord-Nord-Est rappresenta una feature: non coincide con nessun asse, ma è definita come loro combinazione ($100 \text { km Nord} + 50 \text { km Est} = 112 \text { km NNE}$). In uno spazio bidimensionale esistono solo 2 assi, ma infinite direzioni.\relax }{figure.caption.44}{}}
\newlabel{fig:geographic_analogy@cref}{{[figure][27][]27}{[1][76][]77}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Direzioni e capacità}{77}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}La Superposition Hypothesis}{78}{subsection.4.4}\protected@file@percent }
\newlabel{subsec:superposition_hypothesis}{{4.4}{78}{La Superposition Hypothesis}{subsection.4.4}{}}
\newlabel{subsec:superposition_hypothesis@cref}{{[subsection][4][4]4.4}{[1][78][]78}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Il caso ideale: direzioni ortogonali}{78}{subsubsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Superposition: più feature che neuroni}{78}{subsubsection.4.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Rappresentazione disentangled ideale (2 feature, 2 neuroni). In questo scenario, le direzioni delle feature ($\mathbf  {w}_i$) coincidono perfettamente con gli assi dei neuroni ($\mathbf  {d}_i$). Questa configurazione garantisce \textbf  {ortogonalità perfetta} ($\mathbf  {w}_{\text  {torta}} \cdot \mathbf  {w}_{\text  {quant}} = 0$): attivare il concetto di ``torta'' non crea alcuna interferenza sulla dimensione della ``meccanica quantistica''.\relax }}{79}{figure.caption.45}\protected@file@percent }
\newlabel{fig:ideal_two_features}{{28}{79}{Rappresentazione disentangled ideale (2 feature, 2 neuroni). In questo scenario, le direzioni delle feature ($\mathbf {w}_i$) coincidono perfettamente con gli assi dei neuroni ($\mathbf {d}_i$). Questa configurazione garantisce \textbf {ortogonalità perfetta} ($\mathbf {w}_{\text {torta}} \cdot \mathbf {w}_{\text {quant}} = 0$): attivare il concetto di ``torta'' non crea alcuna interferenza sulla dimensione della ``meccanica quantistica''.\relax }{figure.caption.45}{}}
\newlabel{fig:ideal_two_features@cref}{{[figure][28][]28}{[1][78][]79}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Perché la superposition funziona: la sparsità}{79}{subsubsection.4.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Il fenomeno della Superposition (3 feature in 2 dimensioni). A differenza del caso ideale, qui le direzioni delle feature ($\mathbf  {w}_i$) non coincidono più con gli assi dei neuroni ($\mathbf  {d}_i$). Per comprimere tre concetti in uno spazio bidimensionale mantenendo la massima distinguibilità possibile, i vettori si dispongono a $120^\circ $. Questa configurazione è un esempio di \textbf  {quasi-ortogonalità}: il prodotto scalare non è zero, ma è minimizzato per ridurre le interferenze.\relax }}{80}{figure.caption.46}\protected@file@percent }
\newlabel{fig:superposition_three_features}{{29}{80}{Il fenomeno della Superposition (3 feature in 2 dimensioni). A differenza del caso ideale, qui le direzioni delle feature ($\mathbf {w}_i$) non coincidono più con gli assi dei neuroni ($\mathbf {d}_i$). Per comprimere tre concetti in uno spazio bidimensionale mantenendo la massima distinguibilità possibile, i vettori si dispongono a $120^\circ $. Questa configurazione è un esempio di \textbf {quasi-ortogonalità}: il prodotto scalare non è zero, ma è minimizzato per ridurre le interferenze.\relax }{figure.caption.46}{}}
\newlabel{fig:superposition_three_features@cref}{{[figure][29][]29}{[1][78][]80}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}I due problemi della superposition}{80}{subsection.4.5}\protected@file@percent }
\newlabel{subsec:due_problemi}{{4.5}{80}{I due problemi della superposition}{subsection.4.5}{}}
\newlabel{subsec:due_problemi@cref}{{[subsection][5][4]4.5}{[1][80][]80}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Superposition al variare della sparsità (adattato da Elhage et al.~\blx@tocontentsinit {0}\parencite {elhage2022superposition}). Con sparsità crescente, la rete può rappresentare più feature nello stesso spazio, accettando direzioni sempre meno ortogonali.\relax }}{81}{figure.caption.47}\protected@file@percent }
\newlabel{fig:sparsity_superposition}{{30}{81}{Superposition al variare della sparsità (adattato da Elhage et al.~\parencite {elhage2022superposition}). Con sparsità crescente, la rete può rappresentare più feature nello stesso spazio, accettando direzioni sempre meno ortogonali.\relax }{figure.caption.47}{}}
\newlabel{fig:sparsity_superposition@cref}{{[figure][30][]30}{[1][79][]81}}
\newlabel{eq:linear_superposition}{{33}{81}{I due problemi della superposition}{equation.4.33}{}}
\newlabel{eq:linear_superposition@cref}{{[equation][33][]33}{[1][81][]81}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}Problema 1: l'opacità}{81}{subsubsection.4.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.2}Problema 2: l'interferenza}{82}{subsubsection.4.5.2}\protected@file@percent }
\newlabel{eq:interference}{{35}{82}{Problema 2: l'interferenza}{equation.4.35}{}}
\newlabel{eq:interference@cref}{{[equation][35][]35}{[1][82][]82}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.3}Un esempio concreto}{82}{subsubsection.4.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Interferenza (\textit  {Crosstalk}) nella Superposition. L'input contiene esclusivamente il concetto ``torta'' ($\mathbf  {x} = \mathbf  {w}_{\text  {torta}}$), ma a causa della non-ortogonalità geometrica, si genera una proiezione non nulla sulla direzione ``quantistica''. Poiché $\mathbf  {x} \cdot \mathbf  {w}_{\text  {quant}} = \qopname  \relax o{cos}(120^\circ ) = -0.5$, il modello registra un'attivazione spuria (negativa) per un concetto assente. Questo ``rumore'' è il prezzo da pagare per rappresentare più feature che neuroni.\relax }}{83}{figure.caption.48}\protected@file@percent }
\newlabel{fig:interference}{{31}{83}{Interferenza (\textit {Crosstalk}) nella Superposition. L'input contiene esclusivamente il concetto ``torta'' ($\mathbf {x} = \mathbf {w}_{\text {torta}}$), ma a causa della non-ortogonalità geometrica, si genera una proiezione non nulla sulla direzione ``quantistica''. Poiché $\mathbf {x} \cdot \mathbf {w}_{\text {quant}} = \cos (120^\circ ) = -0.5$, il modello registra un'attivazione spuria (negativa) per un concetto assente. Questo ``rumore'' è il prezzo da pagare per rappresentare più feature che neuroni.\relax }{figure.caption.48}{}}
\newlabel{fig:interference@cref}{{[figure][31][]31}{[1][82][]83}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.4}Riepilogo}{83}{subsubsection.4.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}L'idea del disentanglement}{84}{subsection.4.6}\protected@file@percent }
\newlabel{subsec:idea_disentanglement}{{4.6}{84}{L'idea del disentanglement}{subsection.4.6}{}}
\newlabel{subsec:idea_disentanglement@cref}{{[subsection][6][4]4.6}{[1][83][]84}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.1}Invertire la superposition}{84}{subsubsection.4.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.2}Lo spazio overcomplete}{85}{subsubsection.4.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.3}La sparsità come vincolo}{85}{subsubsection.4.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces Dall'embedding compresso allo spazio overcomplete. A sinistra, BERT codifica le feature come direzioni oblique $\mathbf  {w}_i$ in $d$ dimensioni—mescolate e sconosciute (superposition). A destra, l'encoder $f_\theta $ proietta in uno spazio con $n \gg d$ dimensioni, dove ogni feature può avere un asse dedicato: la coordinata $h_i$ indica direttamente l'intensità della feature $f_i$.\relax }}{86}{figure.caption.49}\protected@file@percent }
\newlabel{fig:overcomplete_space}{{32}{86}{Dall'embedding compresso allo spazio overcomplete. A sinistra, BERT codifica le feature come direzioni oblique $\mathbf {w}_i$ in $d$ dimensioni—mescolate e sconosciute (superposition). A destra, l'encoder $f_\theta $ proietta in uno spazio con $n \gg d$ dimensioni, dove ogni feature può avere un asse dedicato: la coordinata $h_i$ indica direttamente l'intensità della feature $f_i$.\relax }{figure.caption.49}{}}
\newlabel{fig:overcomplete_space@cref}{{[figure][32][]32}{[1][85][]86}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.4}Il risultato: feature separate}{87}{subsubsection.4.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Verso una soluzione: PRISMA}{87}{subsection.4.7}\protected@file@percent }
\newlabel{subsec:verso_soluzione}{{4.7}{87}{Verso una soluzione: PRISMA}{subsection.4.7}{}}
\newlabel{subsec:verso_soluzione@cref}{{[subsection][7][4]4.7}{[1][87][]87}}
\@setckpt{chapters/04_disentangling_dense_embeddings_with_sparse_autoencoders}{
\setcounter{page}{89}
\setcounter{equation}{41}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{section}{4}
\setcounter{subsection}{7}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{32}
\setcounter{table}{3}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{lstnumber}{1}
\setcounter{parentequation}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{56}
\setcounter{maxnames}{1}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{blx@maxsegment@0}{0}
\setcounter{blx@sectionciteorder@0}{58}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{Item}{82}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{82}
\setcounter{FancyVerbLine}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{32}
\setcounter{definition}{0}
\setcounter{lstlisting}{0}
\setcounter{section@level}{2}
}
