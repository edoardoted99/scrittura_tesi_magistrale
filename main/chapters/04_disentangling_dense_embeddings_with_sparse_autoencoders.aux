\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {4}Il problema del Disentanglement}{58}{section.4}\protected@file@percent }
\newlabel{sec:04_disentangling_dense_embeddings_with_sparse_autoencoders}{{4}{58}{Il problema del Disentanglement}{section.4}{}}
\newlabel{sec:04_disentangling_dense_embeddings_with_sparse_autoencoders@cref}{{[section][4][]4}{[1][58][]58}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Il mondo ideale: rappresentazioni disentangled}{59}{subsection.4.1}\protected@file@percent }
\newlabel{subsec:mondo_ideale}{{4.1}{59}{Il mondo ideale: rappresentazioni disentangled}{subsection.4.1}{}}
\newlabel{subsec:mondo_ideale@cref}{{[subsection][1][4]4.1}{[1][59][]59}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Il trade-off tra capacità predittiva e interpretabilità. Le leggi fisiche classiche sono interpretabili ma limitate nella complessità dei fenomeni che descrivono. Le reti neurali raggiungono capacità predittive elevate, ma producono rappresentazioni opache. L'obiettivo ideale è il punto di convergenza in alto a destra: modelli che uniscono la potenza predittiva alla trasparenza semantica.}}{59}{figure.caption.40}\protected@file@percent }
\newlabel{fig:tradeoff_prediction_interpretability}{{25}{59}{Il trade-off tra capacità predittiva e interpretabilità. Le leggi fisiche classiche sono interpretabili ma limitate nella complessità dei fenomeni che descrivono. Le reti neurali raggiungono capacità predittive elevate, ma producono rappresentazioni opache. L'obiettivo ideale è il punto di convergenza in alto a destra: modelli che uniscono la potenza predittiva alla trasparenza semantica}{figure.caption.40}{}}
\newlabel{fig:tradeoff_prediction_interpretability@cref}{{[figure][25][]25}{[1][59][]59}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}La rete ideale}{60}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Due spazi vettoriali}{60}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Definizione di disentanglement}{61}{subsubsection.4.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Il paradosso della capacità}{62}{subsection.4.2}\protected@file@percent }
\newlabel{subsec:paradosso_capacita}{{4.2}{62}{Il paradosso della capacità}{subsection.4.2}{}}
\newlabel{subsec:paradosso_capacita@cref}{{[subsection][2][4]4.2}{[1][62][]62}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Confronto tra rappresentazione entangled e disentangled. In alto: rappresentazione entangled con più feature che neuroni—ogni neurone partecipa alla codifica di più concetti, e ogni concetto è distribuito su più neuroni. In basso: rappresentazione disentangled con corrispondenza biunivoca tra neuroni e feature—ogni neurone codifica esattamente un concetto.}}{63}{figure.caption.41}\protected@file@percent }
\newlabel{fig:entangled_vs_disentangled_vert}{{26}{63}{Confronto tra rappresentazione entangled e disentangled. In alto: rappresentazione entangled con più feature che neuroni—ogni neurone partecipa alla codifica di più concetti, e ogni concetto è distribuito su più neuroni. In basso: rappresentazione disentangled con corrispondenza biunivoca tra neuroni e feature—ogni neurone codifica esattamente un concetto}{figure.caption.41}{}}
\newlabel{fig:entangled_vs_disentangled_vert@cref}{{[figure][26][]26}{[1][62][]63}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Neuroni e feature: una distinzione cruciale}{64}{subsection.4.3}\protected@file@percent }
\newlabel{subsec:neuroni_feature}{{4.3}{64}{Neuroni e feature: una distinzione cruciale}{subsection.4.3}{}}
\newlabel{subsec:neuroni_feature@cref}{{[subsection][3][4]4.3}{[1][64][]64}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}L'analogia geografica}{64}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Analogia geografica tra neuroni e feature. Gli assi cardinali (Nord, Est) rappresentano i neuroni: le direzioni di riferimento del sistema. La direzione Nord-Nord-Est rappresenta una feature: non coincide con nessun asse, ma è definita come loro combinazione ($100 \text  { km Nord} + 50 \text  { km Est} = 112 \text  { km NNE}$). In uno spazio bidimensionale esistono solo 2 assi, ma infinite direzioni.}}{65}{figure.caption.42}\protected@file@percent }
\newlabel{fig:geographic_analogy}{{27}{65}{Analogia geografica tra neuroni e feature. Gli assi cardinali (Nord, Est) rappresentano i neuroni: le direzioni di riferimento del sistema. La direzione Nord-Nord-Est rappresenta una feature: non coincide con nessun asse, ma è definita come loro combinazione ($100 \text { km Nord} + 50 \text { km Est} = 112 \text { km NNE}$). In uno spazio bidimensionale esistono solo 2 assi, ma infinite direzioni}{figure.caption.42}{}}
\newlabel{fig:geographic_analogy@cref}{{[figure][27][]27}{[1][64][]65}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Direzioni e capacità}{65}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}La Superposition Hypothesis}{66}{subsection.4.4}\protected@file@percent }
\newlabel{subsec:superposition_hypothesis}{{4.4}{66}{La Superposition Hypothesis}{subsection.4.4}{}}
\newlabel{subsec:superposition_hypothesis@cref}{{[subsection][4][4]4.4}{[1][66][]66}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Il caso ideale: direzioni ortogonali}{66}{subsubsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Superposition: più feature che neuroni}{66}{subsubsection.4.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Rappresentazione disentangled ideale (2 feature, 2 neuroni). In questo scenario, le direzioni delle feature ($\mathbf  {w}_i$) coincidono perfettamente con gli assi dei neuroni ($\mathbf  {d}_i$). Questa configurazione garantisce \textbf  {ortogonalità perfetta} ($\mathbf  {w}_{\text  {torta}} \cdot \mathbf  {w}_{\text  {quant}} = 0$): attivare il concetto di ``torta'' non crea alcuna interferenza sulla dimensione della ``meccanica quantistica''.}}{67}{figure.caption.43}\protected@file@percent }
\newlabel{fig:ideal_two_features}{{28}{67}{Rappresentazione disentangled ideale (2 feature, 2 neuroni). In questo scenario, le direzioni delle feature ($\mathbf {w}_i$) coincidono perfettamente con gli assi dei neuroni ($\mathbf {d}_i$). Questa configurazione garantisce \textbf {ortogonalità perfetta} ($\mathbf {w}_{\text {torta}} \cdot \mathbf {w}_{\text {quant}} = 0$): attivare il concetto di ``torta'' non crea alcuna interferenza sulla dimensione della ``meccanica quantistica''}{figure.caption.43}{}}
\newlabel{fig:ideal_two_features@cref}{{[figure][28][]28}{[1][66][]67}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Perché la superposition funziona: la sparsità}{67}{subsubsection.4.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Il fenomeno della Superposition (3 feature in 2 dimensioni). A differenza del caso ideale, qui le direzioni delle feature ($\mathbf  {w}_i$) non coincidono più con gli assi dei neuroni ($\mathbf  {d}_i$). Per comprimere tre concetti in uno spazio bidimensionale mantenendo la massima distinguibilità possibile, i vettori si dispongono a $120^\circ $. Questa configurazione è un esempio di \textbf  {quasi-ortogonalità}: il prodotto scalare non è zero, ma è minimizzato per ridurre le interferenze.}}{68}{figure.caption.44}\protected@file@percent }
\newlabel{fig:superposition_three_features}{{29}{68}{Il fenomeno della Superposition (3 feature in 2 dimensioni). A differenza del caso ideale, qui le direzioni delle feature ($\mathbf {w}_i$) non coincidono più con gli assi dei neuroni ($\mathbf {d}_i$). Per comprimere tre concetti in uno spazio bidimensionale mantenendo la massima distinguibilità possibile, i vettori si dispongono a $120^\circ $. Questa configurazione è un esempio di \textbf {quasi-ortogonalità}: il prodotto scalare non è zero, ma è minimizzato per ridurre le interferenze}{figure.caption.44}{}}
\newlabel{fig:superposition_three_features@cref}{{[figure][29][]29}{[1][66][]68}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}I due problemi della superposition}{68}{subsection.4.5}\protected@file@percent }
\newlabel{subsec:due_problemi}{{4.5}{68}{I due problemi della superposition}{subsection.4.5}{}}
\newlabel{subsec:due_problemi@cref}{{[subsection][5][4]4.5}{[1][68][]68}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Superposition al variare della sparsità (adattato da Elhage et al.~\blx@tocontentsinit {0}\parencite {elhage2022superposition}). Con sparsità crescente, la rete può rappresentare più feature nello stesso spazio, accettando direzioni sempre meno ortogonali.}}{69}{figure.caption.45}\protected@file@percent }
\newlabel{fig:sparsity_superposition}{{30}{69}{Superposition al variare della sparsità (adattato da Elhage et al.~\parencite {elhage2022superposition}). Con sparsità crescente, la rete può rappresentare più feature nello stesso spazio, accettando direzioni sempre meno ortogonali}{figure.caption.45}{}}
\newlabel{fig:sparsity_superposition@cref}{{[figure][30][]30}{[1][67][]69}{}{}{}}
\newlabel{eq:linear_superposition}{{33}{69}{I due problemi della superposition}{equation.33}{}}
\newlabel{eq:linear_superposition@cref}{{[equation][33][]33}{[1][69][]69}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}Problema 1: l'opacità}{69}{subsubsection.4.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.2}Problema 2: l'interferenza}{70}{subsubsection.4.5.2}\protected@file@percent }
\newlabel{eq:interference}{{35}{70}{Problema 2: l'interferenza}{equation.35}{}}
\newlabel{eq:interference@cref}{{[equation][35][]35}{[1][70][]70}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.3}Un esempio concreto}{70}{subsubsection.4.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Interferenza (\textit  {Crosstalk}) nella Superposition. L'input contiene esclusivamente il concetto ``torta'' ($\mathbf  {x} = \mathbf  {w}_{\text  {torta}}$), ma a causa della non-ortogonalità geometrica, si genera una proiezione non nulla sulla direzione ``quantistica''. Poiché $\mathbf  {x} \cdot \mathbf  {w}_{\text  {quant}} = \cos (120^\circ ) = -0.5$, il modello registra un'attivazione spuria (negativa) per un concetto assente. Questo ``rumore'' è il prezzo da pagare per rappresentare più feature che neuroni.}}{71}{figure.caption.46}\protected@file@percent }
\newlabel{fig:interference}{{31}{71}{Interferenza (\textit {Crosstalk}) nella Superposition. L'input contiene esclusivamente il concetto ``torta'' ($\mathbf {x} = \mathbf {w}_{\text {torta}}$), ma a causa della non-ortogonalità geometrica, si genera una proiezione non nulla sulla direzione ``quantistica''. Poiché $\mathbf {x} \cdot \mathbf {w}_{\text {quant}} = \cos (120^\circ ) = -0.5$, il modello registra un'attivazione spuria (negativa) per un concetto assente. Questo ``rumore'' è il prezzo da pagare per rappresentare più feature che neuroni}{figure.caption.46}{}}
\newlabel{fig:interference@cref}{{[figure][31][]31}{[1][70][]71}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.4}Riepilogo}{71}{subsubsection.4.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}L'idea del disentanglement}{72}{subsection.4.6}\protected@file@percent }
\newlabel{subsec:idea_disentanglement}{{4.6}{72}{L'idea del disentanglement}{subsection.4.6}{}}
\newlabel{subsec:idea_disentanglement@cref}{{[subsection][6][4]4.6}{[1][71][]72}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.1}Invertire la superposition}{72}{subsubsection.4.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.2}Lo spazio overcomplete}{73}{subsubsection.4.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.3}La sparsità come vincolo}{73}{subsubsection.4.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces Dall'embedding compresso allo spazio overcomplete. A sinistra, BERT codifica le feature come direzioni oblique $\mathbf  {w}_i$ in $d$ dimensioni—mescolate e sconosciute (superposition). A destra, l'encoder $f_\theta $ proietta in uno spazio con $n \gg d$ dimensioni, dove ogni feature può avere un asse dedicato: la coordinata $h_i$ indica direttamente l'intensità della feature $f_i$.}}{74}{figure.caption.47}\protected@file@percent }
\newlabel{fig:overcomplete_space}{{32}{74}{Dall'embedding compresso allo spazio overcomplete. A sinistra, BERT codifica le feature come direzioni oblique $\mathbf {w}_i$ in $d$ dimensioni—mescolate e sconosciute (superposition). A destra, l'encoder $f_\theta $ proietta in uno spazio con $n \gg d$ dimensioni, dove ogni feature può avere un asse dedicato: la coordinata $h_i$ indica direttamente l'intensità della feature $f_i$}{figure.caption.47}{}}
\newlabel{fig:overcomplete_space@cref}{{[figure][32][]32}{[1][73][]74}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.4}Il risultato: feature separate}{75}{subsubsection.4.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Verso una soluzione: PRISMA}{75}{subsection.4.7}\protected@file@percent }
\newlabel{subsec:verso_soluzione}{{4.7}{75}{Verso una soluzione: PRISMA}{subsection.4.7}{}}
\newlabel{subsec:verso_soluzione@cref}{{[subsection][7][4]4.7}{[1][75][]75}{}{}{}}
\@setckpt{chapters/04_disentangling_dense_embeddings_with_sparse_autoencoders}{
\setcounter{page}{77}
\setcounter{equation}{41}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{section}{4}
\setcounter{subsection}{7}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{32}
\setcounter{table}{2}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{lstnumber}{1}
\setcounter{parentequation}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{22}
\setcounter{maxnames}{1}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{section@level}{2}
\setcounter{Item}{78}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{72}
\setcounter{FancyVerbLine}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{32}
\setcounter{definition}{0}
\setcounter{lstlisting}{0}
}
