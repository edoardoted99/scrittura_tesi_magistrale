\begin{titlepage}
    \centering

    \includegraphics[width=\textwidth]{pictures/tesiSCIENZE_TECNOLOGIE.jpg}\\[1.2cm]

    {\Huge\bfseries PRISMA}\\[0.4cm]

    {\large
    Disentanglement e scomposizione in feature monosemantiche delle rappresentazioni latenti nei LLM\\
    tramite Sparse Autoencoders
    }\\[1.2cm]

    {\Large
    Corso di Laurea Magistrale in Fisica
    }\\[2.2cm]

    \begin{flushleft}
    \textbf{Relatore esterno:} Prof.\ Mirko Cesarini\\
    \textbf{Relatore interno:} Prof.\ Marco Gherardi
    \end{flushleft}

    \vfill

    \begin{flushright}
    \textbf{Candidato:}\\
    Edoardo Tedesco
    \end{flushright}

    \vfill

    {\large Anno Accademico 2024--2025}
\end{titlepage}

\clearpage

\begin{abstract}
Le rappresentazioni dense apprese dai modelli linguistici --- gli embedding --- codificano informazione semantica ricchissima in spazi ad alta dimensionalità, ma risultano opache: ogni neurone partecipa alla codifica di molteplici concetti (polisemanticità), rendendo impossibile un'ispezione diretta del contenuto rappresentato. La \textit{Superposition Hypothesis} spiega questo fenomeno come una strategia di compressione in cui il numero di concetti codificati supera il numero di neuroni disponibili, sfruttando la sparsità naturale delle feature per stiparle come direzioni quasi-ortogonali nello spazio delle attivazioni. Per invertire questo processo è stato sviluppato PRISMA (\textit{Projection of Representations for Interpretability via Sparse Monosemantic Autoencoders}), un metodo e un applicativo che --- analogamente a un prisma ottico che scompone la luce bianca nelle sue componenti cromatiche --- scompone le rappresentazioni dense nei concetti atomici che le costituiscono. Lo strumento centrale è lo \textit{Sparse Autoencoder} (SAE): un'architettura neurale che proietta gli embedding in uno spazio latente overcomplete, la cui dimensionalità è controllata dall'\textit{expansion factor} $\rho = n/d$, definito come il rapporto tra la dimensione dello spazio latente sparso $n$ e quella dello spazio denso degli embedding $d$. Un vincolo di sparsità Top-K forza ogni input ad attivare solo poche direzioni in questo spazio espanso, ciascuna corrispondente a un concetto identificabile, mentre la linearità del decoder garantisce che ogni feature contribuisca in modo indipendente e trasparente alla ricostruzione. L'etichettatura semantica delle feature è automatizzata tramite un LLM locale (Gemma 3 27B), eseguito interamente sulla macchina di elaborazione senza trasmissione di dati a servizi esterni. PRISMA è stato applicato a un corpus multidisciplinare di circa 2,5 milioni di abstract scientifici estratti da arXiv. Il SAE ha estratto migliaia di feature monosemantiche --- concetti atomici interpretabili che un esperto di dominio riconosce immediatamente --- organizzate spontaneamente in 590 famiglie gerarchiche coerenti con le tassonomie del dominio scientifico. Per quantificare il grado in cui la struttura semantica dei dati vincola le rappresentazioni apprese, si è proposto il \textit{Semantic Compression Ratio} (SCR), una metrica originale basata sull'\textit{Effective Rank} della matrice delle attivazioni, che misura la riduzione della dimensionalità effettiva del codice sparso rispetto a un'ipotesi nulla priva di contenuto semantico. I risultati mostrano che questa compressione è sistematica, cresce monotonicamente con l'expansion factor e raggiunge il 59,9\% per $\rho = 64$: la semantica si manifesta come riduzione dei gradi di libertà nello spazio dei concetti estratti, poiché i concetti non sono indipendenti ma vincolati da relazioni di co-attivazione che riflettono la struttura del dominio.
\end{abstract}