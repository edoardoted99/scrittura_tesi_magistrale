\babel@toc {italian}{}\relax 
\babel@toc {italian}{}\relax 
\contentsline {section}{\numberline {1}Introduzione}{1}{section.1}%
\contentsline {section}{\numberline {2}Autoencoders e Sparse Autoencoders}{2}{section.2}%
\contentsline {subsubsection}{\numberline {2.0.1}Apprendimento non supervisionato}{6}{subsubsection.2.0.1}%
\contentsline {subsubsection}{\numberline {2.0.2}Encoder, decoder e spazio latente}{6}{subsubsection.2.0.2}%
\contentsline {subsubsection}{\numberline {2.0.3}Funzione obiettivo e errore di ricostruzione}{7}{subsubsection.2.0.3}%
\contentsline {subsection}{\numberline {2.1}Il problema dell’identità e la necessità di vincoli}{8}{subsection.2.1}%
\contentsline {subsubsection}{\numberline {2.1.1}Bottleneck e riduzione della dimensionalità}{8}{subsubsection.2.1.1}%
\contentsline {subsubsection}{\numberline {2.1.2}Introduzione di vincoli}{9}{subsubsection.2.1.2}%
\contentsline {subsubsection}{\numberline {2.1.3}Relazioni con la PCA}{10}{subsubsection.2.1.3}%
\contentsline {paragraph}{Caso di un autoencoder lineare}{12}{section*.8}%
\contentsline {paragraph}{Caso di un autoencoder non lineare}{13}{section*.9}%
\contentsline {subsection}{\numberline {2.2}Interpretabilità delle feature latenti}{14}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1}Rappresentazioni latenti disentangled}{15}{subsubsection.2.2.1}%
\contentsline {subsection}{\numberline {2.3}Sparse Autoencoders}{16}{subsection.2.3}%
\contentsline {section}{\numberline {3}Embeddings}{18}{section.3}%
\contentsline {section}{\numberline {4}Introduzione}{19}{section.4}%
\contentsline {section}{\numberline {5}L'ipotesi distribuzionale}{19}{section.5}%
\contentsline {section}{\numberline {6}Ipotesi di Osgood}{20}{section.6}%
\contentsline {section}{\numberline {7}Embeddings}{21}{section.7}%
\contentsline {subsection}{\numberline {7.1}Embeddings count-based}{22}{subsection.7.1}%
\contentsline {subsubsection}{\numberline {7.1.1}Matrice termine-documento}{22}{subsubsection.7.1.1}%
\contentsline {subsubsection}{\numberline {7.1.2}Matrice termine-termine}{23}{subsubsection.7.1.2}%
\contentsline {subsection}{\numberline {7.2}Riduzione dimensionale tramite SVD}{25}{subsection.7.2}%
\contentsline {subsection}{\numberline {7.3}Cosine Similarity}{26}{subsection.7.3}%
\contentsline {subsection}{\numberline {7.4}Word2Vec: un approccio predittivo}{27}{subsection.7.4}%
\contentsline {subsubsection}{\numberline {7.4.1}Il classificatore e la funzione sigmoide}{27}{subsubsection.7.4.1}%
\contentsline {subsubsection}{\numberline {7.4.2}Apprendimento e Negative Sampling}{28}{subsubsection.7.4.2}%
\contentsline {subsubsection}{\numberline {7.4.3}Perché due matrici? Il ruolo di $W$ e $C$}{28}{subsubsection.7.4.3}%
\contentsline {subsection}{\numberline {7.5}Proprietà semantiche degli embeddings}{29}{subsection.7.5}%
\contentsline {section}{\numberline {8}Embeddings dinamici}{31}{section.8}%
\contentsline {section}{\numberline {9}Reti Neurali Ricorrenti}{32}{section.9}%
\contentsline {subsection}{\numberline {9.1}RNN come Language Models}{34}{subsection.9.1}%
\contentsline {subsection}{\numberline {9.2}Generaizone di Embeddings tramite RNN}{35}{subsection.9.2}%
\contentsline {subsection}{\numberline {9.3}RNN Bidirezionali (Bi-RNN)}{36}{subsection.9.3}%
\contentsline {subsection}{\numberline {9.4}Il problema del Gradiente Svanente}{36}{subsection.9.4}%
\contentsline {section}{\numberline {10}LSTM: Long Short-Term Memory}{37}{section.10}%
\contentsline {subsection}{\numberline {10.1}Meccanismi di Gating}{38}{subsection.10.1}%
\contentsline {subsection}{\numberline {10.2}Le equazioni del modello}{38}{subsection.10.2}%
\contentsline {subsection}{\numberline {10.3}Modularità ed Embeddings}{39}{subsection.10.3}%
\contentsline {section}{\numberline {11}Architettura Encoder-Decoder e limite del \textit {bottleneck}}{39}{section.11}%
\contentsline {subsection}{\numberline {11.1}Il problema \textit {sequence-to-sequence}}{40}{subsection.11.1}%
\contentsline {subsection}{\numberline {11.2}Il limite del \textit {bottleneck} informativo}{42}{subsection.11.2}%
\contentsline {subsection}{\numberline {11.3}Soluzione al bottleneck: Meccanismo dell'attenzione}{43}{subsection.11.3}%
\contentsline {subsection}{\numberline {11.4}Verso i Transformer}{45}{subsection.11.4}%
\contentsline {section}{\numberline {12}Il Transformer}{46}{section.12}%
\contentsline {subsection}{\numberline {12.1}Self-attention}{48}{subsection.12.1}%
\contentsline {subsubsection}{\numberline {12.1.1}Motivazione: dalle rappresentazioni statiche alle rappresentazioni contestuali}{48}{subsubsection.12.1.1}%
\contentsline {subsubsection}{\numberline {12.1.2}Self-attention causale: dominio informativo e vincolo di autoregressione}{50}{subsubsection.12.1.2}%
\contentsline {subsubsection}{\numberline {12.1.3}Intuizione: self-attention come combinazione pesata del contesto}{50}{subsubsection.12.1.3}%
\contentsline {subsubsection}{\numberline {12.1.4}Scaled dot-product attention: ruoli di query, key e value}{51}{subsubsection.12.1.4}%
\contentsline {paragraph}{Punteggi di compatibilità (scores).}{51}{section*.26}%
\contentsline {paragraph}{Normalizzazione tramite softmax e vincolo causale.}{52}{section*.27}%
\contentsline {paragraph}{Aggregazione dei value e proiezione in output.}{52}{section*.28}%
\contentsline {subsubsection}{\numberline {12.1.5}Forma matriciale e dimensioni (utile per l’implementazione)}{52}{subsubsection.12.1.5}%
\contentsline {subsubsection}{\numberline {12.1.6}Multi-head attention: pluralità di criteri di selezione}{54}{subsubsection.12.1.6}%
\contentsline {subsubsection}{\numberline {12.1.7}Osservazione conclusiva: self-attention e contestualizzazione progressiva}{54}{subsubsection.12.1.7}%
\contentsline {subsection}{\numberline {12.2}Blocco Transformer}{55}{subsection.12.2}%
\contentsline {subsubsection}{\numberline {12.2.1}Input del blocco e informazione di posizione}{56}{subsubsection.12.2.1}%
\contentsline {subsubsection}{\numberline {12.2.2}Residual stream: il flusso informativo del token}{56}{subsubsection.12.2.2}%
\contentsline {subsubsection}{\numberline {12.2.3}Perché la LayerNorm (motivazione)}{58}{subsubsection.12.2.3}%
\contentsline {subsubsection}{\numberline {12.2.4}Feedforward network (ruolo)}{58}{subsubsection.12.2.4}%
\contentsline {subsubsection}{\numberline {12.2.5}Equazioni del blocco (variante \emph {prenorm})}{59}{subsubsection.12.2.5}%
\contentsline {subsubsection}{\numberline {12.2.6}L’attenzione come “movimento” di informazione tra stream}{59}{subsubsection.12.2.6}%
\contentsline {subsection}{\numberline {12.3}Parallelizzazione del calcolo con una singola matrice $X$}{59}{subsection.12.3}%
\contentsline {subsubsection}{\numberline {12.3.1}Impacchettare la sequenza in una matrice}{60}{subsubsection.12.3.1}%
\contentsline {subsubsection}{\numberline {12.3.2}Self-attention in forma matriciale (una testa)}{60}{subsubsection.12.3.2}%
\contentsline {subsubsection}{\numberline {12.3.3}Mascheramento causale: eliminare il futuro}{61}{subsubsection.12.3.3}%
\contentsline {subsubsection}{\numberline {12.3.4}Schema completo per una testa (in parallelo)}{62}{subsubsection.12.3.4}%
\contentsline {subsubsection}{\numberline {12.3.5}Costo computazionale e dipendenza quadratica}{62}{subsubsection.12.3.5}%
\contentsline {subsubsection}{\numberline {12.3.6}Multi-head attention in parallelo}{62}{subsubsection.12.3.6}%
\contentsline {subsubsection}{\numberline {12.3.7}Il blocco Transformer in forma parallela}{63}{subsubsection.12.3.7}%
\contentsline {subsection}{\numberline {12.4}L'input del Transformer: embeddings di token e di posizione}{64}{subsection.12.4}%
\contentsline {subsubsection}{\numberline {12.4.1}Token embeddings e matrice di embedding}{64}{subsubsection.12.4.1}%
\contentsline {paragraph}{Selezione via one-hot (interpretazione equivalente).}{64}{section*.36}%
\contentsline {paragraph}{Dalla sequenza alla matrice.}{65}{section*.38}%
\contentsline {subsubsection}{\numberline {12.4.2}Perché servono gli embeddings posizionali}{65}{subsubsection.12.4.2}%
\contentsline {subsubsection}{\numberline {12.4.3}Posizione assoluta e composizione dell’input}{66}{subsubsection.12.4.3}%
\contentsline {subsection}{\numberline {12.5}Limiti della posizione assoluta e alternative: sinusoidale e posizione relativa}{66}{subsection.12.5}%
\contentsline {paragraph}{Embeddings posizionali sinusoidali.}{67}{section*.41}%
\contentsline {paragraph}{Posizione relativa.}{67}{section*.42}%
\contentsline {subsection}{\numberline {12.6}La \textit {language modeling head}}{68}{subsection.12.6}%
\contentsline {subsubsection}{\numberline {12.6.1}Cosa entra e cosa esce: dal vettore $h_N^L$ alle probabilità sul vocabolario}{68}{subsubsection.12.6.1}%
\contentsline {subsubsection}{\numberline {12.6.2}Logits e livello di \textit {unembedding}}{68}{subsubsection.12.6.2}%
\contentsline {paragraph}{Weight tying: perché spesso $U=E^\top $.}{69}{section*.44}%
\contentsline {subsubsection}{\numberline {12.6.3}Softmax: da logits a probabilità}{70}{subsubsection.12.6.3}%
\contentsline {subsubsection}{\numberline {12.6.4}Dal modello alla generazione: scegliere il prossimo token}{70}{subsubsection.12.6.4}%
\contentsline {subsubsection}{\numberline {12.6.5}Visione d’insieme: un \textit {decoder-only} che impila blocchi}{70}{subsubsection.12.6.5}%
\contentsline {subsection}{\numberline {12.7}Nota: \textit {logit lens} e terminologia \textit {decoder-only}}{70}{subsection.12.7}%
\contentsline {paragraph}{Nota terminologica: \textit {decoder-only}.}{72}{section*.46}%
\contentsline {section}{\numberline {13}Large Language Models}{72}{section.13}%
\contentsline {subsection}{\numberline {13.1}Large Language Models con Transformer: generazione condizionata}{73}{subsection.13.1}%
\contentsline {paragraph}{Perché “predire parole” è utile per tanti task.}{73}{section*.48}%
\contentsline {paragraph}{Esempio: riassunto come generazione condizionata.}{74}{section*.49}%
\contentsline {paragraph}{Nota sul passo successivo (ponte verso BERT).}{74}{section*.52}%
\contentsline {section}{\numberline {14}Sampling per la generazione con LLM}{76}{section.14}%
\contentsline {subsection}{\numberline {14.1}Perché non basta il campionamento ``puro''}{76}{subsection.14.1}%
\contentsline {subsection}{\numberline {14.2}Top-$k$ sampling}{77}{subsection.14.2}%
\contentsline {subsection}{\numberline {14.3}Top-$p$ (nucleus) sampling}{77}{subsection.14.3}%
\contentsline {subsection}{\numberline {14.4}Temperature sampling}{78}{subsection.14.4}%
\contentsline {section}{\numberline {15}Pretraining dei Large Language Models}{78}{section.15}%
\contentsline {subsection}{\numberline {15.1}Setup, notazione e obiettivo di language modeling}{79}{subsection.15.1}%
\contentsline {subsection}{\numberline {15.2}Self-supervision e funzione obiettivo}{79}{subsection.15.2}%
\contentsline {subsection}{\numberline {15.3}Teacher forcing}{80}{subsection.15.3}%
\contentsline {subsection}{\numberline {15.4}Efficienza computazionale: parallelismo nei transformer}{81}{subsection.15.4}%
\contentsline {subsection}{\numberline {15.5}Dati di pretraining: fonti e filtraggio}{82}{subsection.15.5}%
\contentsline {paragraph}{Filtri di qualità e sicurezza.}{83}{section*.55}%
\contentsline {paragraph}{Aspetti etici e legali (panoramica).}{83}{section*.56}%
\contentsline {subsection}{\numberline {15.6}Dal pretraining all'adattamento: finetuning}{83}{subsection.15.6}%
\contentsline {paragraph}{Tipi di adattamento (senza anticipare modelli specifici).}{83}{section*.58}%
\contentsline {paragraph}{Collegamento alle sezioni successive.}{84}{section*.59}%
\contentsline {section}{\numberline {16}Il problema del disentanglement}{85}{section.16}%
\contentsline {section}{\numberline {17}Introduzione}{85}{section.17}%
\contentsline {section}{\numberline {18}Il fenomeno della Superposizione}{85}{section.18}%
\contentsline {subsection}{\numberline {18.1}Formalismo matematico}{86}{subsection.18.1}%
\contentsline {subsection}{\numberline {18.2}Il ruolo della sparsità}{86}{subsection.18.2}%
\contentsline {section}{\numberline {19}Disentangling Dense Embeddings \\with Sparse Autoencoders}{88}{section.19}%
\contentsline {section}{\numberline {20}Introduzione}{88}{section.20}%
\contentsline {section}{\numberline {21}Ipotesi della superposizione}{88}{section.21}%
\contentsline {section}{\numberline {22}Metodologia e Architettura}{88}{section.22}%
\contentsline {subsection}{\numberline {22.1}Definizione del Modello}{89}{subsection.22.1}%
\contentsline {subsection}{\numberline {22.2}Vincolo di Sparsità \textit {k-Sparse}}{89}{subsection.22.2}%
\contentsline {subsection}{\numberline {22.3}Funzione di Costo e Addestramento}{90}{subsection.22.3}%
\contentsline {section}{\numberline {23}Interpretazione Automatizzata delle Feature}{90}{section.23}%
\contentsline {section}{\numberline {24}Feature Families e Struttura Gerarchica}{91}{section.24}%
\contentsline {section}{\numberline {25}Feature Families e Struttura Gerarchica}{91}{section.25}%
\contentsline {paragraph}{Criterio di identificazione delle feature genitore e figlie}{92}{section*.60}%
\contentsline {subsection}{\numberline {25.1}Costruzione del Grafo di Co-occorrenza}{92}{subsection.25.1}%
\contentsline {subsection}{\numberline {25.2}Identificazione delle Feature Families}{93}{subsection.25.2}%
\contentsline {section}{\numberline {26}Prisma}{95}{section.26}%
\contentsline {section}{\numberline {27}Introduzione}{95}{section.27}%
\contentsline {section}{\numberline {28}Architettura}{95}{section.28}%
\contentsline {section}{\numberline {29}Generazione Embeddings}{95}{section.29}%
\contentsline {subsection}{\numberline {29.1}Gestione di documenti lunghi: strategia \textit {chunk-and-average}}{96}{subsection.29.1}%
\contentsline {paragraph}{Tokenizzazione e vincoli di lunghezza}{96}{section*.62}%
\contentsline {paragraph}{Segmentazione in chunk contigui}{97}{section*.63}%
\contentsline {paragraph}{Embedding per chunk e concetto di pooling}{97}{section*.64}%
\contentsline {paragraph}{Aggregazione a livello documento}{98}{section*.65}%
\contentsline {section}{\numberline {30}Training SAE}{98}{section.30}%
\contentsline {section}{\numberline {31}Interpretazione}{99}{section.31}%
\contentsline {section}{\numberline {32}Esperimenti}{99}{section.32}%
\contentsline {section}{\numberline {33}Pedianet}{99}{section.33}%
\contentsline {subsection}{\numberline {33.1}Scelta del modello di embedding}{100}{subsection.33.1}%
\contentsline {subsection}{\numberline {33.2}Esperimento}{100}{subsection.33.2}%
\contentsline {section}{\numberline {34}Abstracts}{101}{section.34}%
\contentsline {section}{\numberline {35}Qual `e la dimensionalit`a intrinseca della variet`a su cui poggia il linguaggio umano?}{102}{section.35}%
\contentsline {section}{\numberline {36}PubMed}{102}{section.36}%
