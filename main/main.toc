\babel@toc {italian}{}\relax 
\babel@toc {italian}{}\relax 
\contentsline {section}{\numberline {1}Introduzione}{1}{section.1}%
\contentsline {section}{\numberline {2}Autoencoders}{2}{section.2}%
\contentsline {subsubsection}{\numberline {2.0.1}Apprendimento non supervisionato}{6}{subsubsection.2.0.1}%
\contentsline {subsubsection}{\numberline {2.0.2}Encoder, decoder e spazio latente}{6}{subsubsection.2.0.2}%
\contentsline {subsubsection}{\numberline {2.0.3}Funzione obiettivo e errore di ricostruzione}{7}{subsubsection.2.0.3}%
\contentsline {subsection}{\numberline {2.1}Il problema dell’identità e la necessità di vincoli}{8}{subsection.2.1}%
\contentsline {subsubsection}{\numberline {2.1.1}Bottleneck e riduzione della dimensionalità}{8}{subsubsection.2.1.1}%
\contentsline {subsubsection}{\numberline {2.1.2}Introduzione di vincoli}{9}{subsubsection.2.1.2}%
\contentsline {subsubsection}{\numberline {2.1.3}Relazioni con la PCA}{10}{subsubsection.2.1.3}%
\contentsline {subsubsection}{\numberline {2.1.4}Caso di un autoencoder lineare}{12}{subsubsection.2.1.4}%
\contentsline {subsubsection}{\numberline {2.1.5}Caso di un autoencoder non lineare}{13}{subsubsection.2.1.5}%
\contentsline {subsection}{\numberline {2.2}Interpretabilità delle feature latenti}{14}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}Rappresentazioni latenti disentangled}{15}{subsection.2.3}%
\contentsline {subsection}{\numberline {2.4}Sparse Autoencoders}{18}{subsection.2.4}%
\contentsline {section}{\numberline {3}Word Embeddings}{21}{section.3}%
\contentsline {subsection}{\numberline {3.1}Introduzione}{22}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}L'ipotesi distribuzionale}{22}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Ipotesi di Osgood}{23}{subsection.3.3}%
\contentsline {subsection}{\numberline {3.4}Verso i word embeddings}{24}{subsection.3.4}%
\contentsline {subsection}{\numberline {3.5}Embeddings count-based}{25}{subsection.3.5}%
\contentsline {subsubsection}{\numberline {3.5.1}Matrice termine-documento}{25}{subsubsection.3.5.1}%
\contentsline {subsubsection}{\numberline {3.5.2}Matrice termine-termine}{26}{subsubsection.3.5.2}%
\contentsline {subsection}{\numberline {3.6}Riduzione dimensionale tramite SVD}{28}{subsection.3.6}%
\contentsline {subsection}{\numberline {3.7}Cosine Similarity}{29}{subsection.3.7}%
\contentsline {subsection}{\numberline {3.8}Word2Vec: un approccio predittivo}{30}{subsection.3.8}%
\contentsline {subsubsection}{\numberline {3.8.1}Il classificatore e la funzione sigmoide}{30}{subsubsection.3.8.1}%
\contentsline {subsubsection}{\numberline {3.8.2}Perché due matrici? Il ruolo di $W$ e $C$}{31}{subsubsection.3.8.2}%
\contentsline {subsection}{\numberline {3.9}Proprietà semantiche degli embeddings}{32}{subsection.3.9}%
\contentsline {subsection}{\numberline {3.10}Embeddings dinamici}{34}{subsection.3.10}%
\contentsline {section}{\numberline {4}Reti Neurali Ricorrenti}{35}{section.4}%
\contentsline {subsection}{\numberline {4.1}RNN come Language Models}{36}{subsection.4.1}%
\contentsline {subsubsection}{\numberline {4.1.1}Generaizone di Embeddings tramite RNN}{38}{subsubsection.4.1.1}%
\contentsline {subsubsection}{\numberline {4.1.2}RNN Bidirezionali (Bi-RNN)}{38}{subsubsection.4.1.2}%
\contentsline {subsubsection}{\numberline {4.1.3}Il problema del Gradiente Svanente}{39}{subsubsection.4.1.3}%
\contentsline {subsection}{\numberline {4.2}LSTM: Long Short-Term Memory}{40}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Meccanismi di Gating}{40}{subsection.4.3}%
\contentsline {subsection}{\numberline {4.4}Le equazioni del modello}{40}{subsection.4.4}%
\contentsline {subsubsection}{\numberline {4.4.1}Modularità ed Embeddings}{42}{subsubsection.4.4.1}%
\contentsline {section}{\numberline {5}Architettura Encoder-Decoder e limite del \textit {bottleneck}}{42}{section.5}%
\contentsline {subsection}{\numberline {5.1}Il problema \textit {sequence-to-sequence}}{43}{subsection.5.1}%
\contentsline {subsubsection}{\numberline {5.1.1}Il limite del \textit {bottleneck} informativo}{45}{subsubsection.5.1.1}%
\contentsline {subsubsection}{\numberline {5.1.2}Soluzione al bottleneck: Meccanismo dell'attenzione}{46}{subsubsection.5.1.2}%
\contentsline {subsubsection}{\numberline {5.1.3}Verso i Transformer}{48}{subsubsection.5.1.3}%
\contentsline {subsection}{\numberline {5.2}Il Transformer}{49}{subsection.5.2}%
\contentsline {subsection}{\numberline {5.3}Self-attention}{51}{subsection.5.3}%
\contentsline {subsubsection}{\numberline {5.3.1}Motivazione: dalle rappresentazioni statiche alle rappresentazioni contestuali}{51}{subsubsection.5.3.1}%
\contentsline {subsubsection}{\numberline {5.3.2}Self-attention causale: dominio informativo e vincolo di autoregressione}{53}{subsubsection.5.3.2}%
\contentsline {subsubsection}{\numberline {5.3.3}Intuizione: self-attention come combinazione pesata del contesto}{53}{subsubsection.5.3.3}%
\contentsline {subsubsection}{\numberline {5.3.4}Scaled dot-product attention: ruoli di query, key e value}{54}{subsubsection.5.3.4}%
\contentsline {paragraph}{Punteggi di compatibilità (scores).}{54}{section*.27}%
\contentsline {paragraph}{Normalizzazione tramite softmax e vincolo causale.}{55}{section*.28}%
\contentsline {paragraph}{Aggregazione dei value e proiezione in output.}{55}{section*.29}%
\contentsline {subsubsection}{\numberline {5.3.5}Forma matriciale e dimensioni (utile per l’implementazione)}{55}{subsubsection.5.3.5}%
\contentsline {subsubsection}{\numberline {5.3.6}Multi-head attention: pluralità di criteri di selezione}{57}{subsubsection.5.3.6}%
\contentsline {subsubsection}{\numberline {5.3.7}Osservazione conclusiva: self-attention e contestualizzazione progressiva}{57}{subsubsection.5.3.7}%
\contentsline {subsection}{\numberline {5.4}Blocco Transformer}{58}{subsection.5.4}%
\contentsline {subsubsection}{\numberline {5.4.1}Input del blocco e informazione di posizione}{59}{subsubsection.5.4.1}%
\contentsline {subsubsection}{\numberline {5.4.2}Residual stream: il flusso informativo del token}{59}{subsubsection.5.4.2}%
\contentsline {subsubsection}{\numberline {5.4.3}Perché la LayerNorm (motivazione)}{61}{subsubsection.5.4.3}%
\contentsline {subsubsection}{\numberline {5.4.4}Feedforward network (ruolo)}{61}{subsubsection.5.4.4}%
\contentsline {subsubsection}{\numberline {5.4.5}Equazioni del blocco (variante \emph {prenorm})}{62}{subsubsection.5.4.5}%
\contentsline {subsubsection}{\numberline {5.4.6}L’attenzione come “movimento” di informazione tra stream}{62}{subsubsection.5.4.6}%
\contentsline {subsection}{\numberline {5.5}Parallelizzazione del calcolo con una singola matrice $X$}{62}{subsection.5.5}%
\contentsline {subsubsection}{\numberline {5.5.1}Impacchettare la sequenza in una matrice}{63}{subsubsection.5.5.1}%
\contentsline {subsubsection}{\numberline {5.5.2}Self-attention in forma matriciale (una testa)}{63}{subsubsection.5.5.2}%
\contentsline {subsubsection}{\numberline {5.5.3}Mascheramento causale: eliminare il futuro}{64}{subsubsection.5.5.3}%
\contentsline {subsubsection}{\numberline {5.5.4}Schema completo per una testa (in parallelo)}{65}{subsubsection.5.5.4}%
\contentsline {subsubsection}{\numberline {5.5.5}Costo computazionale e dipendenza quadratica}{65}{subsubsection.5.5.5}%
\contentsline {subsubsection}{\numberline {5.5.6}Multi-head attention in parallelo}{65}{subsubsection.5.5.6}%
\contentsline {subsubsection}{\numberline {5.5.7}Il blocco Transformer in forma parallela}{66}{subsubsection.5.5.7}%
\contentsline {subsection}{\numberline {5.6}L'input del Transformer: embeddings di token e di posizione}{67}{subsection.5.6}%
\contentsline {subsubsection}{\numberline {5.6.1}Token embeddings e matrice di embedding}{67}{subsubsection.5.6.1}%
\contentsline {paragraph}{Selezione via one-hot (interpretazione equivalente).}{67}{section*.37}%
\contentsline {paragraph}{Dalla sequenza alla matrice.}{68}{section*.39}%
\contentsline {subsubsection}{\numberline {5.6.2}Perché servono gli embeddings posizionali}{68}{subsubsection.5.6.2}%
\contentsline {subsubsection}{\numberline {5.6.3}Posizione assoluta e composizione dell’input}{69}{subsubsection.5.6.3}%
\contentsline {subsection}{\numberline {5.7}Limiti della posizione assoluta e alternative: sinusoidale e posizione relativa}{69}{subsection.5.7}%
\contentsline {paragraph}{Embeddings posizionali sinusoidali.}{70}{section*.42}%
\contentsline {paragraph}{Posizione relativa.}{70}{section*.43}%
\contentsline {subsection}{\numberline {5.8}La \textit {language modeling head}}{71}{subsection.5.8}%
\contentsline {subsubsection}{\numberline {5.8.1}Cosa entra e cosa esce: dal vettore $h_N^L$ alle probabilità sul vocabolario}{71}{subsubsection.5.8.1}%
\contentsline {subsubsection}{\numberline {5.8.2}Logits e livello di \textit {unembedding}}{71}{subsubsection.5.8.2}%
\contentsline {paragraph}{Weight tying: perché spesso $U=E^\top $.}{72}{section*.45}%
\contentsline {subsubsection}{\numberline {5.8.3}Softmax: da logits a probabilità}{73}{subsubsection.5.8.3}%
\contentsline {subsubsection}{\numberline {5.8.4}Dal modello alla generazione: scegliere il prossimo token}{73}{subsubsection.5.8.4}%
\contentsline {subsubsection}{\numberline {5.8.5}Visione d’insieme: un \textit {decoder-only} che impila blocchi}{73}{subsubsection.5.8.5}%
\contentsline {subsection}{\numberline {5.9}Nota: \textit {logit lens} e terminologia \textit {decoder-only}}{73}{subsection.5.9}%
\contentsline {paragraph}{Nota terminologica: \textit {decoder-only}.}{75}{section*.47}%
\contentsline {section}{\numberline {6}Large Language Models}{75}{section.6}%
\contentsline {subsection}{\numberline {6.1}Large Language Models con Transformer: generazione condizionata}{76}{subsection.6.1}%
\contentsline {paragraph}{Perché “predire parole” è utile per tanti task.}{76}{section*.49}%
\contentsline {paragraph}{Esempio: riassunto come generazione condizionata.}{77}{section*.50}%
\contentsline {paragraph}{Nota sul passo successivo (ponte verso BERT).}{77}{section*.53}%
\contentsline {section}{\numberline {7}Sampling per la generazione con LLM}{79}{section.7}%
\contentsline {subsection}{\numberline {7.1}Perché non basta il campionamento ``puro''}{79}{subsection.7.1}%
\contentsline {subsection}{\numberline {7.2}Top-$k$ sampling}{80}{subsection.7.2}%
\contentsline {subsection}{\numberline {7.3}Top-$p$ (nucleus) sampling}{80}{subsection.7.3}%
\contentsline {subsection}{\numberline {7.4}Temperature sampling}{81}{subsection.7.4}%
\contentsline {section}{\numberline {8}Pretraining dei Large Language Models}{81}{section.8}%
\contentsline {subsection}{\numberline {8.1}Setup, notazione e obiettivo di language modeling}{82}{subsection.8.1}%
\contentsline {subsection}{\numberline {8.2}Self-supervision e funzione obiettivo}{82}{subsection.8.2}%
\contentsline {subsection}{\numberline {8.3}Teacher forcing}{83}{subsection.8.3}%
\contentsline {subsection}{\numberline {8.4}Efficienza computazionale: parallelismo nei transformer}{84}{subsection.8.4}%
\contentsline {subsection}{\numberline {8.5}Dati di pretraining: fonti e filtraggio}{85}{subsection.8.5}%
\contentsline {paragraph}{Filtri di qualità e sicurezza.}{86}{section*.56}%
\contentsline {paragraph}{Aspetti etici e legali (panoramica).}{86}{section*.57}%
\contentsline {subsection}{\numberline {8.6}Dal pretraining all'adattamento: finetuning}{86}{subsection.8.6}%
\contentsline {paragraph}{Tipi di adattamento (senza anticipare modelli specifici).}{86}{section*.59}%
\contentsline {paragraph}{Collegamento alle sezioni successive.}{87}{section*.60}%
\contentsline {section}{\numberline {9}Il problema del disentanglement}{88}{section.9}%
\contentsline {section}{\numberline {10}Introduzione}{88}{section.10}%
\contentsline {section}{\numberline {11}Il fenomeno della Superposizione}{88}{section.11}%
\contentsline {subsection}{\numberline {11.1}Formalismo matematico}{89}{subsection.11.1}%
\contentsline {subsection}{\numberline {11.2}Il ruolo della sparsità}{89}{subsection.11.2}%
\contentsline {section}{\numberline {12}Disentangling Dense Embeddings \\with Sparse Autoencoders}{91}{section.12}%
\contentsline {section}{\numberline {13}Introduzione}{91}{section.13}%
\contentsline {section}{\numberline {14}Ipotesi della superposizione}{91}{section.14}%
\contentsline {section}{\numberline {15}Metodologia e Architettura}{91}{section.15}%
\contentsline {subsection}{\numberline {15.1}Definizione del Modello}{92}{subsection.15.1}%
\contentsline {subsection}{\numberline {15.2}Vincolo di Sparsità \textit {k-Sparse}}{92}{subsection.15.2}%
\contentsline {subsection}{\numberline {15.3}Funzione di Costo e Addestramento}{93}{subsection.15.3}%
\contentsline {section}{\numberline {16}Interpretazione Automatizzata delle Feature}{93}{section.16}%
\contentsline {section}{\numberline {17}Feature Families e Struttura Gerarchica}{94}{section.17}%
\contentsline {section}{\numberline {18}Feature Families e Struttura Gerarchica}{94}{section.18}%
\contentsline {paragraph}{Criterio di identificazione delle feature genitore e figlie}{95}{section*.61}%
\contentsline {subsection}{\numberline {18.1}Costruzione del Grafo di Co-occorrenza}{95}{subsection.18.1}%
\contentsline {subsection}{\numberline {18.2}Identificazione delle Feature Families}{96}{subsection.18.2}%
\contentsline {section}{\numberline {19}Prisma}{98}{section.19}%
\contentsline {section}{\numberline {20}Introduzione}{98}{section.20}%
\contentsline {section}{\numberline {21}Architettura}{98}{section.21}%
\contentsline {section}{\numberline {22}Generazione Embeddings}{98}{section.22}%
\contentsline {subsection}{\numberline {22.1}Gestione di documenti lunghi: strategia \textit {chunk-and-average}}{99}{subsection.22.1}%
\contentsline {paragraph}{Tokenizzazione e vincoli di lunghezza}{99}{section*.63}%
\contentsline {paragraph}{Segmentazione in chunk contigui}{100}{section*.64}%
\contentsline {paragraph}{Embedding per chunk e concetto di pooling}{100}{section*.65}%
\contentsline {paragraph}{Aggregazione a livello documento}{101}{section*.66}%
\contentsline {section}{\numberline {23}Training SAE}{101}{section.23}%
\contentsline {section}{\numberline {24}Interpretazione}{102}{section.24}%
\contentsline {section}{\numberline {25}Esperimenti}{102}{section.25}%
\contentsline {section}{\numberline {26}Pedianet}{102}{section.26}%
\contentsline {subsection}{\numberline {26.1}Scelta del modello di embedding}{103}{subsection.26.1}%
\contentsline {subsection}{\numberline {26.2}Esperimento}{103}{subsection.26.2}%
\contentsline {section}{\numberline {27}Abstracts}{104}{section.27}%
\contentsline {section}{\numberline {28}Qual `e la dimensionalit`a intrinseca della variet`a su cui poggia il linguaggio umano?}{105}{section.28}%
\contentsline {section}{\numberline {29}PubMed}{105}{section.29}%
