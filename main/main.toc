\babel@toc {italian}{}\relax 
\babel@toc {italian}{}\relax 
\contentsline {section}{\numberline {1}Introduzione}{1}{section.1}%
\contentsline {section}{\numberline {2}Autoencoders}{2}{section.2}%
\contentsline {subsubsection}{\numberline {2.0.1}Apprendimento non supervisionato}{6}{subsubsection.2.0.1}%
\contentsline {subsubsection}{\numberline {2.0.2}Encoder, decoder e spazio latente}{6}{subsubsection.2.0.2}%
\contentsline {subsubsection}{\numberline {2.0.3}Funzione obiettivo e errore di ricostruzione}{7}{subsubsection.2.0.3}%
\contentsline {subsection}{\numberline {2.1}Il problema dell’identità e la necessità di vincoli}{8}{subsection.2.1}%
\contentsline {subsubsection}{\numberline {2.1.1}Bottleneck e riduzione della dimensionalità}{8}{subsubsection.2.1.1}%
\contentsline {subsubsection}{\numberline {2.1.2}Introduzione di vincoli}{9}{subsubsection.2.1.2}%
\contentsline {subsubsection}{\numberline {2.1.3}Relazioni con la PCA}{10}{subsubsection.2.1.3}%
\contentsline {subsubsection}{\numberline {2.1.4}Caso di un autoencoder lineare}{12}{subsubsection.2.1.4}%
\contentsline {subsubsection}{\numberline {2.1.5}Caso di un autoencoder non lineare}{13}{subsubsection.2.1.5}%
\contentsline {subsection}{\numberline {2.2}Interpretabilità delle feature latenti}{14}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}Rappresentazioni latenti disentangled}{15}{subsection.2.3}%
\contentsline {subsection}{\numberline {2.4}Sparse Autoencoders}{18}{subsection.2.4}%
\contentsline {section}{\numberline {3}Embeddings}{21}{section.3}%
\contentsline {section}{\numberline {4}Introduzione}{22}{section.4}%
\contentsline {section}{\numberline {5}L'ipotesi distribuzionale}{22}{section.5}%
\contentsline {section}{\numberline {6}Ipotesi di Osgood}{23}{section.6}%
\contentsline {section}{\numberline {7}Embeddings}{24}{section.7}%
\contentsline {subsection}{\numberline {7.1}Embeddings count-based}{25}{subsection.7.1}%
\contentsline {subsubsection}{\numberline {7.1.1}Matrice termine-documento}{25}{subsubsection.7.1.1}%
\contentsline {subsubsection}{\numberline {7.1.2}Matrice termine-termine}{26}{subsubsection.7.1.2}%
\contentsline {subsection}{\numberline {7.2}Riduzione dimensionale tramite SVD}{28}{subsection.7.2}%
\contentsline {subsection}{\numberline {7.3}Cosine Similarity}{29}{subsection.7.3}%
\contentsline {subsection}{\numberline {7.4}Word2Vec: un approccio predittivo}{30}{subsection.7.4}%
\contentsline {subsubsection}{\numberline {7.4.1}Il classificatore e la funzione sigmoide}{30}{subsubsection.7.4.1}%
\contentsline {subsubsection}{\numberline {7.4.2}Apprendimento e Negative Sampling}{31}{subsubsection.7.4.2}%
\contentsline {subsubsection}{\numberline {7.4.3}Perché due matrici? Il ruolo di $W$ e $C$}{31}{subsubsection.7.4.3}%
\contentsline {subsection}{\numberline {7.5}Proprietà semantiche degli embeddings}{32}{subsection.7.5}%
\contentsline {section}{\numberline {8}Embeddings dinamici}{34}{section.8}%
\contentsline {section}{\numberline {9}Reti Neurali Ricorrenti}{35}{section.9}%
\contentsline {subsection}{\numberline {9.1}RNN come Language Models}{37}{subsection.9.1}%
\contentsline {subsection}{\numberline {9.2}Generaizone di Embeddings tramite RNN}{38}{subsection.9.2}%
\contentsline {subsection}{\numberline {9.3}RNN Bidirezionali (Bi-RNN)}{39}{subsection.9.3}%
\contentsline {subsection}{\numberline {9.4}Il problema del Gradiente Svanente}{39}{subsection.9.4}%
\contentsline {section}{\numberline {10}LSTM: Long Short-Term Memory}{40}{section.10}%
\contentsline {subsection}{\numberline {10.1}Meccanismi di Gating}{41}{subsection.10.1}%
\contentsline {subsection}{\numberline {10.2}Le equazioni del modello}{41}{subsection.10.2}%
\contentsline {subsection}{\numberline {10.3}Modularità ed Embeddings}{42}{subsection.10.3}%
\contentsline {section}{\numberline {11}Architettura Encoder-Decoder e limite del \textit {bottleneck}}{42}{section.11}%
\contentsline {subsection}{\numberline {11.1}Il problema \textit {sequence-to-sequence}}{43}{subsection.11.1}%
\contentsline {subsection}{\numberline {11.2}Il limite del \textit {bottleneck} informativo}{45}{subsection.11.2}%
\contentsline {subsection}{\numberline {11.3}Soluzione al bottleneck: Meccanismo dell'attenzione}{46}{subsection.11.3}%
\contentsline {subsection}{\numberline {11.4}Verso i Transformer}{48}{subsection.11.4}%
\contentsline {section}{\numberline {12}Il Transformer}{49}{section.12}%
\contentsline {subsection}{\numberline {12.1}Self-attention}{51}{subsection.12.1}%
\contentsline {subsubsection}{\numberline {12.1.1}Motivazione: dalle rappresentazioni statiche alle rappresentazioni contestuali}{51}{subsubsection.12.1.1}%
\contentsline {subsubsection}{\numberline {12.1.2}Self-attention causale: dominio informativo e vincolo di autoregressione}{53}{subsubsection.12.1.2}%
\contentsline {subsubsection}{\numberline {12.1.3}Intuizione: self-attention come combinazione pesata del contesto}{53}{subsubsection.12.1.3}%
\contentsline {subsubsection}{\numberline {12.1.4}Scaled dot-product attention: ruoli di query, key e value}{54}{subsubsection.12.1.4}%
\contentsline {paragraph}{Punteggi di compatibilità (scores).}{54}{section*.27}%
\contentsline {paragraph}{Normalizzazione tramite softmax e vincolo causale.}{55}{section*.28}%
\contentsline {paragraph}{Aggregazione dei value e proiezione in output.}{55}{section*.29}%
\contentsline {subsubsection}{\numberline {12.1.5}Forma matriciale e dimensioni (utile per l’implementazione)}{55}{subsubsection.12.1.5}%
\contentsline {subsubsection}{\numberline {12.1.6}Multi-head attention: pluralità di criteri di selezione}{57}{subsubsection.12.1.6}%
\contentsline {subsubsection}{\numberline {12.1.7}Osservazione conclusiva: self-attention e contestualizzazione progressiva}{57}{subsubsection.12.1.7}%
\contentsline {subsection}{\numberline {12.2}Blocco Transformer}{58}{subsection.12.2}%
\contentsline {subsubsection}{\numberline {12.2.1}Input del blocco e informazione di posizione}{59}{subsubsection.12.2.1}%
\contentsline {subsubsection}{\numberline {12.2.2}Residual stream: il flusso informativo del token}{59}{subsubsection.12.2.2}%
\contentsline {subsubsection}{\numberline {12.2.3}Perché la LayerNorm (motivazione)}{61}{subsubsection.12.2.3}%
\contentsline {subsubsection}{\numberline {12.2.4}Feedforward network (ruolo)}{61}{subsubsection.12.2.4}%
\contentsline {subsubsection}{\numberline {12.2.5}Equazioni del blocco (variante \emph {prenorm})}{62}{subsubsection.12.2.5}%
\contentsline {subsubsection}{\numberline {12.2.6}L’attenzione come “movimento” di informazione tra stream}{62}{subsubsection.12.2.6}%
\contentsline {subsection}{\numberline {12.3}Parallelizzazione del calcolo con una singola matrice $X$}{62}{subsection.12.3}%
\contentsline {subsubsection}{\numberline {12.3.1}Impacchettare la sequenza in una matrice}{63}{subsubsection.12.3.1}%
\contentsline {subsubsection}{\numberline {12.3.2}Self-attention in forma matriciale (una testa)}{63}{subsubsection.12.3.2}%
\contentsline {subsubsection}{\numberline {12.3.3}Mascheramento causale: eliminare il futuro}{64}{subsubsection.12.3.3}%
\contentsline {subsubsection}{\numberline {12.3.4}Schema completo per una testa (in parallelo)}{65}{subsubsection.12.3.4}%
\contentsline {subsubsection}{\numberline {12.3.5}Costo computazionale e dipendenza quadratica}{65}{subsubsection.12.3.5}%
\contentsline {subsubsection}{\numberline {12.3.6}Multi-head attention in parallelo}{65}{subsubsection.12.3.6}%
\contentsline {subsubsection}{\numberline {12.3.7}Il blocco Transformer in forma parallela}{66}{subsubsection.12.3.7}%
\contentsline {subsection}{\numberline {12.4}L'input del Transformer: embeddings di token e di posizione}{67}{subsection.12.4}%
\contentsline {subsubsection}{\numberline {12.4.1}Token embeddings e matrice di embedding}{67}{subsubsection.12.4.1}%
\contentsline {paragraph}{Selezione via one-hot (interpretazione equivalente).}{67}{section*.37}%
\contentsline {paragraph}{Dalla sequenza alla matrice.}{68}{section*.39}%
\contentsline {subsubsection}{\numberline {12.4.2}Perché servono gli embeddings posizionali}{68}{subsubsection.12.4.2}%
\contentsline {subsubsection}{\numberline {12.4.3}Posizione assoluta e composizione dell’input}{69}{subsubsection.12.4.3}%
\contentsline {subsection}{\numberline {12.5}Limiti della posizione assoluta e alternative: sinusoidale e posizione relativa}{69}{subsection.12.5}%
\contentsline {paragraph}{Embeddings posizionali sinusoidali.}{70}{section*.42}%
\contentsline {paragraph}{Posizione relativa.}{70}{section*.43}%
\contentsline {subsection}{\numberline {12.6}La \textit {language modeling head}}{71}{subsection.12.6}%
\contentsline {subsubsection}{\numberline {12.6.1}Cosa entra e cosa esce: dal vettore $h_N^L$ alle probabilità sul vocabolario}{71}{subsubsection.12.6.1}%
\contentsline {subsubsection}{\numberline {12.6.2}Logits e livello di \textit {unembedding}}{71}{subsubsection.12.6.2}%
\contentsline {paragraph}{Weight tying: perché spesso $U=E^\top $.}{72}{section*.45}%
\contentsline {subsubsection}{\numberline {12.6.3}Softmax: da logits a probabilità}{73}{subsubsection.12.6.3}%
\contentsline {subsubsection}{\numberline {12.6.4}Dal modello alla generazione: scegliere il prossimo token}{73}{subsubsection.12.6.4}%
\contentsline {subsubsection}{\numberline {12.6.5}Visione d’insieme: un \textit {decoder-only} che impila blocchi}{73}{subsubsection.12.6.5}%
\contentsline {subsection}{\numberline {12.7}Nota: \textit {logit lens} e terminologia \textit {decoder-only}}{73}{subsection.12.7}%
\contentsline {paragraph}{Nota terminologica: \textit {decoder-only}.}{75}{section*.47}%
\contentsline {section}{\numberline {13}Large Language Models}{75}{section.13}%
\contentsline {subsection}{\numberline {13.1}Large Language Models con Transformer: generazione condizionata}{76}{subsection.13.1}%
\contentsline {paragraph}{Perché “predire parole” è utile per tanti task.}{76}{section*.49}%
\contentsline {paragraph}{Esempio: riassunto come generazione condizionata.}{77}{section*.50}%
\contentsline {paragraph}{Nota sul passo successivo (ponte verso BERT).}{77}{section*.53}%
\contentsline {section}{\numberline {14}Sampling per la generazione con LLM}{79}{section.14}%
\contentsline {subsection}{\numberline {14.1}Perché non basta il campionamento ``puro''}{79}{subsection.14.1}%
\contentsline {subsection}{\numberline {14.2}Top-$k$ sampling}{80}{subsection.14.2}%
\contentsline {subsection}{\numberline {14.3}Top-$p$ (nucleus) sampling}{80}{subsection.14.3}%
\contentsline {subsection}{\numberline {14.4}Temperature sampling}{81}{subsection.14.4}%
\contentsline {section}{\numberline {15}Pretraining dei Large Language Models}{81}{section.15}%
\contentsline {subsection}{\numberline {15.1}Setup, notazione e obiettivo di language modeling}{82}{subsection.15.1}%
\contentsline {subsection}{\numberline {15.2}Self-supervision e funzione obiettivo}{82}{subsection.15.2}%
\contentsline {subsection}{\numberline {15.3}Teacher forcing}{83}{subsection.15.3}%
\contentsline {subsection}{\numberline {15.4}Efficienza computazionale: parallelismo nei transformer}{84}{subsection.15.4}%
\contentsline {subsection}{\numberline {15.5}Dati di pretraining: fonti e filtraggio}{85}{subsection.15.5}%
\contentsline {paragraph}{Filtri di qualità e sicurezza.}{86}{section*.56}%
\contentsline {paragraph}{Aspetti etici e legali (panoramica).}{86}{section*.57}%
\contentsline {subsection}{\numberline {15.6}Dal pretraining all'adattamento: finetuning}{86}{subsection.15.6}%
\contentsline {paragraph}{Tipi di adattamento (senza anticipare modelli specifici).}{86}{section*.59}%
\contentsline {paragraph}{Collegamento alle sezioni successive.}{87}{section*.60}%
\contentsline {section}{\numberline {16}Il problema del disentanglement}{88}{section.16}%
\contentsline {section}{\numberline {17}Introduzione}{88}{section.17}%
\contentsline {section}{\numberline {18}Il fenomeno della Superposizione}{88}{section.18}%
\contentsline {subsection}{\numberline {18.1}Formalismo matematico}{89}{subsection.18.1}%
\contentsline {subsection}{\numberline {18.2}Il ruolo della sparsità}{89}{subsection.18.2}%
\contentsline {section}{\numberline {19}Disentangling Dense Embeddings \\with Sparse Autoencoders}{91}{section.19}%
\contentsline {section}{\numberline {20}Introduzione}{91}{section.20}%
\contentsline {section}{\numberline {21}Ipotesi della superposizione}{91}{section.21}%
\contentsline {section}{\numberline {22}Metodologia e Architettura}{91}{section.22}%
\contentsline {subsection}{\numberline {22.1}Definizione del Modello}{92}{subsection.22.1}%
\contentsline {subsection}{\numberline {22.2}Vincolo di Sparsità \textit {k-Sparse}}{92}{subsection.22.2}%
\contentsline {subsection}{\numberline {22.3}Funzione di Costo e Addestramento}{93}{subsection.22.3}%
\contentsline {section}{\numberline {23}Interpretazione Automatizzata delle Feature}{93}{section.23}%
\contentsline {section}{\numberline {24}Feature Families e Struttura Gerarchica}{94}{section.24}%
\contentsline {section}{\numberline {25}Feature Families e Struttura Gerarchica}{94}{section.25}%
\contentsline {paragraph}{Criterio di identificazione delle feature genitore e figlie}{95}{section*.61}%
\contentsline {subsection}{\numberline {25.1}Costruzione del Grafo di Co-occorrenza}{95}{subsection.25.1}%
\contentsline {subsection}{\numberline {25.2}Identificazione delle Feature Families}{96}{subsection.25.2}%
\contentsline {section}{\numberline {26}Prisma}{98}{section.26}%
\contentsline {section}{\numberline {27}Introduzione}{98}{section.27}%
\contentsline {section}{\numberline {28}Architettura}{98}{section.28}%
\contentsline {section}{\numberline {29}Generazione Embeddings}{98}{section.29}%
\contentsline {subsection}{\numberline {29.1}Gestione di documenti lunghi: strategia \textit {chunk-and-average}}{99}{subsection.29.1}%
\contentsline {paragraph}{Tokenizzazione e vincoli di lunghezza}{99}{section*.63}%
\contentsline {paragraph}{Segmentazione in chunk contigui}{100}{section*.64}%
\contentsline {paragraph}{Embedding per chunk e concetto di pooling}{100}{section*.65}%
\contentsline {paragraph}{Aggregazione a livello documento}{101}{section*.66}%
\contentsline {section}{\numberline {30}Training SAE}{101}{section.30}%
\contentsline {section}{\numberline {31}Interpretazione}{102}{section.31}%
\contentsline {section}{\numberline {32}Esperimenti}{102}{section.32}%
\contentsline {section}{\numberline {33}Pedianet}{102}{section.33}%
\contentsline {subsection}{\numberline {33.1}Scelta del modello di embedding}{103}{subsection.33.1}%
\contentsline {subsection}{\numberline {33.2}Esperimento}{103}{subsection.33.2}%
\contentsline {section}{\numberline {34}Abstracts}{104}{section.34}%
\contentsline {section}{\numberline {35}Qual `e la dimensionalit`a intrinseca della variet`a su cui poggia il linguaggio umano?}{105}{section.35}%
\contentsline {section}{\numberline {36}PubMed}{105}{section.36}%
