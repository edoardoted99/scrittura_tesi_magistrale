\babel@toc {italian}{}\relax 
\babel@toc {italian}{}\relax 
\contentsline {section}{\numberline {1}Introduzione}{1}{section.1}%
\contentsline {section}{\numberline {2}Autoencoders}{2}{section.2}%
\contentsline {subsection}{\numberline {2.1}Definizione e formulazione generale}{3}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Il problema dell'identità e la necessità di vincoli}{3}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}Introduzione dei vincoli: architettura e regolarizzazione}{4}{subsection.2.3}%
\contentsline {subsubsection}{\numberline {2.3.1}Vincoli architetturali: bottleneck e riduzione della dimensionalità}{4}{subsubsection.2.3.1}%
\contentsline {subsubsection}{\numberline {2.3.2}Vincoli nella funzione obiettivo: regolarizzazione dello spazio latente}{8}{subsubsection.2.3.2}%
\contentsline {subsection}{\numberline {2.4}La semantica emerge dai vincoli}{9}{subsection.2.4}%
\contentsline {subsubsection}{\numberline {2.4.1}Il manifold dei dati e le regioni vuote}{9}{subsubsection.2.4.1}%
\contentsline {subsubsection}{\numberline {2.4.2}Analogia fisica: lo spazio delle fasi}{10}{subsubsection.2.4.2}%
\contentsline {subsubsection}{\numberline {2.4.3}Implicazioni per il design di autoencoder}{11}{subsubsection.2.4.3}%
\contentsline {subsection}{\numberline {2.5}Interpretabilità e Disentanglement}{11}{subsection.2.5}%
\contentsline {subsubsection}{\numberline {2.5.1}Fattori di variazione e rappresentazioni entangled}{13}{subsubsection.2.5.1}%
\contentsline {subsubsection}{\numberline {2.5.2}Definizione di Disentanglement}{14}{subsubsection.2.5.2}%
\contentsline {subsubsection}{\numberline {2.5.3}Esempio: InfoGAN su MNIST}{14}{subsubsection.2.5.3}%
\contentsline {subsubsection}{\numberline {2.5.4}Perché il disentanglement non emerge spontaneamente}{16}{subsubsection.2.5.4}%
\contentsline {subsection}{\numberline {2.6}Regolarizzare lo spazio latente: i $\beta $-VAE}{17}{subsection.2.6}%
\contentsline {subsubsection}{\numberline {2.6.1}Dai vincoli architetturali ai vincoli probabilistici}{17}{subsubsection.2.6.1}%
\contentsline {subsubsection}{\numberline {2.6.2}Formulazione matematica}{17}{subsubsection.2.6.2}%
\contentsline {subsubsection}{\numberline {2.6.3}Effetto geometrico del vincolo probabilistico}{18}{subsubsection.2.6.3}%
\contentsline {subsubsection}{\numberline {2.6.4}Il trade-off tra ricostruzione e regolarizzazione}{18}{subsubsection.2.6.4}%
\contentsline {subsubsection}{\numberline {2.6.5}$\beta $-VAE e disentanglement}{20}{subsubsection.2.6.5}%
\contentsline {subsubsection}{\numberline {2.6.6}Limiti dei vincoli probabilistici}{20}{subsubsection.2.6.6}%
\contentsline {subsection}{\numberline {2.7}Sparse Autoencoders}{21}{subsection.2.7}%
\contentsline {subsubsection}{\numberline {2.7.1}Dall'undercomplete all'overcomplete: inversione del paradigma}{21}{subsubsection.2.7.1}%
\contentsline {subsubsection}{\numberline {2.7.2}Formulazione matematica}{22}{subsubsection.2.7.2}%
\contentsline {paragraph}{Regolarizzazione $\ell _1$.}{22}{section*.11}%
\contentsline {paragraph}{Vincolo top-$k$.}{22}{section*.12}%
\contentsline {subsubsection}{\numberline {2.7.3}Interpretazione geometrica e confronto con i vincoli probabilistici}{23}{subsubsection.2.7.3}%
\contentsline {subsubsection}{\numberline {2.7.4}Perché la sparsità favorisce l'interpretabilità}{23}{subsubsection.2.7.4}%
\contentsline {subsubsection}{\numberline {2.7.5}Conclusioni e prospettive}{23}{subsubsection.2.7.5}%
\contentsline {section}{\numberline {3}Word Embeddings}{25}{section.3}%
\contentsline {subsection}{\numberline {3.1}Dalla semantica alla rappresentazione vettoriale}{26}{subsection.3.1}%
\contentsline {subsubsection}{\numberline {3.1.1}Simboli e significati}{26}{subsubsection.3.1.1}%
\contentsline {subsubsection}{\numberline {3.1.2}Gli assi del linguaggio}{26}{subsubsection.3.1.2}%
\contentsline {subsubsection}{\numberline {3.1.3}L'ipotesi distribuzionale}{27}{subsubsection.3.1.3}%
\contentsline {subsubsection}{\numberline {3.1.4}Ipotesi di Osgood: il significato come vettore}{30}{subsubsection.3.1.4}%
\contentsline {subsubsection}{\numberline {3.1.5}Verso i word embeddings}{31}{subsubsection.3.1.5}%
\contentsline {subsection}{\numberline {3.2}Embeddings statici}{33}{subsection.3.2}%
\contentsline {subsubsection}{\numberline {3.2.1}Embeddings count-based}{33}{subsubsection.3.2.1}%
\contentsline {paragraph}{Matrice termine-documento.}{33}{section*.18}%
\contentsline {paragraph}{Matrice termine-termine.}{34}{section*.20}%
\contentsline {paragraph}{Riduzione dimensionale tramite SVD.}{35}{section*.22}%
\contentsline {paragraph}{Cosine similarity.}{36}{section*.23}%
\contentsline {subsubsection}{\numberline {3.2.2}Word2Vec: un approccio predittivo}{36}{subsubsection.3.2.2}%
\contentsline {paragraph}{Il classificatore e la funzione sigmoide.}{37}{section*.25}%
\contentsline {paragraph}{Perché due matrici? Il ruolo di $W$ e $C$.}{38}{section*.26}%
\contentsline {paragraph}{Finestra di contesto e precisione semantica.}{38}{section*.28}%
\contentsline {paragraph}{Geometria dell'analogia: il modello del parallelogramma.}{39}{section*.29}%
\contentsline {subsubsection}{\numberline {3.2.3}Limiti degli embeddings statici}{39}{subsubsection.3.2.3}%
\contentsline {subsection}{\numberline {3.3}Embeddings dinamici (contestuali)}{41}{subsection.3.3}%
\contentsline {subsubsection}{\numberline {3.3.1}RNN: memoria sequenziale}{43}{subsubsection.3.3.1}%
\contentsline {subsubsection}{\numberline {3.3.2}LSTM: controllare il flusso di informazione}{44}{subsubsection.3.3.2}%
\contentsline {subsubsection}{\numberline {3.3.3}Bidirezionalità}{46}{subsubsection.3.3.3}%
\contentsline {subsubsection}{\numberline {3.3.4}Encoder-Decoder: trasformare sequenze}{48}{subsubsection.3.3.4}%
\contentsline {subsubsection}{\numberline {3.3.5}Attenzione: superare il bottleneck}{50}{subsubsection.3.3.5}%
\contentsline {subsubsection}{\numberline {3.3.6}Transformer: attenzione senza ricorrenza}{52}{subsubsection.3.3.6}%
\contentsline {subsubsection}{\numberline {3.3.7}BERT: codifica bidirezionale}{54}{subsubsection.3.3.7}%
\contentsline {paragraph}{Pre-training e fine-tuning.}{56}{section*.38}%
\contentsline {paragraph}{Il problema dell'opacità.}{56}{section*.39}%
\contentsline {section}{\numberline {4}PRISMA: Projection of Representations for Interpretability via Sparse Monosemantic Autoencoders}{58}{section.4}%
\contentsline {subsection}{\numberline {4.1}Fare luce nella black box degli embedding}{59}{subsection.4.1}%
\contentsline {section}{\numberline {5}PRISMA: Projection of Representations for Interpretability Sparse Monosemantic Autoencoders}{62}{section.5}%
\contentsline {subsection}{\numberline {5.1}Fare luce nella black box degli embeddings}{62}{subsection.5.1}%
\contentsline {section}{\numberline {6}Qual `e la dimensionalit`a intrinseca della variet`a su cui poggia il linguaggio umano?}{64}{section.6}%
\contentsline {section}{\numberline {7}PubMed}{64}{section.7}%
