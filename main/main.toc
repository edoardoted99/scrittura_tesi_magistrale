\babel@toc {italian}{}\relax 
\babel@toc {italian}{}\relax 
\contentsline {section}{\numberline {1}Introduzione}{1}{section.1}%
\contentsline {section}{\numberline {2}Autoencoders}{2}{section.2}%
\contentsline {subsection}{\numberline {2.1}Definizione e formulazione generale}{3}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Il problema dell'identità e la necessità di vincoli}{3}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}Introduzione dei vincoli: architettura e regolarizzazione}{4}{subsection.2.3}%
\contentsline {subsubsection}{\numberline {2.3.1}Vincoli architetturali: bottleneck e riduzione della dimensionalità}{4}{subsubsection.2.3.1}%
\contentsline {subsubsection}{\numberline {2.3.2}Vincoli nella funzione obiettivo: regolarizzazione dello spazio latente}{8}{subsubsection.2.3.2}%
\contentsline {subsection}{\numberline {2.4}La semantica emerge dai vincoli}{9}{subsection.2.4}%
\contentsline {subsubsection}{\numberline {2.4.1}Il manifold dei dati e le regioni vuote}{9}{subsubsection.2.4.1}%
\contentsline {subsubsection}{\numberline {2.4.2}Analogia fisica: lo spazio delle fasi}{10}{subsubsection.2.4.2}%
\contentsline {subsubsection}{\numberline {2.4.3}Implicazioni per il design di autoencoder}{11}{subsubsection.2.4.3}%
\contentsline {subsection}{\numberline {2.5}Interpretabilità e Disentanglement}{11}{subsection.2.5}%
\contentsline {subsubsection}{\numberline {2.5.1}Fattori di variazione e rappresentazioni entangled}{13}{subsubsection.2.5.1}%
\contentsline {subsubsection}{\numberline {2.5.2}Definizione di Disentanglement}{14}{subsubsection.2.5.2}%
\contentsline {subsubsection}{\numberline {2.5.3}Esempio: InfoGAN su MNIST}{14}{subsubsection.2.5.3}%
\contentsline {subsubsection}{\numberline {2.5.4}Perché il disentanglement non emerge spontaneamente}{16}{subsubsection.2.5.4}%
\contentsline {subsection}{\numberline {2.6}Regolarizzare lo spazio latente: i $\beta $-VAE}{17}{subsection.2.6}%
\contentsline {subsubsection}{\numberline {2.6.1}Dai vincoli architetturali ai vincoli probabilistici}{17}{subsubsection.2.6.1}%
\contentsline {subsubsection}{\numberline {2.6.2}Formulazione matematica}{17}{subsubsection.2.6.2}%
\contentsline {subsubsection}{\numberline {2.6.3}Effetto geometrico del vincolo probabilistico}{18}{subsubsection.2.6.3}%
\contentsline {subsubsection}{\numberline {2.6.4}Il trade-off tra ricostruzione e regolarizzazione}{18}{subsubsection.2.6.4}%
\contentsline {subsubsection}{\numberline {2.6.5}$\beta $-VAE e disentanglement}{20}{subsubsection.2.6.5}%
\contentsline {subsubsection}{\numberline {2.6.6}Limiti dei vincoli probabilistici}{20}{subsubsection.2.6.6}%
\contentsline {subsection}{\numberline {2.7}Sparse Autoencoders}{21}{subsection.2.7}%
\contentsline {subsubsection}{\numberline {2.7.1}Dall'undercomplete all'overcomplete: inversione del paradigma}{21}{subsubsection.2.7.1}%
\contentsline {subsubsection}{\numberline {2.7.2}Formulazione matematica}{22}{subsubsection.2.7.2}%
\contentsline {paragraph}{Regolarizzazione $\ell _1$.}{22}{section*.11}%
\contentsline {paragraph}{Vincolo top-$k$.}{22}{section*.12}%
\contentsline {subsubsection}{\numberline {2.7.3}Interpretazione geometrica e confronto con i vincoli probabilistici}{23}{subsubsection.2.7.3}%
\contentsline {subsubsection}{\numberline {2.7.4}Perché la sparsità favorisce l'interpretabilità}{23}{subsubsection.2.7.4}%
\contentsline {subsubsection}{\numberline {2.7.5}Conclusioni e prospettive}{23}{subsubsection.2.7.5}%
\contentsline {section}{\numberline {3}Word Embeddings}{25}{section.3}%
\contentsline {subsection}{\numberline {3.1}Dalla semantica alla rappresentazione vettoriale}{26}{subsection.3.1}%
\contentsline {subsubsection}{\numberline {3.1.1}Simboli e significati}{26}{subsubsection.3.1.1}%
\contentsline {subsubsection}{\numberline {3.1.2}Gli assi del linguaggio}{26}{subsubsection.3.1.2}%
\contentsline {subsubsection}{\numberline {3.1.3}L'ipotesi distribuzionale}{27}{subsubsection.3.1.3}%
\contentsline {subsubsection}{\numberline {3.1.4}Ipotesi di Osgood: il significato come vettore}{30}{subsubsection.3.1.4}%
\contentsline {subsubsection}{\numberline {3.1.5}Verso i word embeddings}{31}{subsubsection.3.1.5}%
\contentsline {subsection}{\numberline {3.2}Embeddings statici}{33}{subsection.3.2}%
\contentsline {subsubsection}{\numberline {3.2.1}Embeddings count-based}{33}{subsubsection.3.2.1}%
\contentsline {paragraph}{Matrice termine-documento.}{33}{section*.18}%
\contentsline {paragraph}{Matrice termine-termine.}{34}{section*.20}%
\contentsline {paragraph}{Riduzione dimensionale tramite SVD.}{35}{section*.22}%
\contentsline {paragraph}{Cosine similarity.}{36}{section*.23}%
\contentsline {subsubsection}{\numberline {3.2.2}Word2Vec: un approccio predittivo}{36}{subsubsection.3.2.2}%
\contentsline {paragraph}{Il classificatore e la funzione sigmoide.}{37}{section*.25}%
\contentsline {paragraph}{Perché due matrici? Il ruolo di $W$ e $C$.}{38}{section*.26}%
\contentsline {paragraph}{Finestra di contesto e precisione semantica.}{38}{section*.28}%
\contentsline {paragraph}{Geometria dell'analogia: il modello del parallelogramma.}{39}{section*.29}%
\contentsline {subsubsection}{\numberline {3.2.3}Limiti degli embeddings statici}{39}{subsubsection.3.2.3}%
\contentsline {subsection}{\numberline {3.3}Embeddings dinamici (contestuali)}{41}{subsection.3.3}%
\contentsline {subsubsection}{\numberline {3.3.1}RNN: memoria sequenziale}{43}{subsubsection.3.3.1}%
\contentsline {subsubsection}{\numberline {3.3.2}LSTM: controllare il flusso di informazione}{44}{subsubsection.3.3.2}%
\contentsline {subsubsection}{\numberline {3.3.3}Bidirezionalità}{46}{subsubsection.3.3.3}%
\contentsline {subsubsection}{\numberline {3.3.4}Encoder-Decoder: trasformare sequenze}{48}{subsubsection.3.3.4}%
\contentsline {subsubsection}{\numberline {3.3.5}Attenzione: superare il bottleneck}{50}{subsubsection.3.3.5}%
\contentsline {subsubsection}{\numberline {3.3.6}Transformer: attenzione senza ricorrenza}{52}{subsubsection.3.3.6}%
\contentsline {subsubsection}{\numberline {3.3.7}BERT: codifica bidirezionale}{54}{subsubsection.3.3.7}%
\contentsline {paragraph}{Pre-training e fine-tuning.}{56}{section*.38}%
\contentsline {paragraph}{Il problema dell'opacità.}{56}{section*.39}%
\contentsline {section}{\numberline {4}Il problema del Disentanglement}{58}{section.4}%
\contentsline {subsection}{\numberline {4.1}Il mondo ideale: rappresentazioni disentangled}{59}{subsection.4.1}%
\contentsline {subsubsection}{\numberline {4.1.1}La rete ideale}{60}{subsubsection.4.1.1}%
\contentsline {subsubsection}{\numberline {4.1.2}Due spazi vettoriali}{60}{subsubsection.4.1.2}%
\contentsline {subsubsection}{\numberline {4.1.3}Definizione di disentanglement}{61}{subsubsection.4.1.3}%
\contentsline {subsection}{\numberline {4.2}Il paradosso della capacità}{62}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Neuroni e feature: una distinzione cruciale}{64}{subsection.4.3}%
\contentsline {subsubsection}{\numberline {4.3.1}L'analogia geografica}{64}{subsubsection.4.3.1}%
\contentsline {subsubsection}{\numberline {4.3.2}Direzioni e capacità}{65}{subsubsection.4.3.2}%
\contentsline {subsection}{\numberline {4.4}La Superposition Hypothesis}{66}{subsection.4.4}%
\contentsline {subsubsection}{\numberline {4.4.1}Il caso ideale: direzioni ortogonali}{66}{subsubsection.4.4.1}%
\contentsline {subsubsection}{\numberline {4.4.2}Superposition: più feature che neuroni}{66}{subsubsection.4.4.2}%
\contentsline {subsubsection}{\numberline {4.4.3}Perché la superposition funziona: la sparsità}{67}{subsubsection.4.4.3}%
\contentsline {subsection}{\numberline {4.5}I due problemi della superposition}{68}{subsection.4.5}%
\contentsline {subsubsection}{\numberline {4.5.1}Problema 1: l'opacità}{69}{subsubsection.4.5.1}%
\contentsline {subsubsection}{\numberline {4.5.2}Problema 2: l'interferenza}{70}{subsubsection.4.5.2}%
\contentsline {subsubsection}{\numberline {4.5.3}Un esempio concreto}{70}{subsubsection.4.5.3}%
\contentsline {subsubsection}{\numberline {4.5.4}Riepilogo}{71}{subsubsection.4.5.4}%
\contentsline {subsection}{\numberline {4.6}L'idea del disentanglement}{72}{subsection.4.6}%
\contentsline {subsubsection}{\numberline {4.6.1}Invertire la superposition}{72}{subsubsection.4.6.1}%
\contentsline {subsubsection}{\numberline {4.6.2}Lo spazio overcomplete}{73}{subsubsection.4.6.2}%
\contentsline {subsubsection}{\numberline {4.6.3}La sparsità come vincolo}{73}{subsubsection.4.6.3}%
\contentsline {subsubsection}{\numberline {4.6.4}Il risultato: feature separate}{75}{subsubsection.4.6.4}%
\contentsline {subsection}{\numberline {4.7}Verso una soluzione: PRISMA}{75}{subsection.4.7}%
\contentsline {section}{\numberline {5}PRISMA: Projection of Representations for Interpretability via Sparse Monosemantic Autoencoders}{77}{section.5}%
\contentsline {subsection}{\numberline {5.1}Fare luce nella black box degli embedding}{78}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Architettura dello Sparse Autoencoder}{80}{subsection.5.2}%
\contentsline {subsubsection}{\numberline {5.2.1}Panoramica dell'architettura}{80}{subsubsection.5.2.1}%
\contentsline {subsubsection}{\numberline {5.2.2}Lo spazio latente overcomplete}{83}{subsubsection.5.2.2}%
\contentsline {subsubsection}{\numberline {5.2.3}La linearità del decoder: la chiave del disentanglement}{85}{subsubsection.5.2.3}%
\contentsline {subsubsection}{\numberline {5.2.4}Feature come direzioni nello spazio degli embedding}{87}{subsubsection.5.2.4}%
\contentsline {subsection}{\numberline {5.3}Funzione di perdita e addestramento}{93}{subsection.5.3}%
\contentsline {subsubsection}{\numberline {5.3.1}La funzione di perdita complessiva}{93}{subsubsection.5.3.1}%
\contentsline {subsubsection}{\numberline {5.3.2}Il vincolo di sparsità Top-K}{94}{subsubsection.5.3.2}%
\contentsline {subsubsection}{\numberline {5.3.3}Auxiliary loss e il problema dei dead latents}{97}{subsubsection.5.3.3}%
\contentsline {subsection}{\numberline {5.4}Interpretabilità automatica delle feature}{99}{subsection.5.4}%
\contentsline {subsubsection}{\numberline {5.4.1}L'Interpreter LLM}{99}{subsubsection.5.4.1}%
\contentsline {subsubsection}{\numberline {5.4.2}Implementazione in PRISMA}{101}{subsubsection.5.4.2}%
\contentsline {subsection}{\numberline {5.5}Analisi della struttura delle feature}{102}{subsection.5.5}%
\contentsline {subsubsection}{\numberline {5.5.1}Notazione e matrici fondamentali}{103}{subsubsection.5.5.1}%
\contentsline {paragraph}{Matrice delle attivazioni.}{103}{section*.63}%
\contentsline {paragraph}{Matrice delle attivazioni binaria.}{103}{section*.64}%
\contentsline {paragraph}{Matrice di co-occorrenza.}{103}{section*.66}%
\contentsline {paragraph}{Matrice di similarità delle attivazioni.}{104}{section*.67}%
\contentsline {paragraph}{Similarità geometrica tra SAE.}{104}{section*.68}%
\contentsline {subsubsection}{\numberline {5.5.2}Feature families: struttura gerarchica intra-SAE}{105}{subsubsection.5.5.2}%
\contentsline {subsubsection}{\numberline {5.5.3}Feature splitting: evoluzione inter-SAE}{110}{subsubsection.5.5.3}%
\contentsline {subsubsection}{\numberline {5.5.4}Quantificare la struttura: Effective Rank}{114}{subsubsection.5.5.4}%
\contentsline {section}{\numberline {6}Qual `e la dimensionalit`a intrinseca della variet`a su cui poggia il linguaggio umano?}{118}{section.6}%
\contentsline {section}{\numberline {7}PubMed}{118}{section.7}%
