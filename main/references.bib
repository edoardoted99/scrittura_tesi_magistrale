@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={Nature},
  volume={323},
  pages={533--536},
  year={1986}
}

@incollection{rumelhart1987learning,
  title={Learning internal representations by error propagation},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  booktitle={Parallel Distributed Processing: Explorations in the Microstructure of Cognition},
  volume={1},
  publisher={MIT Press},
  year={1987}
}


@article{autoencodersbank2020autoencoders,

  author       = {Dor Bank and
                  Noam Koenigstein and
                  Raja Giryes},
  title        = {Autoencoders},
  journal      = {CoRR},
  volume       = {abs/2003.05991},
  year         = {2020},
  url          = {https://arxiv.org/abs/2003.05991},
  eprinttype    = {arXiv},
  eprint       = {2003.05991},
  timestamp    = {Tue, 17 Mar 2020 14:18:27 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2003-05991.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{michelucci2022introductionautoencoders,
      title={An Introduction to Autoencoders}, 
      author={Umberto Michelucci},
      year={2022},
      eprint={2201.03898},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2201.03898}, 
}

@misc{wang2024disentangledrepresentationlearning,
      title={Disentangled Representation Learning}, 
      author={Xin Wang and Hong Chen and Si'ao Tang and Zihao Wu and Wenwu Zhu},
      year={2024},
      eprint={2211.11695},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.11695}, 
}

@Book{jm3,
  author =       "Daniel Jurafsky and James H. Martin",
  title =        "Speech and Language Processing: An Introduction to
                 Natural Language Processing, Computational Linguistics,
                 and Speech Recognition with Language Models",
  year =         "2025",
  url = {https://web.stanford.edu/~jurafsky/slp3/},
  note = "Online manuscript released August 24, 2025",
  edition =         "3rd",
  }

  @misc{davies2015wikipedia,
  author       = {Davies, Mark},
  title        = {The Wikipedia Corpus: 4.6 million articles, 1.9 billion words},
  year         = {2015},
  howpublished = {\url{https://www.english-corpora.org/wiki/}},
  note         = {Adapted from Wikipedia}
}


@article{BUONOCORE2023104431,
title = {Localizing in-domain adaptation of transformer-based biomedical language models},
journal = {Journal of Biomedical Informatics},
volume = {144},
pages = {104431},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104431},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423001521},
author = {Tommaso Mario Buonocore and Claudio Crema and Alberto Redolfi and Riccardo Bellazzi and Enea Parimbelli},
keywords = {Natural language processing, Deep learning, Language model, Biomedical text mining, Transformer},
abstract = {In the era of digital healthcare, the huge volumes of textual information generated every day in hospitals constitute an essential but underused asset that could be exploited with task-specific, fine-tuned biomedical language representation models, improving patient care and management. For such specialized domains, previous research has shown that fine-tuning models stemming from broad-coverage checkpoints can largely benefit additional training rounds over large-scale in-domain resources. However, these resources are often unreachable for less-resourced languages like Italian, preventing local medical institutions to employ in-domain adaptation. In order to reduce this gap, our work investigates two accessible approaches to derive biomedical language models in languages other than English, taking Italian as a concrete use-case: one based on neural machine translation of English resources, favoring quantity over quality; the other based on a high-grade, narrow-scoped corpus natively written in Italian, thus preferring quality over quantity. Our study shows that data quantity is a harder constraint than data quality for biomedical adaptation, but the concatenation of high-quality data can improve model performance even when dealing with relatively size-limited corpora. The models published from our investigations have the potential to unlock important research opportunities for Italian hospitals and academia. Finally, the set of lessons learned from the study constitutes valuable insights towards a solution to build biomedical language models that are generalizable to other less-resourced languages and different domain settings.}
}

@book{Goodfellow2016DeepLearning,
  title     = {Deep Learning},
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year      = {2016},
  publisher = {MIT Press},
  address   = {Cambridge, MA},
  url       = {https://www.deeplearningbook.org}
}

@misc{chen2016infoganinterpretablerepresentationlearning,
      title={InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}, 
      author={Xi Chen and Yan Duan and Rein Houthooft and John Schulman and Ilya Sutskever and Pieter Abbeel},
      year={2016},
      eprint={1606.03657},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1606.03657}, 
}

@misc{chen2019isolatingsourcesdisentanglementvariational,
      title={Isolating Sources of Disentanglement in Variational Autoencoders}, 
      author={Ricky T. Q. Chen and Xuechen Li and Roger Grosse and David Duvenaud},
      year={2019},
      eprint={1802.04942},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1802.04942}, 
}
@misc{gordon2020machine,
  author       = {Gordon},
  title        = {Machine Learning Edinburgh: Disentangled Representations},
  howpublished = {\url{https://www.youtube.com/watch?v=itOlzH9FHkI}},
  year         = {2020},
  note         = {Pubblicato dal canale YouTube CodeplaySoftware il 26 febbraio 2020. Evento organizzato presso l'Universit√† di Edimburgo.},
  organization = {YouTube}
}


@misc{mikolov2013efficientestimationwordrepresentations,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1301.3781}, 
}

@misc{feynman_wikipedia,
  author       = {{Wikipedia contributors}},
  title        = {Richard Feynman},
  year         = {2025},
  howpublished = {\url{https://en.wikipedia.org/wiki/Richard_Feynman}},
  note         = {Accessed: 26 January 2026}
}

@misc{burgess2018understandingdisentanglingbetavae,
      title={Understanding disentangling in $\beta$-VAE}, 
      author={Christopher P. Burgess and Irina Higgins and Arka Pal and Loic Matthey and Nick Watters and Guillaume Desjardins and Alexander Lerchner},
      year={2018},
      eprint={1804.03599},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1804.03599}, 
}