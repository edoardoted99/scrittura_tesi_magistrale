@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={Nature},
  volume={323},
  pages={533--536},
  year={1986}
}

@incollection{rumelhart1987learning,
  title={Learning internal representations by error propagation},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  booktitle={Parallel Distributed Processing: Explorations in the Microstructure of Cognition},
  volume={1},
  publisher={MIT Press},
  year={1987}
}


@article{autoencodersbank2020autoencoders,

  author       = {Dor Bank and
                  Noam Koenigstein and
                  Raja Giryes},
  title        = {Autoencoders},
  journal      = {CoRR},
  volume       = {abs/2003.05991},
  year         = {2020},
  url          = {https://arxiv.org/abs/2003.05991},
  eprinttype    = {arXiv},
  eprint       = {2003.05991},
  timestamp    = {Tue, 17 Mar 2020 14:18:27 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2003-05991.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{michelucci2022introductionautoencoders,
      title={An Introduction to Autoencoders}, 
      author={Umberto Michelucci},
      year={2022},
      eprint={2201.03898},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2201.03898}, 
}

@misc{wang2024disentangledrepresentationlearning,
      title={Disentangled Representation Learning}, 
      author={Xin Wang and Hong Chen and Si'ao Tang and Zihao Wu and Wenwu Zhu},
      year={2024},
      eprint={2211.11695},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.11695}, 
}

@Book{jm3,
  author =       "Daniel Jurafsky and James H. Martin",
  title =        "Speech and Language Processing: An Introduction to
                 Natural Language Processing, Computational Linguistics,
                 and Speech Recognition with Language Models",
  year =         "2025",
  url = {https://web.stanford.edu/~jurafsky/slp3/},
  note = "Online manuscript released August 24, 2025",
  edition =         "3rd",
  }

  @misc{davies2015wikipedia,
  author       = {Davies, Mark},
  title        = {The Wikipedia Corpus: 4.6 million articles, 1.9 billion words},
  year         = {2015},
  howpublished = {\url{https://www.english-corpora.org/wiki/}},
  note         = {Adapted from Wikipedia}
}


@article{BUONOCORE2023104431,
title = {Localizing in-domain adaptation of transformer-based biomedical language models},
journal = {Journal of Biomedical Informatics},
volume = {144},
pages = {104431},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104431},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423001521},
author = {Tommaso Mario Buonocore and Claudio Crema and Alberto Redolfi and Riccardo Bellazzi and Enea Parimbelli},
keywords = {Natural language processing, Deep learning, Language model, Biomedical text mining, Transformer},
abstract = {In the era of digital healthcare, the huge volumes of textual information generated every day in hospitals constitute an essential but underused asset that could be exploited with task-specific, fine-tuned biomedical language representation models, improving patient care and management. For such specialized domains, previous research has shown that fine-tuning models stemming from broad-coverage checkpoints can largely benefit additional training rounds over large-scale in-domain resources. However, these resources are often unreachable for less-resourced languages like Italian, preventing local medical institutions to employ in-domain adaptation. In order to reduce this gap, our work investigates two accessible approaches to derive biomedical language models in languages other than English, taking Italian as a concrete use-case: one based on neural machine translation of English resources, favoring quantity over quality; the other based on a high-grade, narrow-scoped corpus natively written in Italian, thus preferring quality over quantity. Our study shows that data quantity is a harder constraint than data quality for biomedical adaptation, but the concatenation of high-quality data can improve model performance even when dealing with relatively size-limited corpora. The models published from our investigations have the potential to unlock important research opportunities for Italian hospitals and academia. Finally, the set of lessons learned from the study constitutes valuable insights towards a solution to build biomedical language models that are generalizable to other less-resourced languages and different domain settings.}
}

@book{Goodfellow2016DeepLearning,
  title     = {Deep Learning},
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year      = {2016},
  publisher = {MIT Press},
  address   = {Cambridge, MA},
  url       = {https://www.deeplearningbook.org}
}

@misc{chen2016infoganinterpretablerepresentationlearning,
      title={InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}, 
      author={Xi Chen and Yan Duan and Rein Houthooft and John Schulman and Ilya Sutskever and Pieter Abbeel},
      year={2016},
      eprint={1606.03657},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1606.03657}, 
}

@misc{chen2019isolatingsourcesdisentanglementvariational,
      title={Isolating Sources of Disentanglement in Variational Autoencoders}, 
      author={Ricky T. Q. Chen and Xuechen Li and Roger Grosse and David Duvenaud},
      year={2019},
      eprint={1802.04942},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1802.04942}, 
}
@misc{gordon2020machine,
  author       = {Gordon},
  title        = {Machine Learning Edinburgh: Disentangled Representations},
  howpublished = {\url{https://www.youtube.com/watch?v=itOlzH9FHkI}},
  year         = {2020},
  note         = {Pubblicato dal canale YouTube CodeplaySoftware il 26 febbraio 2020. Evento organizzato presso l'Università di Edimburgo.},
  organization = {YouTube}
}


@misc{mikolov2013efficientestimationwordrepresentations,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1301.3781}, 
}

@misc{feynman_wikipedia,
  author       = {{Wikipedia contributors}},
  title        = {Richard Feynman},
  year         = {2025},
  howpublished = {\url{https://en.wikipedia.org/wiki/Richard_Feynman}},
  note         = {Accessed: 26 January 2026}
}

@misc{burgess2018understandingdisentanglingbetavae,
      title={Understanding disentangling in $\beta$-VAE}, 
      author={Christopher P. Burgess and Irina Higgins and Arka Pal and Loic Matthey and Nick Watters and Guillaume Desjardins and Alexander Lerchner},
      year={2018},
      eprint={1804.03599},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1804.03599}, 
}

@misc{saussure_wikipedia_image,
  author       = {Jullien},
  title        = {Ferdinand de Saussure},
  year         = {1900},
  howpublished = {\url{https://it.wikipedia.org/wiki/Ferdinand_de_Saussure}},
  note         = {Immagine tratta da Wikimedia Commons}
}


@misc{sutskever2014sequencesequencelearningneural,
      title={Sequence to Sequence Learning with Neural Networks}, 
      author={Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
      year={2014},
      eprint={1409.3215},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1409.3215}, 
}



@book{Osgood1957Measurement,
  author    = {Osgood, Charles E. and Suci, George J. and Tannenbaum, Percy H.},
  title     = {The Measurement of Meaning},
  year      = {1957},
  publisher = {University of Illinois Press},
  address   = {Urbana, IL}
}

@article{hochreiter1997long,
  author    = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  title     = {Long Short-Term Memory},
  journal   = {Neural Computation},
  volume    = {9},
  number    = {8},
  pages     = {1735--1780},
  year      = {1997},
  doi       = {https://doi.org/10.1162/neco.1997.9.8.1735}
}

@misc{bahdanau2014neural,
  author    = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  year      = {2014},
  eprint    = {1409.0473},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url       = {https://arxiv.org/abs/1409.0473}
}

@misc{vaswani2017attention,
  author    = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Łukasz Kaiser and Illia Polosukhin},
  title     = {Attention Is All You Need},
  year      = {2017},
  eprint    = {1706.03762},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url       = {https://arxiv.org/abs/1706.03762}
}

@misc{devlin2019bert,
  author    = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year      = {2019},
  eprint    = {1810.04805},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url       = {https://arxiv.org/abs/1810.04805}
}

@misc{huh2024platonicrepresentationhypothesis,
      title={The Platonic Representation Hypothesis}, 
      author={Minyoung Huh and Brian Cheung and Tongzhou Wang and Phillip Isola},
      year={2024},
      eprint={2405.07987},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.07987}, 
}


@misc{elhage2022toy,
      title={Toy Models of Superposition}, 
      author={Nelson Elhage and Tristan Hume and Catherine Olsson and Nicholas Schiefer and Tom Henighan and Shauna Kravec and Zac Hatfield-Dodds and Robert Lasenby and Dawn Drain and Carol Chen and Roger Grosse and Sam McCandlish and Jared Kaplan and Dario Amodei and Martin Wattenberg and Christopher Olah},
      year={2022},
      eprint={2209.10652},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2209.10652}, 
}


%%%%%%%%%%%%%%%

% ============================================================
% NUOVE ENTRY BIB PER LA SEZIONE 1.2 (Stato dell'arte)
% Da aggiungere al file .bib esistente
% ============================================================

% --- Mechanistic Interpretability ---

@article{olah2017feature,
  title={Feature Visualization},
  author={Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  journal={Distill},
  year={2017},
  doi={10.23915/distill.00007},
  url={https://distill.pub/2017/feature-visualization/}
}

@article{olah2020zoom,
  title={Zoom In: An Introduction to Circuits},
  author={Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal={Distill},
  year={2020},
  doi={10.23915/distill.00024.001},
  url={https://distill.pub/2020/circuits/zoom-in/}
}

@article{elhage2021mathematical,
  title={A Mathematical Framework for Transformer Circuits},
  author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Goldber, Zac and Hallacy, Sam and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  journal={Transformer Circuits Thread},
  year={2021},
  url={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{olsson2022context,
  title={In-context Learning and Induction Heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  journal={Transformer Circuits Thread},
  year={2022},
  url={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}

@misc{nanda2023progress,
  title={Progress measures for grokking via mechanistic interpretability},
  author={Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt},
  year={2023},
  eprint={2301.05217},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2301.05217}
}

% --- Sparse Autoencoders per interpretabilità ---

@misc{bricken2023monosemanticity,
  title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
  author={Trenton Bricken and Adly Templeton and Joshua Batson and Brian Chen and Adam Jermyn and Tom Conerly and Nick Turner and Cem Anil and Carson Denison and Amanda Askell and Robert Lasenby and Yifan Wu and Shauna Kravec and Nicholas Schiefer and Tim Maxwell and Nicholas Joseph and Zac Hatfield-Dodds and Alex Tamkin and Karina Nguyen and Brayden McLean and Josiah E Burke and Tristan Hume and Shan Carter and Tom Henighan and Chris Olah},
  year={2023},
  howpublished={Transformer Circuits Thread},
  url={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
}

@misc{cunningham2023sparse,
  title={Sparse Autoencoders Find Highly Interpretable Features in Language Models},
  author={Hoagy Cunningham and Aidan Ewart and Logan Riggs and Robert Huben and Lee Sharkey},
  year={2023},
  eprint={2309.08600},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2309.08600}
}

@misc{templeton2024scaling,
  title={Scaling Monosemanticity: Extracting Interpretable Features from {C}laude 3 {S}onnet},
  author={Adly Templeton and Tom Conerly and Jonathan Marcus and Jack Clark and Saurav Kadavath and Sam Brown and Dawn Drain and Nick El Haitem and Zac Hatfield-Dodds and Robert Lasenby and Nicholas Joseph and Sam McCandlish and Tom Henighan and Tristan Hume and Shan Carter and Jared Kaplan and Chris Olah},
  year={2024},
  howpublished={Transformer Circuits Thread},
  url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
}

@misc{rajamanoharan2024improving,
  title={Improving Dictionary Learning with Gated Sparse Autoencoders},
  author={Senthooran Rajamanoharan and Arthur Conmy and Lewis Smith and Tom Lieberum and Vikrant Varma and János Kramár and Rohin Shah and Neel Nanda},
  year={2024},
  eprint={2404.16014},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2404.16014}
}

@misc{rajamanoharan2024jumping,
  title={Jumping Ahead: Improving Reconstruction Fidelity with {JumpReLU} Sparse Autoencoders},
  author={Senthooran Rajamanoharan and Tom Lieberum and Nicolas Sonnerat and Arthur Conmy and Vikrant Varma and János Kramár and Neel Nanda},
  year={2024},
  eprint={2407.14435},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2407.14435}
}

@misc{gao2024scaling,
  title={Scaling and evaluating sparse autoencoders},
  author={Leo Gao and Tom Dupré la Tour and Henk Tillman and Gabriel Goh and Rajan Troll and Alec Radford and Ilya Sutskever and Jan Leike and Jeffrey Wu},
  year={2024},
  eprint={2406.04093},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2406.04093}
}


%%%%%%%%%%%%%%STATE OF THE ART%%%%%%%%
% --- Interpretazione automatica ---

@misc{bills2023language,
  title={Language models can explain neurons in language models},
  author={Steven Bills and Nick Cammarata and Dan Mossing and Henk Tillman and Leo Gao and Gabriel Goh and Ilya Sutskever and Jan Leike and Jeff Wu and William Saunders},
  year={2023},
  howpublished={OpenAI Blog},
  url={https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html}
}

% --- Probing e metodi alternativi ---

@misc{alain2017understanding,
  title={Understanding intermediate layers using linear classifier probes},
  author={Guillaume Alain and Yoshua Bengio},
  year={2017},
  eprint={1610.01644},
  archivePrefix={arXiv},
  primaryClass={stat.ML},
  url={https://arxiv.org/abs/1610.01644}
}

@article{belinkov2017neural,
  title={What do Neural Machine Translation Models Learn about Morphology?},
  author={Yonatan Belinkov and Nadir Durrani and Fahim Dalvi and Hassan Sajjad and James Glass},
  journal={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics},
  year={2017},
  pages={861--872},
  url={https://aclanthology.org/P17-1080/}
}

@inproceedings{kim2018interpretability,
  title={Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors ({TCAV})},
  author={Been Kim and Martin Wattenberg and Justin Gilmer and Carrie Cai and James Wexler and Fernanda Viegas and Rory Sayres},
  booktitle={Proceedings of the 35th International Conference on Machine Learning},
  pages={2668--2677},
  year={2018},
  url={https://proceedings.mlr.press/v80/kim18d.html}
}

@inproceedings{clark2019does,
  title={What Does {BERT} Look at? An Analysis of {BERT}'s Attention},
  author={Kevin Clark and Urvashi Khandelwal and Omer Levy and Christopher D. Manning},
  booktitle={Proceedings of the 2019 ACL Workshop BlackboxNLP},
  pages={276--286},
  year={2019},
  url={https://aclanthology.org/W19-4828/}
}

@inproceedings{jain2019attention,
  title={Attention is not Explanation},
  author={Sarthak Jain and Byron C. Wallace},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics},
  pages={3543--3556},
  year={2019},
  url={https://aclanthology.org/N19-1357/}
}

% --- Gemma 3 ---

@misc{gemma3report,
  title={Gemma 3 Technical Report},
  author={{Gemma Team, Google DeepMind}},
  year={2025},
  eprint={2503.19786},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2503.19786}
}


%%%%%%%%%%%%%%FINE STATE%%%%%%%%%%%%%%%%


%%%%%%APPENDICEE%%%

@article{baldi1989neural,
  author    = {Pierre Baldi and Kurt Hornik},
  title     = {Neural Networks and Principal Component Analysis: Learning from Examples Without Local Minima},
  journal   = {Neural Networks},
  volume    = {2},
  number    = {1},
  pages     = {53--58},
  year      = {1989},
  doi       = {10.1016/0893-6080(89)90014-2},
}

@misc{frantar2023gptqaccurateposttrainingquantization,
      title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers}, 
      author={Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
      year={2023},
      eprint={2210.17323},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.17323}, 
}

@misc{lin2024awqactivationawareweightquantization,
      title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration}, 
      author={Ji Lin and Jiaming Tang and Haotian Tang and Shang Yang and Wei-Ming Chen and Wei-Chen Wang and Guangxuan Xiao and Xingyu Dang and Chuang Gan and Song Han},
      year={2024},
      eprint={2306.00978},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.00978}, 
}

