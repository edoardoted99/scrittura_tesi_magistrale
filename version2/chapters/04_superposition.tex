\chapter{Il problema del disentanglement}

\section{Introduzione}

Nel capitolo precedente abbiamo esplorato come i modelli di \textit{embedding} siano in grado di mappare i significati latenti del linguaggio all'interno di spazi vettoriali ad alta dimensionalità. Un fenomeno empirico di particolare rilievo, nonché una proprietà estremamente desiderabile, è la tendenza di tali significati a organizzarsi secondo strutture lineari.
È noto, come abbiamo visto, che le relazioni semantiche possano riflettersi in precise operazioni geometriche: il vettore che congiunge le rappresentazioni di \textit{re} e \textit{regina} approssima lo stesso spostamento semantico che intercorre tra \textit{uomo} e \textit{donna}. Questa regolarità suggerisce che la semantica possa essere descritta attraverso una rappresentazione strutturata, ordinata e, almeno in linea di principio, interpretabile.
Con l'avvento dei grandi modelli di linguaggio (\textit{Large Language Models}), l'estrazione di una struttura leggibile dallo spazio rappresentazionale ha smesso di essere un mero interesse di ricerca teorica per diventare una necessità impellente. Dal momento in cui l'interazione tra esseri umani e modelli generativi è diventata centrale, la capacità di esercitare un controllo puntuale su tali strumenti è diventata prioritaria. Tale controllo presuppone il bisogno fondamentale di interpretare il loro funzionamento interno: il \textit{disentanglement} dei fattori di variazione diventa quindi la chiave per trasformare sistemi opachi in strumenti trasparenti e affidabili.
Dal punto di vista della geometria dello spazio rappresentazionale, l'ideale teorico consisterebbe nell'associare ogni singola dimensione a un concetto semantico univoco. In un'ottica ingegneristica, ciò corrisponderebbe alla presenza di "neuroni" specializzati all'interno della rete neurale, la cui attivazione risulti proporzionale all'intensità del concetto espresso. Tuttavia, nella pratica, tale corrispondenza biunivoca è raramente riscontrabile: le rappresentazioni tendono a essere distribuite e i concetti risultano spesso intrecciati tra loro. La sfida di isolare queste componenti semantiche indipendenti costituisce il cuore del problema del \textit{disentanglement}.


\section{La superposizione e l'ipotesi dei modelli giocattolo}

L'aspirazione verso un modello perfettamente \textit{disentangled} si scontra con una realtà empirica complessa: nei modelli di linguaggio di grandi dimensioni, è raro che un singolo neurone corrisponda a una singola caratteristica semantica (\textit{feature}) chiaramente interpretabile. Sebbene in alcuni casi si osservino neuroni "monosemantici" (che si attivano, ad esempio, solo per riferimenti a basi chimiche o testi legali), la maggior parte dei neuroni appare "polisemantica", attivandosi per una serie di concetti apparentemente non correlati tra loro.

Per indagare questa discrepanza, \cite{elhage2022toy} propongono il concetto di \textbf{superposizione} (\textit{superposition}). L'ipotesi di base è che i modelli neurali tentino di rappresentare un numero di caratteristiche $S$ superiore al numero di dimensioni fisiche $d$ disponibili nello spazio latente ($S > d$). 

\subsection{L'emergere della polisemanticità}

Secondo lo studio dei "modelli giocattolo" (\textit{toy models}) condotto dai ricercatori di Anthropic, la superposizione non è un errore di addestramento, ma una strategia di compressione efficiente basata sulla \textbf{sparsità} delle caratteristiche. Nel linguaggio naturale, la maggior parte dei concetti non è presente simultaneamente in una data frase; questa sparsità consente al modello di "impacchettare" più informazioni in uno spazio ristretto.

Dal punto di vista geometrico, se le \textit{feature} fossero dense, il modello tenderebbe a organizzarle in una base ortogonale (similmente a quanto accade in una \textit{Principal Component Analysis}), limitando il numero di concetti rappresentabili alla dimensionalità dello spazio. Tuttavia, quando le \textit{feature} sono sparse, il modello può permettersi di rinunciare all'ortogonalità, accettando una piccola quota di "interferenza" tra i segnali, che viene successivamente filtrata dalle funzioni di attivazione non lineari (come le ReLU).

\subsection{Diagrammi di fase e strutture geometriche}

Il lavoro di \cite{elhage2022toy} dimostra che il passaggio da neuroni monosemantici a polisemantici non è casuale, ma è regolato da un vero e proprio \textbf{cambiamento di fase}. Tale transizione dipende da due fattori critici:
\begin{itemize}
    \item \textbf{Sparsità:} Più una caratteristica è rara, più è probabile che il modello la collochi in superposizione.
    \item \textbf{Importanza:} Caratteristiche fondamentali per il task tendono a reclamare una dimensione dedicata (diventando monosemantiche), mentre caratteristiche marginali vengono compresse.
\end{itemize}

In questo regime di superposizione, le caratteristiche si organizzano in strutture geometriche sorprendentemente regolari, come poligoni e politopi (triangoli, pentagoni, tetraedri). Queste configurazioni rappresentano il punto di equilibrio ottimale tra la necessità di minimizzare l'interferenza reciproca e quella di massimizzare il numero di concetti memorizzati.

\subsection{Verso una simulazione rumorosa}

L'implicazione più profonda di questa teoria è che le reti neurali che osserviamo nella pratica possano essere interpretate come una "simulazione rumorosa" di reti molto più grandi e sparse. In questa prospettiva, un modello polisemantico non è intrinsecamente privo di struttura, ma sta eseguendo calcoli in uno spazio "virtuale" di dimensioni molto superiori a quelle fisiche del suo strato nascosto. 

Il problema del \textit{disentanglement} diventa quindi la sfida di invertire questa compressione: trovare il modo di mappare i neuroni polisemantici verso queste caratteristiche latenti originali, rendendo nuovamente trasparente ciò che il modello ha dovuto "intrecciare" per ragioni di efficienza computazionale.