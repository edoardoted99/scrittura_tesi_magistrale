

\subsection{Capacit√† del modello e soluzioni banali}

Un rischio fondamentale nell'addestramento degli autoencoder √® che il modello impari semplicemente a copiare l'input nell'output senza estrarre alcuna informazione utile. Se l'architettura della rete ha una capacit√† eccessiva ‚Äî ad esempio, se il numero di neuroni nello spazio latente √® uguale o superiore alla dimensione dell'input ($q \geq n$) ‚Äî il modello pu√≤ facilmente apprendere la \textbf{funzione identit√†}.

In questo scenario, l'errore di ricostruzione sar√† nullo, ma il modello fallir√† nel suo scopo primario: l'apprendimento di una rappresentazione "informativa" o compressa dei dati. Una soluzione identitaria √® considerata una \textbf{soluzione banale} poich√© non richiede che la rete scopra strutture latenti o correlazioni tra le caratteristiche di input; essa si limita a memorizzare e riprodurre i valori originali, rendendo l'autoencoder del tutto inutile per compiti di analisi o estrazione di feature.

\subsection{Rappresentazioni utili vs ricostruzione perfetta}

Per rendere un autoencoder efficace, √® necessario trovare un equilibrio tra la fedelt√† della ricostruzione e l'utilit√† della rappresentazione latente. L'obiettivo non √® necessariamente ottenere una ricostruzione perfetta, ma forzare il modello a estrarre le \textbf{caratteristiche essenziali} del dataset.

Prendendo come esempio il dataset MNIST, una rappresentazione utile non consiste nell'imparare i singoli valori di grigio di ogni pixel, ma nel catturare concetti astratti come il numero di linee richieste, l'angolo delle stesse e il modo in cui esse si connettono per formare una cifra. L'estrazione di queste "informazioni essenziali" permette di comprendere le caratteristiche fondamentali di un dataset e pu√≤ essere utilizzata per compiti successivi come la classificazione o il clustering.

Per prevenire l'apprendimento della funzione identit√† e promuovere queste rappresentazioni significative, si utilizzano principalmente due strategie:
\begin{itemize}
    \item \textbf{Creazione di un bottleneck}: si impone una dimensione dello spazio latente (l'output dell'encoder) molto inferiore a quella dell'input ($q < n$), costringendo la rete a una riduzione della dimensionalit√†.
    \item \textbf{Regolarizzazione}: si aggiungono vincoli alla funzione di perdita (come la regolarizzazione $l_1$ o $l_2$) per indurre propriet√† specifiche nelle attivazioni o nei parametri, favorendo ad esempio la \textbf{sparsit√†} della rappresentazione latente.
\end{itemize}


\section{Bottleneck e riduzione della dimensionalit√†}

Una delle strategie pi√π diffuse per prevenire l‚Äôapprendimento della funzione
identit√† negli autoencoders consiste nell‚Äôimporre una \textit{strozzatura
dimensionale} (\textit{bottleneck}) nello spazio latente. Questo approccio
forza il modello a comprimere l‚Äôinformazione contenuta nei dati di input,
favorendo l‚Äôapprendimento di rappresentazioni compatte e informative.

\subsection{Architetture con strozzatura}

Un‚Äôarchitettura con bottleneck √® caratterizzata da uno spazio latente di
dimensione $q$ strettamente inferiore alla dimensione dell‚Äôinput $n$
($q < n$). In tali condizioni, l‚Äôencoder realizza una mappatura che riduce la
dimensionalit√† dei dati:
\begin{equation}
    g : \mathbb{R}^n \rightarrow \mathbb{R}^q, \quad q < n,
\end{equation}
costringendo il modello a selezionare e preservare soltanto le informazioni
necessarie alla ricostruzione dell‚Äôinput.

Dal punto di vista architetturale, questa strategia √® comunemente
implementata tramite reti neurali feed-forward simmetriche, in cui il numero
di neuroni decresce progressivamente fino a raggiungere un minimo nello
strato centrale, per poi aumentare nuovamente nella fase di decoding. Lo
strato centrale rappresenta lo spazio latente e costituisce il collo di
bottiglia dell‚Äôautoencoder. Come osservato in
\parencite{michelucci2022introductionautoencoders}, la presenza del bottleneck
impedisce al modello di apprendere una mappatura identitaria, poich√© la
ricostruzione deve avvenire a partire da una rappresentazione compressa.

Se l‚Äôaddestramento ha successo, il decoder √® in grado di ricostruire
l‚Äôinput utilizzando un numero di feature latenti significativamente inferiore
a quello delle variabili originali, dimostrando che l‚Äôinformazione rilevante
√® contenuta in una sottostruttura a bassa dimensionalit√†.

\subsection{Relazione con la PCA}

L‚Äôuso di autoencoders con bottleneck √® strettamente legato al problema della
riduzione della dimensionalit√†. In particolare, esiste una relazione formale
tra autoencoders e \textit{Principal Component Analysis} (PCA), uno dei metodi
classici per la compressione lineare dei dati.

Come discusso in \parencite{michelucci2022introductionautoencoders}, un
autoencoder lineare ‚Äî ovvero caratterizzato da funzioni di encoder e decoder
lineari e addestrato minimizzando l‚Äôerrore quadratico medio ‚Äî √® equivalente a
una PCA, a condizione che:
\begin{itemize}
    \item l‚Äôencoder e il decoder siano trasformazioni lineari;
    \item la funzione di perdita sia la Mean Squared Error;
    \item i dati di input siano opportunamente normalizzati.
\end{itemize}

In questo caso, lo spazio latente appreso dall‚Äôautoencoder coincide con lo
spazio generato dalle prime $q$ componenti principali del dataset, e la
ricostruzione dell‚Äôinput corrisponde alla sua proiezione sul sottospazio
principale. Gli autoencoders possono quindi essere interpretati come una
generalizzazione non lineare della PCA, in grado di apprendere trasformazioni
pi√π flessibili e adattive quando vengono utilizzate funzioni di attivazione
non lineari.


\subsubsection*{Formulazione matematica e dimostrazione}

Si consideri un autoencoder lineare, in cui encoder e decoder sono definiti
come trasformazioni lineari:
\begin{equation}
    \mathbf{h}_i = \mathbf{W}_e \mathbf{x}_i, \qquad
    \hat{\mathbf{x}}_i = \mathbf{W}_d \mathbf{h}_i = \mathbf{W}_d \mathbf{W}_e \mathbf{x}_i,
\end{equation}
dove $\mathbf{W}_e \in \mathbb{R}^{q \times n}$ e $\mathbf{W}_d \in \mathbb{R}^{n \times q}$,
con $q < n$. L‚Äôautoencoder √® addestrato minimizzando l‚Äôerrore quadratico medio:
\begin{equation}
    \mathcal{L} = \frac{1}{M} \sum_{i=1}^{M}
    \left\| \mathbf{x}_i - \mathbf{W}_d \mathbf{W}_e \mathbf{x}_i \right\|^2.
\end{equation}

Assumendo che i dati siano centrati e normalizzati, il problema di
ottimizzazione equivale alla ricerca della migliore approssimazione di rango
$q$ della matrice dei dati. √à noto che la soluzione ottimale di tale problema,
nel senso dei minimi quadrati, √® ottenuta proiettando i dati sul sottospazio
generato dagli autovettori associati ai primi $q$ autovalori della matrice di
covarianza, ovvero sulle prime $q$ componenti principali.

In questo contesto, le righe della matrice $\mathbf{W}_e$ coincidono con le
direzioni principali individuate dalla PCA, mentre l‚Äôoperazione di decoding
realizza la proiezione inversa nello spazio originale. Di conseguenza, lo
spazio latente appreso dall‚Äôautoencoder lineare coincide con il sottospazio
principale dei dati, e la ricostruzione dell‚Äôinput corrisponde alla sua
proiezione ortogonale su tale sottospazio
\parencite{michelucci2022introductionautoencoders}.

Questa equivalenza mostra come la PCA possa essere interpretata come un caso
particolare di autoencoder privo di non linearit√†. L‚Äôintroduzione di funzioni
di attivazione non lineari consente agli autoencoders di superare i limiti
della PCA, permettendo l‚Äôapprendimento di rappresentazioni latenti non
lineari che catturano strutture pi√π complesse nei dati. Tuttavia, come verr√†
discusso nelle sezioni successive, tale flessibilit√† non garantisce di per s√©
l‚Äôinterpretabilit√† delle variabili latenti.


\subsection{Limiti del bottleneck}

Nonostante la sua efficacia, l‚Äôapproccio basato esclusivamente sul bottleneck
presenta alcune limitazioni. In primo luogo, una strozzatura eccessiva pu√≤
portare a una perdita significativa di informazione, compromettendo la
qualit√† della ricostruzione. Esiste quindi un compromesso critico tra grado
di compressione e fedelt√† dell‚Äôoutput, che dipende fortemente dalla scelta
della dimensione $q$.

Inoltre, il bottleneck non garantisce che le variabili latenti apprese siano
interpretabili o semanticamente disentangled. Anche in presenza di una
riduzione dimensionale efficace, le feature latenti possono risultare dense e
fortemente entangled, rendendo difficile attribuire un significato chiaro a
ciascuna componente dello spazio latente.

Questi limiti motivano l‚Äôintroduzione di ulteriori vincoli, come la
regolarizzazione delle attivazioni o l‚Äôimposizione di sparsit√† nello spazio
latente, che verranno discussi nelle sezioni successive come strumenti per
migliorare l‚Äôinterpretabilit√† delle rappresentazioni apprese.

\section{Limiti degli autoencoders classici}
\subsection{Rappresentazioni dense ed entangled}
\subsection{Assenza di interpretabilit√† delle variabili latenti}

\section{Sparsit√† nelle rappresentazioni latenti}
\subsection{Sparsit√† come principio induttivo}
\subsection{Sparse coding e rappresentazioni sovracomplete}

\section{Sparse Autoencoders}
\subsection{Definizione e caratteristiche principali}
\subsection{Funzione obiettivo di un Sparse Autoencoder}
\subsection{Metodi per imporre la sparsit√†}
\subsection{Spazi latenti sovracompleti}

\section{Sparse Autoencoders e interpretabilit√†}
\subsection{Disentanglement delle feature}
\subsection{Rappresentazioni monosemantiche}




1) Contextual Embeddings: definizione e motivazione

üìç Capitolo 11 ‚Äì Masked Language Models

Sezione 11.3 ‚Äì Contextual Embeddings
Qui trovi:

definizione formale di contextual embeddings

distinzione word types vs word instances

relazione con polisemia e disambiguazione del senso

esempi geometrici (cluster per senso)


nlp_stanford

üëâ √à il punto ideale per agganciarti direttamente alla frase finale del tuo capitolo.

2) Language Models come sorgente degli embeddings contestuali

üìç Capitolo 9 ‚Äì The Transformer
üìç Capitolo 10 ‚Äì Large Language Models

Cap. 9 (inizio): language modeling neurale e stati contestuali

Cap. 10: paradigma pretrain ‚Üí finetune e transfer learning


nlp_stanford

üëâ Qui capisci perch√© gli embeddings contestuali non sono parametri, ma output di un LM.

3) RNN / LSTM come primi embeddings contestuali

üìç Capitolo 8 ‚Äì Recurrent Neural Networks

Hidden state come rappresentazione dipendente dal contesto

Limiti strutturali (compressione, dipendenze lunghe)

(Questa parte √® implicitamente richiamata nel Cap. 11 come ‚Äúpre-transformer‚Äù)

üëâ Serve come ponte concettuale, non come focus tecnico.

4) Necessit√† del contesto bidirezionale

üìç Capitolo 11 ‚Äì Introduzione

Motivazione della bidirezionalit√†

Limiti dei causal language models per task interpretativi


nlp_stanford

üëâ Qui il libro √® molto chiaro: comprensione ‚â† generazione.

5) ELMo come modello di transizione

üìç Capitolo 11 ‚Äì esempi applicativi

Uso di ELMo embeddings in WSD e analisi semantica

Confronto implicito con BERT


nlp_stanford

üëâ Studialo come tappa storica, non come architettura da dettagliare.

6) Attention e Self-Attention

üìç Capitolo 9 ‚Äì The Transformer

Sezione 9.1 ‚Äì Attention

dot-product attention

query / key / value


nlp_stanford

üëâ √à il prerequisito matematico minimo per capire il Transformer.

7) Transformer Encoder

üìç Capitolo 9 ‚Äì The Transformer

Sezioni 9.1‚Äì9.2

multi-head self-attention

feed-forward per token

residual + layer norm

positional encoding


nlp_stanford

üëâ Studia il blocco encoder: BERT √® encoder-only.

8) BERT e Masked Language Modeling (punto di arrivo)

üìç Capitolo 11 ‚Äì Masked Language Models

Sezione 11.1 ‚Äì Bidirectional Transformer Encoders

differenza encoder-only vs decoder-only

Training con Masked Language Modeling

Next Sentence Prediction (storico)


nlp_stanford

üëâ Questa √® la reference centrale per BERT.

9) Uso degli embeddings BERT

üìç Capitolo 11 + Capitoli applicativi successivi

Token embeddings per NER, POS, WSD

[CLS] per classificazione


nlp_stanford

üëâ Serve per chiudere il capitolo in modo applicativo.