\babel@toc {italian}{}\relax 
\babel@toc {italian}{}\relax 
\contentsline {chapter}{\numberline {1}Introduzione}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Introduzione}{1}{section.1.1}%
\contentsline {chapter}{\numberline {2}Autoencoders}{3}{chapter.2}%
\contentsline {section}{\numberline {2.1}Introduzione}{3}{section.2.1}%
\contentsline {section}{\numberline {2.2}Autoencoders: definizione e formulazione generale}{3}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Apprendimento non supervisionato}{3}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Encoder, decoder e spazio latente}{4}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Funzione obiettivo e errore di ricostruzione}{5}{subsection.2.2.3}%
\contentsline {section}{\numberline {2.3}Il problema dell’identità e la necessità di vincoli}{5}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Bottleneck e riduzione della dimensionalità}{6}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Introduzione di vincoli}{6}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Relazioni con la PCA}{8}{subsection.2.3.3}%
\contentsline {section}{\numberline {2.4}Interpretabilità delle feature latenti}{12}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Rappresentazioni latenti disentangled}{13}{subsection.2.4.1}%
\contentsline {section}{\numberline {2.5}Sparse Autoencoders}{14}{section.2.5}%
\contentsline {chapter}{\numberline {3}Embeddings}{17}{chapter.3}%
\contentsline {section}{\numberline {3.1}Introduzione}{18}{section.3.1}%
\contentsline {section}{\numberline {3.2}L'ipotesi distribuzionale}{18}{section.3.2}%
\contentsline {section}{\numberline {3.3}Ipotesi di Osgood}{19}{section.3.3}%
\contentsline {section}{\numberline {3.4}Embeddings}{20}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Simple count-based embeddings}{20}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Riduzione dimensionale tramite SVD}{22}{subsection.3.4.2}%
\contentsline {subsection}{\numberline {3.4.3}Cosine Similarity}{23}{subsection.3.4.3}%
\contentsline {subsection}{\numberline {3.4.4}Word2Vec}{24}{subsection.3.4.4}%
\contentsline {subsubsection}{Il classificatore}{25}{section*.9}%
\contentsline {subsubsection}{Esempio delle matrici $W$ e $C$}{28}{section*.15}%
\contentsline {subsubsection}{Algoritmo di apprendimento}{29}{section*.16}%
\contentsline {subsection}{\numberline {3.4.5}Proprietà semantiche degli embeddings}{31}{subsection.3.4.5}%
\contentsline {subsubsection}{1. Influenza della finestra di contesto}{31}{section*.17}%
\contentsline {subsubsection}{2. First-order vs. second-order similarity}{31}{section*.18}%
\contentsline {subsubsection}{3. Analogical reasoning (modello del parallelogramma)}{31}{section*.19}%
\contentsline {subsubsection}{4. Struttura geometrica: ortogonalità e parallelismo}{32}{section*.20}%
\contentsline {subsubsection}{5. Effetti pratici}{32}{section*.21}%
\contentsline {section}{\numberline {3.5}Contextual Embeddings}{33}{section.3.5}%
\contentsline {section}{\numberline {3.6}Modelli di linguaggio neurali}{34}{section.3.6}%
\contentsline {section}{\numberline {3.7}Reti neurali ricorrenti e LSTM}{36}{section.3.7}%
\contentsline {section}{\numberline {3.8}ELMo e il contesto bidirezionale}{38}{section.3.8}%
\contentsline {section}{\numberline {3.9}Attention e Self-Attention}{39}{section.3.9}%
\contentsline {section}{\numberline {3.10}BERT e Masked Language Modeling}{40}{section.3.10}%
\contentsline {section}{\numberline {3.11}Motivazione per il disentanglement degli embeddings di BERT}{42}{section.3.11}%
\contentsline {chapter}{\numberline {4}Disentangling Dense Embeddings \\with Sparse Autoencoders}{45}{chapter.4}%
\contentsline {section}{\numberline {4.1}Motivazione e contesto}{45}{section.4.1}%
\contentsline {section}{\numberline {4.2}Metodologia e Architettura}{46}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Definizione del Modello}{46}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Vincolo di Sparsità \textit {k-Sparse}}{47}{subsection.4.2.2}%
\contentsline {subsection}{\numberline {4.2.3}Funzione di Costo e Addestramento}{47}{subsection.4.2.3}%
\contentsline {section}{\numberline {4.3}Interpretazione Automatizzata delle Feature}{48}{section.4.3}%
\contentsline {section}{\numberline {4.4}Feature Families e Struttura Gerarchica}{48}{section.4.4}%
\contentsline {section}{\numberline {4.5}Feature Families e Struttura Gerarchica}{49}{section.4.5}%
\contentsline {paragraph}{Criterio di identificazione delle feature genitore e figlie}{49}{section*.23}%
\contentsline {subsection}{\numberline {4.5.1}Costruzione del Grafo di Co-occorrenza}{50}{subsection.4.5.1}%
\contentsline {subsection}{\numberline {4.5.2}Identificazione delle Feature Families}{51}{subsection.4.5.2}%
