\babel@toc {italian}{}\relax 
\babel@toc {italian}{}\relax 
\contentsline {chapter}{\numberline {1}Introduzione}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Introduzione}{1}{section.1.1}%
\contentsline {chapter}{\numberline {2}Autoencoders}{3}{chapter.2}%
\contentsline {section}{\numberline {2.1}Introduzione}{3}{section.2.1}%
\contentsline {section}{\numberline {2.2}Autoencoders: definizione e formulazione generale}{3}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Apprendimento non supervisionato}{3}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Encoder, decoder e spazio latente}{4}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Funzione obiettivo e errore di ricostruzione}{5}{subsection.2.2.3}%
\contentsline {section}{\numberline {2.3}Il problema dell’identità e la necessità di vincoli}{5}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Bottleneck e riduzione della dimensionalità}{6}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Introduzione di vincoli}{6}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Relazioni con la PCA}{8}{subsection.2.3.3}%
\contentsline {section}{\numberline {2.4}Interpretabilità delle feature latenti}{12}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Rappresentazioni latenti disentangled}{13}{subsection.2.4.1}%
\contentsline {section}{\numberline {2.5}Sparse Autoencoders}{14}{section.2.5}%
\contentsline {chapter}{\numberline {3}Embeddings}{17}{chapter.3}%
\contentsline {section}{\numberline {3.1}Introduzione}{18}{section.3.1}%
\contentsline {section}{\numberline {3.2}L'ipotesi distribuzionale}{18}{section.3.2}%
\contentsline {section}{\numberline {3.3}Ipotesi di Osgood}{19}{section.3.3}%
\contentsline {section}{\numberline {3.4}Embeddings}{20}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Embeddings count-based}{21}{subsection.3.4.1}%
\contentsline {subsubsection}{Matrice termine-documento}{21}{section*.8}%
\contentsline {subsubsection}{Matrice termine-termine}{22}{section*.10}%
\contentsline {subsection}{\numberline {3.4.2}Riduzione dimensionale tramite SVD}{24}{subsection.3.4.2}%
\contentsline {subsection}{\numberline {3.4.3}Cosine Similarity}{25}{subsection.3.4.3}%
\contentsline {subsection}{\numberline {3.4.4}Word2Vec: un approccio predittivo}{26}{subsection.3.4.4}%
\contentsline {subsubsection}{Il classificatore e la funzione sigmoide}{26}{section*.12}%
\contentsline {subsubsection}{Apprendimento e Negative Sampling}{27}{section*.13}%
\contentsline {subsubsection}{Perché due matrici? Il ruolo di $W$ e $C$}{27}{section*.14}%
\contentsline {subsection}{\numberline {3.4.5}Proprietà semantiche degli embeddings}{28}{subsection.3.4.5}%
\contentsline {section}{\numberline {3.5}Embeddings dinamici}{30}{section.3.5}%
\contentsline {section}{\numberline {3.6}Reti Neurali Ricorrenti}{31}{section.3.6}%
\contentsline {subsection}{\numberline {3.6.1}RNN come Language Models}{33}{subsection.3.6.1}%
\contentsline {subsection}{\numberline {3.6.2}Generaizone di Embeddings tramite RNN}{34}{subsection.3.6.2}%
\contentsline {subsection}{\numberline {3.6.3}RNN Bidirezionali (Bi-RNN)}{35}{subsection.3.6.3}%
\contentsline {subsection}{\numberline {3.6.4}Il problema del Gradiente Svanente}{35}{subsection.3.6.4}%
\contentsline {section}{\numberline {3.7}LSTM: Long Short-Term Memory}{36}{section.3.7}%
\contentsline {subsection}{\numberline {3.7.1}Meccanismi di Gating}{37}{subsection.3.7.1}%
\contentsline {subsection}{\numberline {3.7.2}Le equazioni del modello}{37}{subsection.3.7.2}%
\contentsline {subsection}{\numberline {3.7.3}Modularità ed Embeddings}{38}{subsection.3.7.3}%
\contentsline {section}{\numberline {3.8}Architettura Encoder-Decoder e limite del \textit {bottleneck}}{38}{section.3.8}%
\contentsline {subsection}{\numberline {3.8.1}Il problema \textit {sequence-to-sequence}}{39}{subsection.3.8.1}%
\contentsline {subsection}{\numberline {3.8.2}Il limite del \textit {bottleneck} informativo}{40}{subsection.3.8.2}%
\contentsline {subsection}{\numberline {3.8.3}Soluzione al bottleneck: Meccanismo dell'attenzione}{42}{subsection.3.8.3}%
\contentsline {subsection}{\numberline {3.8.4}Verso i Transformer}{44}{subsection.3.8.4}%
\contentsline {section}{\numberline {3.9}Il Transformer}{45}{section.3.9}%
\contentsline {subsection}{\numberline {3.9.1}Self-attention}{48}{subsection.3.9.1}%
\contentsline {subsubsection}{Motivazione: dalle rappresentazioni statiche alle rappresentazioni contestuali}{48}{section*.27}%
\contentsline {subsubsection}{Self-attention causale: dominio informativo e vincolo di autoregressione}{49}{section*.28}%
\contentsline {subsubsection}{Intuizione: self-attention come combinazione pesata del contesto}{49}{section*.29}%
\contentsline {subsubsection}{Scaled dot-product attention: ruoli di query, key e value}{50}{section*.30}%
\contentsline {paragraph}{Punteggi di compatibilità (scores).}{50}{section*.31}%
\contentsline {paragraph}{Normalizzazione tramite softmax e vincolo causale.}{51}{section*.32}%
\contentsline {paragraph}{Aggregazione dei value e proiezione in output.}{51}{section*.33}%
\contentsline {subsubsection}{Forma matriciale e dimensioni (utile per l’implementazione)}{51}{section*.35}%
\contentsline {subsubsection}{Multi-head attention: pluralità di criteri di selezione}{53}{section*.36}%
\contentsline {subsubsection}{Osservazione conclusiva: self-attention e contestualizzazione progressiva}{53}{section*.38}%
\contentsline {subsection}{\numberline {3.9.2}Blocco Transformer}{54}{subsection.3.9.2}%
\contentsline {subsubsection}{Input del blocco e informazione di posizione}{56}{section*.40}%
\contentsline {subsubsection}{Residual stream: il flusso informativo del token}{56}{section*.41}%
\contentsline {subsubsection}{Perché la LayerNorm (motivazione)}{56}{section*.42}%
\contentsline {subsubsection}{Feedforward network (ruolo)}{57}{section*.43}%
\contentsline {subsubsection}{Equazioni del blocco (variante \emph {prenorm})}{57}{section*.44}%
\contentsline {subsubsection}{L’attenzione come “movimento” di informazione tra stream}{58}{section*.45}%
\contentsline {subsection}{\numberline {3.9.3}Parallelizzazione del calcolo con una singola matrice $X$}{58}{subsection.3.9.3}%
\contentsline {subsubsection}{Impacchettare la sequenza in una matrice}{59}{section*.47}%
\contentsline {subsubsection}{Self-attention in forma matriciale (una testa)}{59}{section*.48}%
\contentsline {subsubsection}{Mascheramento causale: eliminare il futuro}{59}{section*.50}%
\contentsline {subsubsection}{Schema completo per una testa (in parallelo)}{60}{section*.52}%
\contentsline {subsubsection}{Costo computazionale e dipendenza quadratica}{61}{section*.54}%
\contentsline {subsubsection}{Multi-head attention in parallelo}{61}{section*.55}%
\contentsline {subsubsection}{Il blocco Transformer in forma parallela}{62}{section*.56}%
\contentsline {subsection}{\numberline {3.9.4}L'input del Transformer: embeddings di token e di posizione}{62}{subsection.3.9.4}%
\contentsline {subsubsection}{Token embeddings e matrice di embedding}{63}{section*.57}%
\contentsline {paragraph}{Selezione via one-hot (interpretazione equivalente).}{63}{section*.58}%
\contentsline {paragraph}{Dalla sequenza alla matrice.}{63}{section*.60}%
\contentsline {subsubsection}{Perché servono gli embeddings posizionali}{64}{section*.62}%
\contentsline {subsubsection}{Posizione assoluta e composizione dell’input}{64}{section*.63}%
\contentsline {subsection}{\numberline {3.9.5}Limiti della posizione assoluta e alternative: sinusoidale e posizione relativa}{65}{subsection.3.9.5}%
\contentsline {paragraph}{Embeddings posizionali sinusoidali.}{65}{section*.65}%
\contentsline {paragraph}{Posizione relativa.}{66}{section*.66}%
\contentsline {subsection}{\numberline {3.9.6}La \textit {language modeling head}}{66}{subsection.3.9.6}%
\contentsline {subsubsection}{Cosa entra e cosa esce: dal vettore $h_N^L$ alle probabilità sul vocabolario}{67}{section*.67}%
\contentsline {subsubsection}{Logits e livello di \textit {unembedding}}{67}{section*.69}%
\contentsline {paragraph}{Weight tying: perché spesso $U=E^\top $.}{68}{section*.70}%
\contentsline {subsubsection}{Softmax: da logits a probabilità}{68}{section*.71}%
\contentsline {subsubsection}{Dal modello alla generazione: scegliere il prossimo token}{69}{section*.72}%
\contentsline {subsubsection}{Visione d’insieme: un \textit {decoder-only} che impila blocchi}{69}{section*.73}%
\contentsline {subsection}{\numberline {3.9.7}Nota: \textit {logit lens} e terminologia \textit {decoder-only}}{69}{subsection.3.9.7}%
\contentsline {paragraph}{Nota terminologica: \textit {decoder-only}.}{69}{section*.75}%
\contentsline {section}{\numberline {3.10}Large Language Models}{71}{section.3.10}%
\contentsline {subsection}{\numberline {3.10.1}Large Language Models con Transformer: generazione condizionata}{71}{subsection.3.10.1}%
\contentsline {paragraph}{Perché “predire parole” è utile per tanti task.}{72}{section*.77}%
\contentsline {paragraph}{Esempio: riassunto come generazione condizionata.}{73}{section*.78}%
\contentsline {paragraph}{Nota sul passo successivo (ponte verso BERT).}{73}{section*.81}%
\contentsline {section}{\numberline {3.11}Sampling per la generazione con LLM}{74}{section.3.11}%
\contentsline {subsection}{\numberline {3.11.1}Perché non basta il campionamento ``puro''}{75}{subsection.3.11.1}%
\contentsline {subsection}{\numberline {3.11.2}Top-$k$ sampling}{75}{subsection.3.11.2}%
\contentsline {subsection}{\numberline {3.11.3}Top-$p$ (nucleus) sampling}{76}{subsection.3.11.3}%
\contentsline {subsection}{\numberline {3.11.4}Temperature sampling}{76}{subsection.3.11.4}%
\contentsline {section}{\numberline {3.12}Pretraining dei Large Language Models}{77}{section.3.12}%
\contentsline {subsection}{\numberline {3.12.1}Setup, notazione e obiettivo di language modeling}{77}{subsection.3.12.1}%
\contentsline {subsection}{\numberline {3.12.2}Self-supervision e funzione obiettivo}{78}{subsection.3.12.2}%
\contentsline {subsection}{\numberline {3.12.3}Teacher forcing}{79}{subsection.3.12.3}%
\contentsline {subsection}{\numberline {3.12.4}Efficienza computazionale: parallelismo nei transformer}{80}{subsection.3.12.4}%
\contentsline {subsection}{\numberline {3.12.5}Dati di pretraining: fonti e filtraggio}{80}{subsection.3.12.5}%
\contentsline {paragraph}{Filtri di qualità e sicurezza.}{81}{section*.84}%
\contentsline {paragraph}{Aspetti etici e legali (panoramica).}{81}{section*.85}%
\contentsline {subsection}{\numberline {3.12.6}Dal pretraining all'adattamento: finetuning}{82}{subsection.3.12.6}%
\contentsline {paragraph}{Tipi di adattamento (senza anticipare modelli specifici).}{82}{section*.87}%
\contentsline {paragraph}{Collegamento alle sezioni successive.}{83}{section*.88}%
\contentsline {chapter}{\numberline {4}Il problema del disentanglement}{85}{chapter.4}%
\contentsline {section}{\numberline {4.1}Introduzione}{85}{section.4.1}%
\contentsline {section}{\numberline {4.2}La superposizione e l'ipotesi dei modelli giocattolo}{86}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}L'emergere della polisemanticità}{86}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Diagrammi di fase e strutture geometriche}{87}{subsection.4.2.2}%
\contentsline {subsection}{\numberline {4.2.3}Verso una simulazione rumorosa}{87}{subsection.4.2.3}%
\contentsline {chapter}{\numberline {5}Disentangling Dense Embeddings \\with Sparse Autoencoders}{89}{chapter.5}%
\contentsline {section}{\numberline {5.1}Introduzione}{89}{section.5.1}%
\contentsline {section}{\numberline {5.2}Ipotesi della superposizione}{90}{section.5.2}%
\contentsline {section}{\numberline {5.3}Metodologia e Architettura}{90}{section.5.3}%
\contentsline {subsection}{\numberline {5.3.1}Definizione del Modello}{90}{subsection.5.3.1}%
\contentsline {subsection}{\numberline {5.3.2}Vincolo di Sparsità \textit {k-Sparse}}{91}{subsection.5.3.2}%
\contentsline {subsection}{\numberline {5.3.3}Funzione di Costo e Addestramento}{91}{subsection.5.3.3}%
\contentsline {section}{\numberline {5.4}Interpretazione Automatizzata delle Feature}{92}{section.5.4}%
\contentsline {section}{\numberline {5.5}Feature Families e Struttura Gerarchica}{92}{section.5.5}%
\contentsline {section}{\numberline {5.6}Feature Families e Struttura Gerarchica}{93}{section.5.6}%
\contentsline {paragraph}{Criterio di identificazione delle feature genitore e figlie}{93}{section*.89}%
\contentsline {subsection}{\numberline {5.6.1}Costruzione del Grafo di Co-occorrenza}{94}{subsection.5.6.1}%
\contentsline {subsection}{\numberline {5.6.2}Identificazione delle Feature Families}{95}{subsection.5.6.2}%
\contentsline {chapter}{\numberline {6}Prisma}{97}{chapter.6}%
\contentsline {section}{\numberline {6.1}Introduzione}{97}{section.6.1}%
\contentsline {section}{\numberline {6.2}Architettura}{97}{section.6.2}%
\contentsline {section}{\numberline {6.3}Generazione Embeddings}{98}{section.6.3}%
\contentsline {subsection}{\numberline {6.3.1}Gestione di documenti lunghi: strategia \textit {chunk-and-average}}{98}{subsection.6.3.1}%
\contentsline {paragraph}{Tokenizzazione e vincoli di lunghezza}{99}{section*.91}%
\contentsline {paragraph}{Segmentazione in chunk contigui}{99}{section*.92}%
\contentsline {paragraph}{Embedding per chunk e concetto di pooling}{99}{section*.93}%
\contentsline {paragraph}{Aggregazione a livello documento}{100}{section*.94}%
\contentsline {section}{\numberline {6.4}Training SAE}{100}{section.6.4}%
\contentsline {section}{\numberline {6.5}Interpretazione}{101}{section.6.5}%
\contentsline {chapter}{\numberline {7}Esperimenti e risultati}{103}{chapter.7}%
\contentsline {section}{\numberline {7.1}Introduzione}{103}{section.7.1}%
\contentsline {section}{\numberline {7.2}Pedianet}{103}{section.7.2}%
\contentsline {subsection}{\numberline {7.2.1}Scelta del modello di embedding}{104}{subsection.7.2.1}%
\contentsline {subsection}{\numberline {7.2.2}Esperimento}{104}{subsection.7.2.2}%
\contentsline {section}{\numberline {7.3}Abstracts}{105}{section.7.3}%
\contentsline {section}{\numberline {7.4}PubMed}{105}{section.7.4}%
