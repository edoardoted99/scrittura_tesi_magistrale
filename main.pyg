def byte_pair_encoding(corpus, num_merges):
    """
    Implementazione semplificata del training BPE.

    Parametri:
        corpus (list of list of str): corpus tokenizzato a livello di carattere.
            Esempio: [["n", "e", "w"], ["r", "e", "n", "e", "w"]]
        num_merges (int): numero di merge (k)

    Ritorna:
        vocab (set): insieme dei token appresi
    """

    # 1. vocabolario iniziale: tutti i caratteri unici
    vocab = set(char for word in corpus for char in word)

    # 2. funzione ausiliaria per contare coppie adiacenti
    def get_stats(corpus):
        pairs = {}
        for word in corpus:
            for i in range(len(word) - 1):
                pair = (word[i], word[i + 1])
                pairs[pair] = pairs.get(pair, 0) + 1
        return pairs

    # 3. funzione per unire la coppia più frequente
    def merge_pair(pair, corpus):
        merged = []
        bigram = " ".join(pair)
        replacement = "".join(pair)
        for word in corpus:
            word_str = " ".join(word)
            # sostituisce la coppia più frequente con il nuovo token
            new_word = word_str.replace(bigram, replacement)
            merged.append(new_word.split())
        return merged

    # 4. ciclo principale: effettua k fusioni
    for i in range(num_merges):
        pairs = get_stats(corpus)
        if not pairs:
            break
        # coppia più frequente
        best_pair = max(pairs, key=pairs.get)
        corpus = merge_pair(best_pair, corpus)
        vocab.add("".join(best_pair))
        print(f"Merge {i+1}: {best_pair} -> {''.join(best_pair)}")

    return vocab


# Esempio d'uso
corpus = [
    ["_", "n", "e", "w"],
    ["_", "r", "e", "n", "e", "w"],
    ["_", "s", "e", "t"],
    ["_", "r", "e", "s", "e", "t"]
]

vocab = byte_pair_encoding(corpus, num_merges=7)
print("\nVocabolario finale:")
print(vocab)
