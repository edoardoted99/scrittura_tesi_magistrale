\babel@toc {italian}{}\relax 
\babel@toc {italian}{}\relax 
\contentsline {chapter}{\numberline {1}Introduzione}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Introduzione}{1}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Motivazioni}{1}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Obiettivi e struttura della tesi}{2}{subsection.1.1.2}%
\contentsline {chapter}{\numberline {2}Parole e Tokens}{3}{chapter.2}%
\contentsline {section}{\numberline {2.1}Parole e Tokens}{3}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Le parole}{3}{subsection.2.1.1}%
\contentsline {subsection}{\numberline {2.1.2}Unicode}{5}{subsection.2.1.2}%
\contentsline {subsubsection}{Code points}{6}{section*.3}%
\contentsline {subsubsection}{UTF-8 Encoding}{7}{section*.5}%
\contentsline {subsection}{\numberline {2.1.3}Subword Tokenization: Byte-Pair Encoding}{8}{subsection.2.1.3}%
\contentsline {subsection}{\numberline {2.1.4}BPE Training}{10}{subsection.2.1.4}%
\contentsline {subsection}{\numberline {2.1.5}BPE Encoder}{14}{subsection.2.1.5}%
\contentsline {paragraph}{Le regole di fusione.}{15}{section*.9}%
\contentsline {subsection}{\numberline {2.1.6}Corpora}{15}{subsection.2.1.6}%
\contentsline {subsection}{\numberline {2.1.7}Minimum Edit Distance}{17}{subsection.2.1.7}%
\contentsline {chapter}{\numberline {3}N-gram Language Models}{21}{chapter.3}%
\contentsline {section}{\numberline {3.1}N-Grams}{21}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}La regola della catena}{22}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}L’assunzione di Markov e i modelli n-gram}{22}{subsection.3.1.2}%
\contentsline {subsection}{\numberline {3.1.3}Stima MLE per modelli n-gram}{23}{subsection.3.1.3}%
\contentsline {subsection}{\numberline {3.1.4}Calcolo in log-spazio e modelli n-gram estesi}{23}{subsection.3.1.4}%
\contentsline {subsection}{\numberline {3.1.5}Set di addestramento, sviluppo e test}{24}{subsection.3.1.5}%
\contentsline {subsection}{\numberline {3.1.6}Perplexity}{24}{subsection.3.1.6}%
\contentsline {subsection}{\numberline {3.1.7}Perplessità come fattore medio di diramazione}{25}{subsection.3.1.7}%
\contentsline {subsection}{\numberline {3.1.8}Campionamento da un modello linguistico}{25}{subsection.3.1.8}%
\contentsline {section}{\numberline {3.2}Smoothing, Interpolation e Backoff}{27}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Laplace (Add-One) Smoothing}{27}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Add-k Smoothing}{28}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3}Interpolazione}{28}{subsection.3.2.3}%
\contentsline {subsection}{\numberline {3.2.4}Backoff e Stupid Backoff}{28}{subsection.3.2.4}%
\contentsline {section}{\numberline {3.3}Perplessità ed Entropia}{29}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Entropia}{30}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Entropia di una sequenza}{30}{subsection.3.3.2}%
\contentsline {paragraph}{Tasso di entropia.}{30}{section*.13}%
\contentsline {paragraph}{Il teorema di Shannon–McMillan–Breiman.}{31}{section*.14}%
\contentsline {paragraph}{Intuizione.}{31}{section*.15}%
\contentsline {subsection}{\numberline {3.3.3}Cross-Entropy}{31}{subsection.3.3.3}%
\contentsline {paragraph}{Stima su sequenze lunghe.}{32}{section*.16}%
\contentsline {paragraph}{Relazione con l’entropia.}{32}{section*.17}%
\contentsline {paragraph}{Interpretazione intuitiva.}{32}{section*.18}%
\contentsline {subsection}{\numberline {3.3.4}Relazione con la Perplessità}{32}{subsection.3.3.4}%
\contentsline {paragraph}{Intuizione.}{32}{section*.19}%
\contentsline {paragraph}{Forma pratica.}{33}{section*.20}%
\contentsline {paragraph}{Interpretazione.}{33}{section*.21}%
\contentsline {subsection}{\numberline {3.3.5}Entropia e Compressione dei Dati}{33}{subsection.3.3.5}%
\contentsline {paragraph}{Codici a lunghezza fissa.}{33}{section*.22}%
\contentsline {paragraph}{Codici a lunghezza variabile.}{33}{section*.23}%
\contentsline {paragraph}{Esempio intuitivo.}{34}{section*.24}%
\contentsline {chapter}{\numberline {4}Regressione Logistica e classificazione del testo}{35}{chapter.4}%
\contentsline {section}{\numberline {4.1}Machine learning e classificazione}{35}{section.4.1}%
\contentsline {section}{\numberline {4.2}Funzione sigmoide}{36}{section.4.2}%
\contentsline {chapter}{\numberline {5}Embeddings}{39}{chapter.5}%
\contentsline {section}{\numberline {5.1}Embeddings}{40}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Ipotesi distribuzionale}{40}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}Semantica lessicale}{41}{subsection.5.1.2}%
\contentsline {subsection}{\numberline {5.1.3}Proprietà del significato lessicale}{42}{subsection.5.1.3}%
\contentsline {subsubsection}{L'ipotesi di Osgood: il significato come punto in uno spazio vettoriale}{43}{section*.27}%
\contentsline {subsection}{\numberline {5.1.4}Semantica vettoriale}{44}{subsection.5.1.4}%
\contentsline {subsection}{\numberline {5.1.5}Simple count-based embeddings}{45}{subsection.5.1.5}%
\contentsline {subsection}{\numberline {5.1.6}Cosine Similarity}{48}{subsection.5.1.6}%
\contentsline {subsection}{\numberline {5.1.7}Word2Vec}{49}{subsection.5.1.7}%
\contentsline {subsubsection}{Il classificatore}{51}{section*.32}%
\contentsline {subsubsection}{Algoritmo di apprendimento}{55}{section*.38}%
\contentsline {subsection}{\numberline {5.1.8}Proprietà semantiche degli embeddings}{56}{subsection.5.1.8}%
\contentsline {subsubsection}{1. Influenza della finestra di contesto}{56}{section*.39}%
\contentsline {subsubsection}{2. First-order vs. second-order similarity}{57}{section*.40}%
\contentsline {subsubsection}{3. Analogical reasoning (modello del parallelogramma)}{57}{section*.41}%
\contentsline {subsubsection}{4. Struttura geometrica: ortogonalità e parallelismo}{58}{section*.42}%
\contentsline {subsubsection}{5. Effetti pratici}{58}{section*.43}%
\contentsline {subsection}{\numberline {5.1.9}Embeddings as the input to neural net classifiers}{58}{subsection.5.1.9}%
\contentsline {chapter}{\numberline {6}Reti Neurali}{59}{chapter.6}%
\contentsline {section}{\numberline {6.1}Unità neurali}{60}{section.6.1}%
\contentsline {section}{\numberline {6.2}Il problema dell’XOR e la necessità dei livelli nascosti}{60}{section.6.2}%
\contentsline {section}{\numberline {6.3}Feedforward Neural Networks}{61}{section.6.3}%
\contentsline {section}{\numberline {6.4}Embeddings come input dei classificatori neurali}{62}{section.6.4}%
\contentsline {subsection}{\numberline {6.4.1}La embedding matrix}{62}{subsection.6.4.1}%
\contentsline {subsection}{\numberline {6.4.2}Ottenere l’embedding tramite vettori one-hot}{63}{subsection.6.4.2}%
\contentsline {subsection}{\numberline {6.4.3}Rappresentare una sequenza di parole}{63}{subsection.6.4.3}%
\contentsline {subsection}{\numberline {6.4.4}Dare gli embedding in input a un classificatore}{63}{subsection.6.4.4}%
\contentsline {subsection}{\numberline {6.4.5}Pooling: esempio per la classificazione di sentiment}{64}{subsection.6.4.5}%
\contentsline {subsection}{\numberline {6.4.6}Concatenazione: esempio per il language modeling}{64}{subsection.6.4.6}%
\contentsline {subsection}{\numberline {6.4.7}Perché gli embedding migliorano i modelli neurali}{65}{subsection.6.4.7}%
