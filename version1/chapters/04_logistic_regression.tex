\chapter{Regressione Logistica e classificazione del testo}
Introduzione
\section{Machine learning e classificazione}

Lo scopo della \textbf{classificazione} è quello di prendere una singola \textbf{osservazione}, 
estrarre da essa alcune proprietà utili, chiamate \textbf{feature}, e assegnarla a una delle \textbf{classi} presenti in un insieme discreto di categorie. 

Ad esempio, una \textit{task} di classificazione potrebbe consistere nel determinare se una 
\textbf{email} (l'osservazione), descritta dalle relative \textbf{feature} 
(come le parole contenute nel testo, l'orario di invio o il mittente), appartenga alla classe 
\textit{spam} o \textit{non spam}. 

Il modo più efficace per affrontare questo tipo di problema è utilizzare un approccio di 
\textbf{apprendimento supervisionato}, ovvero un paradigma in cui al modello vengono mostrati 
esempi già etichettati. In questo modo, la macchina impara a generalizzare la funzione 
sottostante che associa le \textbf{feature} ($x \in X$) alle \textbf{classi} ($y \in Y$). 
\section{Funzione sigmoide}

Lo scopo della \textbf{regressione logistica binaria} è quello di addestrare un classificatore
in grado di prendere una decisione binaria sulla classe di un nuovo input. 
A tale scopo introduciamo il \textbf{classificatore sigmoide}, che ci permette di esprimere 
questa decisione in termini probabilistici.

Consideriamo un singolo input $\mathbf{x}$, rappresentato da un vettore di feature:
\[
\mathbf{x} = [x_1, x_2, \dots, x_n].
\]
Il classificatore produce in output una variabile $y$ che può assumere due valori:
$y = 1$ (l'osservazione appartiene alla classe positiva) oppure $y = 0$
(l'osservazione appartiene alla classe negativa). 
Il nostro obiettivo è stimare la probabilità condizionata $P(y = 1 | \mathbf{x})$, 
cioè la probabilità che l'osservazione appartenga alla classe positiva dati i suoi valori di input.

La regressione logistica apprende, a partire da un insieme di addestramento, 
un vettore di \textbf{pesi} $\mathbf{w} = [w_1, w_2, \dots, w_n]$ e un termine di \textbf{bias} $b$, 
detto anche \textbf{intercetta}. 
Ogni peso $w_i$ è un numero reale associato alla feature $x_i$ e rappresenta quanto quella 
feature contribuisce alla decisione finale del classificatore: 
un peso positivo fornisce evidenza a favore della classe positiva, 
mentre un peso negativo fornisce evidenza a favore della classe negativa.

Durante la fase di classificazione, una volta appresi i pesi e il bias, 
il modello calcola una combinazione lineare delle feature di input:
\[
z = \sum_{i=1}^{n} w_i x_i + b = \mathbf{w} \cdot \mathbf{x} + b.
\]
Il valore $z$, detto \textbf{logit}, rappresenta la somma pesata delle evidenze a favore della classe positiva. 
Tuttavia, poiché $z$ può assumere qualsiasi valore reale (da $-\infty$ a $+\infty$), 
non può essere interpretato direttamente come una probabilità.

Per ottenere una probabilità compresa tra 0 e 1, si applica al valore $z$ la 
\textbf{funzione sigmoide} (o \textbf{funzione logistica}), definita come:
\[
\sigma(z) = \frac{1}{1 + e^{-z}}.
\]
La funzione sigmoide mappa un numero reale nel range $(0, 1)$, 
è quasi lineare intorno a $z = 0$, ma tende a saturarsi verso 0 e 1 per valori estremi di $z$.
Inoltre, è continua e derivabile, una proprietà che la rende adatta ai metodi di ottimizzazione 
basati sul gradiente.

Di conseguenza, la probabilità che un'osservazione appartenga alla classe positiva è:
\[
P(y = 1 | \mathbf{x}) = \sigma(\mathbf{w} \cdot \mathbf{x} + b)
\]
mentre la probabilità che appartenga alla classe negativa è:
\[
P(y = 0 | \mathbf{x}) = 1 - \sigma(\mathbf{w} \cdot \mathbf{x} + b) = \sigma(-(\mathbf{w} \cdot \mathbf{x} + b)).
\]
Infine, ricordiamo che il termine \textit{logit} deriva dal fatto che la funzione sigmoide è 
l'inversa della \textbf{funzione logit}, definita come:
\[
\text{logit}(p) = \ln\left(\frac{p}{1 - p}\right).
\]
In altre parole, il valore $z = \mathbf{w} \cdot \mathbf{x} + b$ rappresenta il logaritmo del rapporto 
tra la probabilità che l'osservazione appartenga alla classe positiva e quella che appartenga alla classe negativa.
