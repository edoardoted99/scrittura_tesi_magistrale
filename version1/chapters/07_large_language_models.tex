    \chapter{Large Language models}

    \section{Introduzione}


    Come discusso nei capitoli precedenti, un large language model (LLM) è un modello capace di prevedere la parola successiva in un testo a partire da quelle precedenti, assegnando una distribuzione di probabilità alle possibili continuazioni. Un LLM è semplicemente una versione molto più ampia e potente dei modelli linguistici tradizionali.

    Nel Capitolo 3 abbiamo introdotto i modelli bigramma e trigramma, che possono prevedere la parola successiva basandosi rispettivamente sulla precedente o su un piccolo insieme di parole. Al contrario, i large language models sono in grado di utilizzare contesti enormi — spesso migliaia o addirittura decine di migliaia di parole — permettendo loro di cogliere dipendenze linguistiche profonde e strutture testuali complesse.

    L’intuizione di base dei modelli linguistici è che un modello capace di prevedere il testo, cioè di attribuire probabilità alle parole successive, può anche essere usato per generare testo: 
    basta campionare parole da tale distribuzione. Come ricordato nel Capitolo 3, campionare significa scegliere una parola dalla distribuzione di probabilità prodotta dal modello.

    \begin{figure}
        \centering
        \includegraphics[width=0.8\textwidth]{pictures/decoder.png}
        \caption{Esempio di generazione di testo. Il modello prende in input un testo e prevede in modo autoregeressivo la parola successiva.}
    \end{figure}

\section{Tipi di architetture dei Language Models}

In questa sezione analizzeremo le tre principali architetture utilizzate nei moderni modelli di linguaggio: \textbf{encoder}, \textbf{decoder} ed \textbf{encoder--decoder}. Sebbene differiscano per finalità e flussi informativi, tutte possono essere implementate con varie famiglie di reti neurali. L’architettura oggi più utilizzata è il \textbf{Transformer}, che sarà introdotto nei capitoli successivi e che funge da denominatore comune ai modelli più recenti.

\subsection*{Decoder}

Il \textbf{decoder} è l’architettura generativa per eccellenza: genera un token alla volta basandosi sui token precedenti. L’informazione scorre da sinistra verso destra grazie al meccanismo di \emph{causal masking}, che impedisce al modello di “vedere” i token futuri durante la predizione. Questo rende possibile la generazione iterativa della sequenza, come avviene nei modelli Claude, GPT, Llama e Mistral.

\subsection*{Encoder}

L’\textbf{encoder} prende in input una sequenza di token e produce, per ciascun token, una rappresentazione vettoriale ricca e contestuale. A differenza dei decoder, gli encoder non generano testo: il loro compito è \emph{comprendere} la sequenza in ingresso.

Gli encoder vengono generalmente addestrati tramite \textbf{masked language modeling}: durante l’addestramento alcune parole della frase vengono mascherate (sostituite con un token speciale come \texttt{[MASK]}), e il modello deve prevedere le parole nascoste sfruttando l’intero contesto, sia a sinistra che a destra. Questo processo li rende modelli \emph{bidirezionali}, capaci di catturare relazioni profonde tra i token.

Modelli come BERT, RoBERTa e altri appartenenti alla stessa famiglia seguono questa architettura, eccellendo nei compiti di analisi del linguaggio --- classificazione, estrazione di informazione, similarità semantica --- ma non nella generazione.

\subsection*{Encoder--Decoder}

L’\textbf{encoder--decoder} combina i due approcci. L’encoder elabora l’intera sequenza di input e ne costruisce una rappresentazione strutturata; il decoder genera l’output condizionandosi sia sui token precedenti sia sulle rappresentazioni fornite dall’encoder tramite il meccanismo di \emph{encoder--decoder attention}. 

A differenza dei modelli \emph{decoder-only}, qui esiste un allineamento esplicito e profondo tra token di input e token di output, che consente di mappare accuratamente un tipo di sequenza in un altro. Ciò rende questa architettura ideale per compiti come la \emph{machine translation}, dove i token di input sono in una lingua e quelli di output in un’altra.

\bigskip

Queste tre architetture possono essere implementate con diversi tipi di reti neurali. Il Transformer, che introdurremo nel Capitolo~\ref{chap:transformer}, è oggi l’architettura dominante: ogni token è elaborato da una colonna di livelli Transformer, ciascuno composto da diversi sottocomponenti. In capitoli successivi discuteremo anche architetture precedenti ma ancora rilevanti, come le LSTM, e altre più recenti come i \emph{state space models}.

Tuttavia, per gli scopi di questo capitolo, adotteremo un approccio \emph{agnostico all’architettura}: tratteremo il modulo che implementa il decoder come una scatola nera. L’input di questa scatola è una sequenza di token, e il suo output è una distribuzione sui possibili token successivi da cui è possibile campionare. Le tecniche di addestramento e di decodifica che presenteremo valgono indipendentemente dall’architettura utilizzata.
