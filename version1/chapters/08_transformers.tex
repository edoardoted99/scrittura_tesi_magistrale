\chapter{Transformer}
In questo capitolo introduciamo il \textbf{transformer}, l’architettura standard utilizzata per costruire i moderni modelli di linguaggio.
Un transformer è una rete neurale con una struttura specifica che include un meccanismo chiamato \textbf{self-attention} (o \textbf{multi-head attention} nella sua forma estesa).

L’attenzione può essere interpretata come un modo per costruire rappresentazioni contestuali del significato di un token, pesando l’importanza dei token circostanti e integrando le informazioni provenienti da essi. In questo modo il modello apprende come i token si relazionano tra loro anche a lunghe distanze.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{pictures/transformer.png}
    \caption{Architettura di un transformer (da sinistra a destra). Ogni token di input viene codificato, processato attraverso una serie di blocchi transformer impilati e infine passato a una testa di modellazione linguistica che predice il token successivo.}
    \label{fig:transformer}
\end{figure}

La figura \ref{fig:transformer} illustra la struttura generale di un transformer. Un transformer è composto da tre componenti principali. Al centro troviamo la colonna dei \textbf{blocchi Transformer}, impilati l’uno sull’altro.

Ogni blocco è costituito da:
\begin{itemize}
    \item un livello di \textbf{multi-head self-attention},
    \item un \textbf{feed-forward neural network},
    \item un \textbf{layer di normalizzazione} (tipicamente LayerNorm),
\end{itemize}
ognuno seguito da una \textbf{residual connection}.  
Lo scopo di ciascun blocco è trasformare un vettore di input $x_i$, associato al token $i$, in un nuovo vettore di output $h_i$.  
Una sequenza di $n$ blocchi mappa un’intera \textbf{context window} $[x_1, x_2, \ldots, x_n]$ in un nuovo insieme di vettori $[h_1, h_2, \ldots, h_n]$ della stessa dimensione. Il numero di blocchi può variare da circa 12 fino a 96 o più nei modelli di grandi dimensioni.

Prima dei blocchi troviamo la componente di \textbf{input encoding}, che trasforma ogni token (ad esempio la parola \texttt{thank}) in una rappresentazione vettoriale.  
L’encoding utilizza una \textbf{matrice di embedding $E$} e un meccanismo per codificare la posizione del token, come i \textbf{positional embeddings}.

Dopo la colonna dei blocchi, ogni vettore $h_i$ viene passato a una \textbf{testa di modellazione linguistica} (\textit{language modeling head}), che prende l'embedding in output dato dall'ultimo blocco trasnofrmer e lo passa in una \textbf{unembedding matrix U} e una softmax sopra il vocabolario per generare un singolo tokex per quella colonna. 

\section{Attenzione}

Abbiamo visto che gli embeddings generati da algoritmi come \textit{word2vec} sono \textbf{statici}: il vettore che rappresenta una parola nello spazio semantico è fisso e non varia in base al contesto in cui la parola appare.

Consideriamo ora il seguente esempio. Le frasi sono:
\begin{enumerate}
    \item \textbf{The chicken} didn’t cross the road because \textbf{it} was too tired.
    \item \textbf{The chicken} didn’t cross the road because \textbf{it} was too wide.
\end{enumerate}

Nella prima frase il pronome \textbf{it} si riferisce al pollo, mentre nella seconda si riferisce alla strada. Di conseguenza, se vogliamo modellare correttamente il significato, abbiamo bisogno che il vettore associato alla parola \textbf{it} sia diverso nelle due frasi, così da catturare il corretto riferimento in base al contesto.

Inoltre, se consideriamo la frase parziale
\setcounter{enumi}{2}
\begin{enumerate}
    \item \textbf{The chicken} didn’t cross the road because \textbf{it}
\end{enumerate}

non siamo ancora in grado di sapere a cosa si riferisca il pronome \textbf{it}. Il significato rimane ambiguo fino a quando il resto della frase non viene osservato. Questo esempio mostra la necessità di rappresentazioni \textbf{contestuali}, in cui il significato di una parola (e quindi il suo embedding) dipende dal contesto circostante.
Il fatto che le parole abbiano ricche relazioni linguistiche con altre parole, anche distanti, è una caratteristica pervasiva del linguaggio.
I transformer riescono a costruire rappresentazioni contestuali del significato delle parole i cosiddetti \textbf{contextual embeddings} integrando il contributo di queste parole contestuali utili. 
Nei transformer, strato dopo strato, costruiamo rappresentazioni sempre più ricche e contestualizzate del significato dei token in input. 
A ogni livello calcoliamo la rappresentazione di un token \textit{i} combinando l’informazione su \textit{i} proveniente dal livello precedente con l’informazione dei token vicini, 
producendo così una rappresentazione contestuale per ogni parola in ogni posizione. proveniente dal livello precedente con l’informazione dei token vicini, producendo così una rappresentazione contestuale per ogni parola in ogni posizione.
L'\textbf{attenzione} è il meccanismo nel transformer che pesa e combina le rappresentazioni da un layer \textit{k} per costruire la rappresenzione per i tokens nel successivo \textit{k}+1.
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{pictures/attention.png}
    \caption{Distribuzione dei pesi di self-attention $\alpha$ utilizzata per calcolare la rappresentazione della parola \textit{it} allo strato $k+1$. Il modello presta attenzione in modo diverso ai token dello strato $k$, con sfumature più scure che indicano valori di attenzione più alti. Si nota un’attenzione elevata verso i token \textit{chicken} e \textit{road}, un risultato sensato poiché in quel punto della frase \textit{it} potrebbe plausibilmente correferire con entrambi. Di conseguenza, la rappresentazione di \textit{it} incorpora informazioni provenienti da queste parole precedenti. Figura adattata da Uszkoreit (2017).}
    \label{fig:attention}
\end{figure}

\subsection{L'attenzione più formalmente}
L'attenzione prende una rappresentazione $x_i$ corrispondente al token di input alla posizione $i$ e la context window dei precedenti input $x_i, ..., x_{i-1}$ e produce un output $a_i$.
Nei modelli linguistici causali il contesto consiste in tutte le parole precedenti da sinistra a destra fino alla posizione $i$ ma non ha accesso a quelli successivi. L'attenzione però può essere generalizzata anche al caso in cui ogni token è visto anche dai suoi successivi. 
La figura  \ref{fig:attention} illustra questo flusso di informazione. Un livello di self-attention quindi  mappa 
\begin{equation}
    (x_1, x_2, \ldots, x_n) \rightarrow (a_1, a_2, \ldots, a_n).
\end{equation}
\subsection{Versione semplificata dell'attenzione}
Vediamo ora una versione semplificata del meccanismo di attenzione. In sostanza, l'attenzione altro non è che una somma peseta dei vettori di contesto, con un sacco di complicaizoni aggiuzte su come veegnono calcolati i pesi. 
Per scopi pedagocii descriviamo prima una versione semplificata dell'attenzione, in cui l'output corrispondente al token nella posizione $i$ è calcolato come 
\begin{equation}
    \mathbf{a}_i = \sum_{j=1}^{n} \alpha_{i,j} \mathbf{x}_j,
\end{equation}

Nel meccanismo di attenzione pesiamo ogni embedding precedente in proporzione alla sua somiglianza con il token corrente \textit{i}. L'output dell'attenzione è dunque una somma degli embedding dei token precedenti, ciascuno pesato in base alla somiglianza con il token corrente.
Calcoliamo i punteggi di somiglianza tramite il prodotto scalare che mappa due vettori in un valore scalare che va da $-\infty$ a $+\infty$. Normalizziamo poi questi punteggi con una \textbf{softmax} per ottenere il vettore dei pesi $\alpha_{ij}$ per tutti gli indici $j \leq i$.

\begin{equation*}
    \textrm{score}(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i \cdot \mathbf{x}_j
\end{equation*}

\begin{equation*}
    \alpha_{i,j} = \textrm{softmax}(\textrm{score}(\mathbf{x}_i, \mathbf{x}_j)) \quad \forall j \leq i
\end{equation*}

In riferimento alla figura \ref{fig:attention} possiamo calcolare $a_3$ calcolando i tre punteggi
\begin{equation*}
    \mathbf{x}_3 \cdot \mathbf{x}_1, \quad \mathbf{x}_3 \cdot \mathbf{x}_2, \quad \mathbf{x}_3 \cdot \mathbf{x}_3
\end{equation*}
e poi viene passato dentro alla softmax. Adesso siamo pronti ad eliminare le semplicficazioni fatte e a vedere la versione completa del meccanismo di attenzione.
\subsection{Una singola testa di attenzione: matrici di query, chiave e valore}

Dopo aver introdotto l’intuizione di base del meccanismo di attenzione, presentiamo ora una vera e propria 
\textbf{testa di attenzione}, cioè la versione effettivamente utilizzata nei transformer.  
Una testa di attenzione ci permette di distinguere in modo chiaro i tre diversi ruoli che ogni embedding di input 
può ricoprire durante il processo di calcolo:

\begin{itemize}
    \item \textbf{Query} — il ruolo dell’elemento corrente che viene confrontato con gli input precedenti;
    \item \textbf{Key} — il ruolo degli elementi precedenti che vengono confrontati con l’elemento corrente per determinare un peso di similarità;
    \item \textbf{Value} — il ruolo degli elementi precedenti il cui contenuto viene pesato e sommato per produrre l’output dell’elemento corrente.
\end{itemize}

Per modellare questi tre ruoli distinti, i transformer introducono tre matrici di pesi dedicate:
\[
\mathbf{W}_Q, \quad \mathbf{W}_K, \quad \mathbf{W}_V.
\]

Queste matrici proiettano ciascun vettore di input \(\mathbf{x}_i\) in tre diverse rappresentazioni, una per ciascun ruolo:

\[
\mathbf{q}_i = \mathbf{x}_i \mathbf{W}_Q, \qquad
\mathbf{k}_i = \mathbf{x}_i \mathbf{W}_K, \qquad
\mathbf{v}_i = \mathbf{x}_i \mathbf{W}_V.
\]

Ogni token, dunque, genera una tripla \((\mathbf{q}_i, \mathbf{k}_i, \mathbf{v}_i)\) che viene utilizzata nella computazione dell’attenzione.
