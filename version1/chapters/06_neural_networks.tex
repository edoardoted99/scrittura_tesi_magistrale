\chapter{Reti Neurali}

Le reti neurali costituiscono oggi uno degli strumenti fondamentali per 
l’elaborazione del linguaggio naturale. Sebbene il termine \textit{neural} derivi dai 
primi modelli ispirati ai neuroni biologici — in particolare il neurone di 
McCulloch e Pitts (1943) — le reti neurali moderne devono essere intese come 
architetture computazionali, prive di una reale ispirazione biologica.

Una rete neurale è formata da piccole unità elementari di calcolo collegate tra loro. 
Ogni unità riceve un vettore di input, applica una trasformazione (includendo una 
componente non lineare) e produce un output. Collegando queste unità in sequenza si 
ottengono i \textbf{layer} della rete. Se l’informazione fluisce dai layer iniziali a 
quelli successivi senza cicli o retroazioni, parliamo di \textbf{feedforward neural 
networks}.

Il termine \textit{deep learning} si riferisce proprio alla presenza di molti layer 
successivi: maggiore profondità permette alla rete di apprendere rappresentazioni 
via via più astratte e utili per il compito finale. A differenza dei modelli classici 
come la regressione logistica, che richiedono una progettazione manuale delle 
feature, le reti profonde apprendono automaticamente rappresentazioni intermedie 
(\textit{representation learning}), come abbiamo già visto per gli embeddings del 
Capitolo~5.

In questo capitolo introdurremo le reti neurali feedforward come classificatori. Nei 
capitoli successivi vedremo modelli più complessi come i transformer (Capitolo~8), 
le reti ricorrenti (Capitolo~13) e le reti convoluzionali (Capitolo~15).

\section{Unità neurali}

L’elemento di base di una rete neurale è la singola \textbf{unità neurale}.  
Data un’entrata $\mathbf{x} = (x_1, \dots, x_n)$, la unità calcola una somma pesata 
dell’input:

\begin{equation}
z = \mathbf{w} \cdot \mathbf{x} + b
\label{eq:z_value}
\end{equation}

\noindent
dove $\mathbf{w} = (w_1, \dots, w_n)$ è il vettore dei pesi e $b$ è il bias.  
Il valore $z$ è una trasformazione lineare dell’input. Poiché una rete basata solo su 
trasformazioni lineari non sarebbe in grado di apprendere relazioni complesse, si 
applica a $z$ una funzione di attivazione non lineare $f$:

\begin{equation}
y = a = f(z)
\end{equation}

\noindent
Tra le funzioni più usate troviamo:

\begin{itemize}
    \item \textbf{Sigmoid}: $f(z)=\frac{1}{1+e^{-z}}$, mappa in $(0,1)$.
    \item \textbf{Tanh}: output in $(-1,1)$.
    \item \textbf{ReLU}: $f(z)=\max(0,z)$, molto efficace in reti profonde.
\end{itemize}

La presenza di una non linearità è cruciale: senza di essa la rete, indipendentemente 
dal numero di layer, si comporterebbe come un semplice modello lineare.

\section{Il problema dell’XOR e la necessità dei livelli nascosti}

L’importanza della non linearità e dei livelli nascosti fu evidenziata in uno dei 
risultati più celebri della teoria delle reti neurali: il lavoro di Minsky e Papert 
(1969). Gli autori mostrarono che una singola unità neurale lineare — il 
\textit{perceptron} — non può calcolare la funzione logica XOR.

Consideriamo tre funzioni booleane di due variabili $x_1$ e $x_2$:

\begin{table}[h!]
\centering
\begin{tabular}{c c c | c c c | c c c}
\multicolumn{3}{c|}{\textbf{AND}} & 
\multicolumn{3}{c|}{\textbf{OR}} & 
\multicolumn{3}{c}{\textbf{XOR}} \\
\hline
$x_1$ & $x_2$ & $y$ \; & 
$x_1$ & $x_2$ & $y$ \; & 
$x_1$ & $x_2$ & $y$ \\
\hline
0 & 0 & 0 &   0 & 0 & 0 &   0 & 0 & 0 \\
0 & 1 & 0 &   0 & 1 & 1 &   0 & 1 & 1 \\
1 & 0 & 0 &   1 & 0 & 1 &   1 & 0 & 1 \\
1 & 1 & 1 &   1 & 1 & 1 &   1 & 1 & 0 \\
\end{tabular}
\caption{Tabelle di verità per le funzioni logiche AND, OR e XOR.}
\end{table}

Il percettrone calcola:

\begin{equation}
y =
\begin{cases}
0 & \text{se } \mathbf{w} \cdot \mathbf{x} + b \le 0 \\
1 & \text{se } \mathbf{w} \cdot \mathbf{x} + b > 0
\end{cases}
\end{equation}

Tale unità è un \textbf{classificatore lineare}. Può separare i punti dello spazio 
$(x_1, x_2)$ mediante una linea, riuscendo così a rappresentare AND e OR. Tuttavia, 
la funzione XOR non è linearmente separabile: non esiste una linea che separi i casi 
positivi $(0,1)$ e $(1,0)$ da quelli negativi $(0,0)$ e $(1,1)$.

La soluzione consiste nell’introdurre almeno un \textbf{livello nascosto} dotato di 
funzioni di attivazione non lineari. Tale livello costruisce una rappresentazione 
intermedia degli input che rende XOR linearmente separabile. Questo esempio mostra 
la ragione fondamentale dell’esistenza delle reti multistrato: la capacità di 
apprendere rappresentazioni che superano i limiti dei modelli lineari.

\section{Feedforward Neural Networks}

Una \textbf{feedforward neural network} è una rete in cui l’informazione scorre dagli 
input verso gli output senza retroazioni. Quando contiene uno o più livelli nascosti, 
viene spesso denominata \textit{multilayer perceptron} (MLP), sebbene le unità moderne 
non utilizzino l’attivazione a soglia del perceptrone, ma non linearità continue.

Una rete feedforward tipica è composta da:

\begin{itemize}
    \item \textbf{Layer di input} ($x$), che rappresenta i dati grezzi.
    \item \textbf{Layer nascosto} ($h$), che apprende rappresentazioni intermedie.
    \item \textbf{Layer di output} ($y$), che fornisce la predizione finale.
\end{itemize}

Il layer nascosto calcola:

\begin{equation}
h = \sigma(Wx + b)
\end{equation}

Lo strato di output applica una trasformazione lineare seguita dal softmax per ottenere 
una distribuzione di probabilità:

\begin{equation}
z = Uh, \qquad y = \text{softmax}(z)
\end{equation}

\noindent
dove $W$ e $U$ sono le matrici dei pesi.

Da questa prospettiva, una rete feedforward può essere vista come una regressione 
logistica potenziata: invece di progettare manualmente le feature, la rete apprende da 
sé rappresentazioni utili nei livelli intermedi. Aumentando il numero di livelli, la 
rete diventa \textbf{profonda} e capace di apprendere strutture sempre più complesse.


\section{Embeddings come input dei classificatori neurali}

Nei modelli neurali moderni per l’elaborazione del linguaggio naturale è raro 
utilizzare vettori di feature progettati manualmente. Una delle principali 
caratteristiche del deep learning è infatti la capacità di apprendere 
automaticamente le rappresentazioni utili per il compito da svolgere. Il 
punto di partenza è la rappresentazione delle parole tramite \textbf{word embeddings}.

Un embedding è un vettore denso e di dimensione fissa che rappresenta un token. 
Nei modelli più semplici utilizziamo \textbf{embedding statici}, come word2vec o 
GloVe: ogni parola del vocabolario è associata a un unico vettore fisso, indipendente 
dal contesto in cui appare. Tali vettori vengono raccolti in una matrice chiamata 
\textbf{embedding matrix}.

\subsection{La embedding matrix}

La matrice degli embedding $E$ contiene una riga per ogni parola del vocabolario 
$V$. Se indichiamo con $d$ la dimensione degli embedding, allora:

\[
E \in \mathbb{R}^{|V| \times d}
\]

La riga $E_i$ contiene il vettore denso che rappresenta il token con indice $i$ nel 
vocabolario. Quando vogliamo ottenere l’embedding di una parola, è sufficiente 
selezionare la riga corrispondente.

\subsection{Ottenere l’embedding tramite vettori one-hot}

Una maniera equivalente di vedere questa operazione è attraverso i 
\textbf{vettori one-hot}.  
Un vettore one-hot per la parola $i$ è un vettore di dimensione $|V|$ con tutti 
zeri tranne un 1 nella posizione $i$:

\[
x_i = [0,0,\dots,1,\dots,0]
\]

Moltiplicando la matrice $E$ per questo vettore one-hot:

\[
x_i E = E_i
\]

si seleziona la riga corrispondente della matrice degli embedding. In altre parole, 
la moltiplicazione con un one-hot non fa altro che \textit{prelevare} l’embedding della 
parola.

\subsection{Rappresentare una sequenza di parole}

Per una sequenza di $N$ token otteniamo una matrice di embedding:

\[
\begin{bmatrix}
E_{w_1} \\[3pt]
E_{w_2} \\
\vdots \\
E_{w_N}
\end{bmatrix}
\in \mathbb{R}^{N \times d}
\]

Ogni riga rappresenta un token della frase.

\subsection{Dare gli embedding in input a un classificatore}

Una rete neurale che riceve in input gli embedding deve trasformare questa matrice 
$N \times d$ in un unico vettore che rappresenti l’intero testo, da utilizzare come 
input per lo strato nascosto e lo strato di output del classificatore.

Esistono due strategie principali:

\begin{enumerate}
    \item \textbf{Pooling}: combinare i $N$ vettori in un unico embedding di 
    dimensione $d$ (utile quando l’ordine delle parole è meno importante).
    \item \textbf{Concatenazione}: accodare i vettori ottenendo un embedding di 
    dimensione $Nd$ (utile quando la posizione delle parole conta).
\end{enumerate}

\subsection{Pooling: esempio per la classificazione di sentiment}

Nel sentiment analysis è spesso significativo \textit{che} parole compaiano nel testo, 
più che \textit{dove} compaiano. Per questo motivo una tecnica semplice ed efficace è 
la \textbf{mean-pooling}, che calcola la media degli embedding dei token:

\[
x = \frac{1}{N}\sum_{i=1}^{N} E_{w_i}
\]

Il vettore $x$ rappresenta l’intera frase.

Il classificatore diventa:

\[
h = \sigma(Wx + b)
\]
\[
z = Uh
\]
\[
\hat{y} = \text{softmax}(z)
\]

dove $\hat{y}$ è la distribuzione di probabilità sulle classi (ad esempio: positivo, 
negativo, neutro).

\subsection{Concatenazione: esempio per il language modeling}

In altri compiti, come il \textbf{language modeling}, l’ordine delle parole è invece 
cruciale. Per prevedere la prossima parola, il modello deve conoscere con precisione 
la sequenza dei token precedenti. Per questo motivo, invece del pooling, si 
\textit{concatenano} gli embedding:

\[
e = [E_{w_{t-3}} ; E_{w_{t-2}} ; E_{w_{t-1}}]
\]

ottenendo un vettore di dimensione $3d$ (nel caso di una finestra di 3 parole).

Il modello procede quindi con un normale passaggio feedforward:

\[
h = \sigma(We + b)
\]
\[
z = Uh
\]
\[
\hat{y} = \text{softmax}(z)
\]

dove $\hat{y}$ è un vettore di dimensione $|V|$ che contiene la probabilità per 
ciascuna possibile parola successiva.
\subsection{Perché gli embedding migliorano i modelli neurali}

L’uso degli embedding offre due grandi vantaggi fondamentali per i modelli 
linguistici neurali:

\begin{itemize}
    \item \textbf{Generalizzazione}: parole semanticamente simili hanno 
    embedding simili. Questo consente al modello di estendere ciò che ha appreso 
    anche a contesti mai osservati esplicitamente durante l’addestramento. \\
    Un esempio chiarisce il punto: supponiamo che nel training compaia la frase 
    ``Devo assicurarmi che il gatto venga nutrito'', ma non compaia mai la sequenza 
    ``il cane venga''. Se nel test incontriamo il contesto 
    ``Mi sono dimenticato di assicurarmi che il cane venga'', un modello 
    n-gram tradizionale non potrà prevedere correttamente la parola successiva 
    ``nutrito'', perché non ha mai visto ``cane'' in quel contesto. 
    Un modello neurale invece, avendo appreso che \emph{gatto} e \emph{cane} 
    hanno embedding simili, può generalizzare dal contesto visto con ``gatto'' e 
    assegnare comunque una probabilità alta alla continuazione ``nutrito'' anche 
    dopo ``cane''. 
    
    \item \textbf{Efficienza}: gli embedding riducono drasticamente la 
    dimensionalità rispetto alle rappresentazioni one-hot e permettono una 
    parametrizzazione più compatta. Ciò rende il modello più efficiente, facilita 
    l’ottimizzazione e consente di apprendere rappresentazioni distribuite molto 
    più informative.
\end{itemize}

Questi meccanismi rendono gli embedding la base dei moderni modelli linguistici 
neurali: architetture capaci di apprendere automaticamente rappresentazioni 
sempre più ricche e astratte del contenuto linguistico, andando ben oltre le 
limitazioni dei modelli statistici tradizionali.

