
\subsection{Riduzione dimensionale tramite SVD}
Un metodo per la riduzione della dimensionalità è la Singular Value Decomposition applicata alla word--context matrix. Sia $M \in \mathbb{R}^{|V| \times |V|}$ la word--context matrix, 
eventualmente pesata tramite tf-idf. 
La decomposizione ai valori singolari (Singular Value Decomposition, SVD) 
consente di fattorizzare $M$ come prodotto di tre matrici:
\[
M = U \Sigma V^\top
\]
dove $U$ e $V$ sono matrici ortogonali e $\Sigma$ è una matrice diagonale 
contenente i valori singolari ordinati in modo decrescente.

Ogni valore singolare rappresenta l’importanza di una direzione latente 
nello spazio semantico. I valori singolari maggiori catturano le 
correlazioni più rilevanti tra parole e contesti, mentre quelli più piccoli 
tendono a modellare rumore o variazioni locali meno informative.

Per ottenere una rappresentazione a dimensionalità ridotta, si considera 
una versione troncata della decomposizione, mantenendo solo i primi 
$k$ valori singolari:
\[
M \approx U_k \Sigma_k V_k^\top
\]
con $k \ll |V|$.

Le righe della matrice $U_k \Sigma_k$ costituiscono una rappresentazione 
densa delle parole target in uno spazio latente di dimensione $k$. 
In questo nuovo spazio, ogni parola è descritta da un vettore 
a dimensionalità ridotta, in cui le correlazioni semantiche risultano 
più evidenti rispetto alla rappresentazione originale sparsa.

È importante osservare che la riduzione dimensionale non elimina 
esplicitamente la sparsità della matrice originale, ma proietta le parole 
in uno spazio denso in cui le relazioni semantiche emergono in forma 
compressa e più robusta.


%%%%%%%%%%%%%%%


\section{Modelli di linguaggio neurali}
\label{sec:neural_language_models}

I \textit{modelli di linguaggio neurali} costituiscono il meccanismo fondamentale
attraverso cui vengono generate le rappresentazioni contestuali introdotte nella
sezione precedente. L’idea centrale è che, per poter predire correttamente una
parola in un determinato contesto, un modello debba necessariamente costruire una
rappresentazione interna del contesto stesso. È proprio questa rappresentazione
interna che, nei modelli moderni, viene interpretata come embedding contestuale.

\begin{notebox}
Formalmente, un modello di linguaggio assegna una probabilità a una sequenza di
token $x_1, \ldots, x_n$. Nei modelli neurali tale probabilità viene fattorizzata
tramite la regola della catena:
\[
P(x_1, \ldots, x_n) = \prod_{i=1}^{n} P(x_i \mid x_1, \ldots, x_{i-1}),
\]
riducendo il problema alla stima della probabilità della parola successiva dato il
contesto precedente. Il compito del modello è quindi quello di apprendere le
regolarità statistiche, sintattiche e semantiche del linguaggio a partire da grandi
corpora testuali.
\end{notebox}



Durante l’inferenza forward (o \textit{decoding}), dato un contesto di parole
precedenti, il modello esegue un forward pass sulla rete neurale per produrre una
distribuzione di probabilità sulle possibili parole successive. Nei primi modelli di
linguaggio neurali, il contesto osservabile è limitato a una finestra di dimensione
fissa che considera un numero $N$ di parole nel passato. Per chiarezza espositiva,
consideriamo il caso $N = 3$, in cui il contesto è costituito dalle parole
$w_{t-1}$, $w_{t-2}$ e $w_{t-3}$. Ciascuna parola del contesto viene inizialmente rappresentata come un vettore
\textit{one-hot} di dimensione $|V|$, dove $|V|$ è la dimensione del vocabolario.
Questi vettori vengono poi proiettati in uno spazio denso tramite una matrice di
embedding $E \in \mathbb{R}^{d \times |V|}$. La moltiplicazione della matrice $E$
per un vettore one-hot seleziona la colonna corrispondente alla parola, producendo il
suo embedding vettoriale. Gli embedding delle parole di contesto vengono quindi
concatenati per formare un unico vettore di input $e$.

\medskip

\noindent
Il vettore $e$ viene successivamente trasformato da uno strato nascosto della rete,
tramite una moltiplicazione per una matrice di pesi $W$, l’aggiunta di un termine di
bias $b$ e l’applicazione di una funzione di attivazione non lineare $\sigma$,
ottenendo lo stato nascosto $h$. Lo stato nascosto viene quindi proiettato nello
spazio delle dimensioni del vocabolario tramite una seconda matrice di pesi $U$,
generando un vettore di punteggi $z$. L’ultimo passo consiste nell’applicazione della funzione \textit{softmax}, che
trasforma i punteggi in una distribuzione di probabilità. Dopo l’applicazione del
softmax, ciascun nodo $i$ dello strato di uscita stima la probabilità che la parola
successiva $w_t$ sia la parola del vocabolario con indice $i$, dato il contesto:
\[
P(w_t = i \mid w_{t-1}, w_{t-2}, w_{t-3}).
\]
In sintesi, le equazioni di un modello di linguaggio neurale feedforward con finestra
di contesto di dimensione 3, dati vettori di input one-hot per ciascuna parola di
contesto, sono le seguenti:
\begin{equation}
\begin{aligned}
e &= [E x_{t-3};\ E x_{t-2};\ E x_{t-1}] \\
h &= \sigma(W e + b) \\
z &= U h \\
\hat{y} &= \text{softmax}(z)
\end{aligned}
\tag{3.1}
\end{equation}

\noindent
Si noti che il vettore di embedding $e$ è ottenuto concatenando gli embedding delle
parole di contesto; nel seguito utilizzeremo il punto e virgola per indicare la
concatenazione di vettori. Sebbene questi modelli siano in grado di apprendere rappresentazioni
distribuzionali utili e abbiano rappresentato un passo fondamentale verso la
modellazione neurale del linguaggio, essi presentano limiti strutturali evidenti. In
particolare, la dimensione del contesto è fissa e l’informazione contestuale viene
compressa in un’unica rappresentazione, rendendo difficile catturare dipendenze a
lungo raggio e strutture sintattiche complesse. Questi limiti hanno motivato lo sviluppo di modelli sequenziali più espressivi, in
grado di aggiornare dinamicamente la rappresentazione del contesto man mano che
la sequenza viene processata. Nel prossimo paragrafo analizzeremo le
\textit{Recurrent Neural Networks} (RNN) e le loro estensioni, le
\textit{Long Short-Term Memory} (LSTM), che rappresentano il primo tentativo
sistematico di modellare il linguaggio come una sequenza di lunghezza variabile e di
produrre embeddings contestuali dipendenti dall’intera storia precedente.



\section{Reti neurali ricorrenti e LSTM}
\label{sec:rnn_lstm}

I modelli di linguaggio neurali feedforward introdotti nella sezione precedente
rappresentano un primo passo verso la modellazione distribuzionale del linguaggio,
ma soffrono di un limite strutturale fondamentale: il contesto è limitato a una
finestra di dimensione fissa e non può adattarsi dinamicamente alla lunghezza della
sequenza. Per superare questo vincolo, sono state introdotte le
\textit{Recurrent Neural Networks} (RNN), architetture progettate per elaborare
sequenze di lunghezza arbitraria mantenendo una rappresentazione del contesto che
viene aggiornata passo dopo passo e che consente di mantere una memoria latente tra uno strato e quello successivo. Con una architettura di questo tipo
si spera che la rappresentazione semantica di una parola sia influenzata da quelle precedenti ad una distanza che va oltre la semplice finestra di contesto.



\medskip

\noindent
In una RNN, la sequenza di input viene processata iterativamente. A ogni passo
temporale $t$, il modello riceve in input il token corrente $x_t$ e aggiorna uno
\textit{stato nascosto} $h_t$, che dipende sia dall’input corrente sia dallo stato
precedente:
\[
h_t = f(h_{t-1}, x_t),
\]
dove $f$ è una funzione non lineare parametrizzata. Lo stato nascosto $h_t$
costituisce una rappresentazione compatta del contesto osservato fino al passo $t$ e
può essere interpretato come una rappresentazione contestuale del token corrente.

\medskip

\noindent
Nei modelli di linguaggio basati su RNN, la probabilità della parola successiva viene
stimata a partire dallo stato nascosto corrente:
\[
P(w_t \mid w_1, \ldots, w_{t-1}) = g(h_{t-1}),
\]
dove $g$ è tipicamente una trasformazione affine seguita da una funzione softmax.
In questo modo, la RNN è in grado di modellare dipendenze sequenziali senza imporre
un limite fisso alla dimensione del contesto, rappresentando un avanzamento
significativo rispetto ai modelli feedforward.

\medskip

\noindent
Nonostante questi vantaggi, le RNN presentano importanti difficoltà nell’apprendere
dipendenze a lungo raggio. Durante l’addestramento tramite backpropagation through
time, i gradienti possono tendere a svanire o esplodere, rendendo difficile
l’aggiornamento efficace dei parametri per sequenze lunghe. Questo problema,
noto come \textit{vanishing gradient problem}, limita la capacità delle RNN di
catturare relazioni distanti nel testo.

\medskip

\noindent
Per affrontare tali limiti sono state introdotte le \textit{Long Short-Term Memory}
(LSTM), una variante delle RNN progettata per mantenere e aggiornare informazioni
rilevanti su orizzonti temporali più lunghi. Le LSTM introducono una struttura di
memoria esplicita, controllata da meccanismi di gating, che regolano quali
informazioni debbano essere conservate, aggiornate o dimenticate nel tempo. Grazie
a questi meccanismi, le LSTM risultano più stabili durante l’addestramento e più
efficaci nel modellare dipendenze a lungo raggio.

\medskip

\noindent
Dal punto di vista delle rappresentazioni, gli stati nascosti prodotti da RNN e LSTM
possono essere interpretati come le prime forme di \emph{embeddings contestuali}
appresi da modelli di linguaggio neurali. Ogni token è associato a un vettore che
dipende dall’intera storia precedente della sequenza, consentendo di distinguere
occorrenze diverse della stessa parola in contesti diversi.

\medskip

\noindent
Tuttavia, anche le LSTM presentano limiti strutturali rilevanti. In primo luogo, il
contesto viene comunque compresso in un singolo stato nascosto, che deve riassumere
tutta l’informazione rilevante della sequenza precedente. In secondo luogo, le RNN e
le LSTM elaborano la sequenza in modo intrinsecamente sequenziale, impedendo una
parallelizzazione efficiente del calcolo. Infine, nei modelli di linguaggio standard,
il contesto utilizzato è tipicamente unidirezionale, limitato alle parole precedenti.

\medskip

\noindent
Questi limiti hanno motivato lo sviluppo di modelli in grado di sfruttare informazioni
provenienti sia dal contesto sinistro sia da quello destro di una parola. Nel
prossimo paragrafo analizzeremo ELMo, un modello che introduce embeddings
contestuali bidirezionali basati su reti ricorrenti, rappresentando un’importante
tappa intermedia nel percorso che conduce ai modelli basati sull’architettura
Transformer.


\section{ELMo e il contesto bidirezionale}
\label{sec:elmo}

Le RNN e le LSTM introducono per la prima volta rappresentazioni contestuali
dipendenti dalla sequenza, ma nei modelli di linguaggio standard tali
rappresentazioni sono tipicamente \emph{unidirezionali}, poiché il contesto utilizzato
per predire una parola è limitato alle parole precedenti. Tuttavia, molti compiti di
comprensione del linguaggio naturale richiedono di interpretare una parola alla luce
dell’intera frase, includendo anche il contesto destro. Questa osservazione ha
motivato lo sviluppo di modelli in grado di produrre rappresentazioni
contestuali \emph{bidirezionali}.

\medskip

\noindent
ELMo (\textit{Embeddings from Language Models}) rappresenta uno dei primi modelli
in grado di fornire embeddings contestuali profondi e bidirezionali. Il modello è
basato su una architettura \textit{biLSTM}, ovvero due LSTM separate che processano
la sequenza in direzione sinistra--destra e destra--sinistra. Le rappresentazioni
prodotte da entrambe le direzioni vengono quindi combinate per ottenere un embedding
contestuale per ciascun token.

\medskip

\noindent
Un aspetto distintivo di ELMo è l’utilizzo di rappresentazioni \emph{stratificate}. Il
modello produce infatti più livelli di rappresentazione per ogni token, ciascuno dei
quali cattura informazioni linguistiche a diversi livelli di astrazione. Le
rappresentazioni finali utilizzate nei task downstream non corrispondono
necessariamente all’output di un singolo strato, ma vengono ottenute come
combinazione pesata degli stati prodotti dai diversi strati del modello.

\medskip

\noindent
Dal punto di vista semantico, ELMo fornisce embeddings distinti per ogni occorrenza
di una parola, consentendo di catturare in modo efficace fenomeni di polisemia e
disambiguazione del senso. In questo senso, ELMo rappresenta un passaggio cruciale
nel superamento definitivo del paradigma degli embeddings statici e dimostra
empiricamente l’utilità delle rappresentazioni contestuali profonde per una vasta
gamma di compiti linguistici.

\medskip

\noindent
Nonostante questi progressi, ELMo eredita alcuni limiti strutturali dalle architetture
ricorrenti su cui è basato. In particolare, l’elaborazione della sequenza rimane
intrinsecamente sequenziale, limitando la possibilità di parallelizzazione del
calcolo. Inoltre, sebbene la bidirezionalità consenta di sfruttare l’intero contesto,
l’informazione deve comunque essere mediata attraverso stati ricorrenti, rendendo
difficile modellare direttamente dipendenze molto distanti nella sequenza.

\medskip

\noindent
Questi limiti hanno motivato l’introduzione di un nuovo paradigma architetturale, in
cui le parole di una sequenza possono interagire direttamente tra loro senza essere
mediate da uno stato ricorrente. Nel prossimo paragrafo introdurremo il meccanismo
di \textit{attention} e, in particolare, la \textit{self-attention}, che costituisce il
fondamento dei modelli Transformer.


\section{Attention e Self-Attention}
\label{sec:attention}

I modelli basati su RNN e LSTM, inclusi quelli bidirezionali come ELMo, producono
rappresentazioni contestuali efficaci, ma presentano un limite strutturale comune:
l’informazione contestuale deve essere mediata attraverso uno stato ricorrente che
riassume l’intera sequenza. Questo meccanismo di compressione rende difficile
modellare in modo diretto dipendenze a lungo raggio e relazioni complesse tra parole
distanti nella frase.

\medskip

\noindent
Per superare questo limite è stato introdotto il meccanismo di \textit{attention}, che
permette a un modello di selezionare dinamicamente le parti più rilevanti del
contesto quando deve produrre una rappresentazione o una predizione. L’idea
fondamentale dell’attention è che, anziché affidarsi a una singola rappresentazione
globale del contesto, il modello possa accedere direttamente a tutte le
rappresentazioni disponibili e assegnare loro un peso in base alla rilevanza rispetto
a un determinato obiettivo.

\medskip

\noindent
In termini intuitivi, il meccanismo di attention consente al modello di rispondere
alla domanda: \emph{quali parole del contesto sono più informative per interpretare
il token corrente?} I pesi di attenzione determinano l’importanza relativa di ciascun
token del contesto e vengono utilizzati per combinare le rappresentazioni disponibili
in una nuova rappresentazione contestuale.

\medskip

\noindent
Un’evoluzione fondamentale di questo meccanismo è rappresentata dalla
\textit{self-attention}. A differenza dell’attention classica, in cui l’attenzione è
calcolata tra due sequenze distinte (ad esempio una sequenza sorgente e una
sequenza target), nella self-attention ciascun token di una sequenza può
attendere direttamente a tutti gli altri token della stessa sequenza. In questo modo,
ogni parola costruisce la propria rappresentazione contestuale come combinazione
pesata delle rappresentazioni di tutte le altre parole della frase.

\medskip

\noindent
La self-attention presenta diversi vantaggi rispetto ai modelli ricorrenti. In primo
luogo, consente di modellare direttamente dipendenze a lungo raggio, poiché la
distanza tra due token nella sequenza non influisce sulla loro capacità di interagire.
In secondo luogo, l’elaborazione della sequenza non è più intrinsecamente
sequenziale: le rappresentazioni di tutti i token possono essere calcolate in
parallelo, migliorando significativamente l’efficienza computazionale e la
scalabilità del modello.

\medskip

\noindent
Dal punto di vista delle rappresentazioni, la self-attention produce per ciascun
token una rappresentazione contestuale che integra informazioni provenienti
dall’intera sequenza. Il significato di una parola emerge quindi come risultato
esplicito delle interazioni con tutte le altre parole della frase, piuttosto che come
un riassunto implicito codificato in uno stato ricorrente.

\medskip

\noindent
Tuttavia, il meccanismo di attention da solo non definisce un modello completo di
linguaggio o di rappresentazione. Per ottenere un’architettura in grado di produrre
rappresentazioni contestuali profonde e composizionali è necessario combinare la
self-attention con ulteriori componenti strutturali, come trasformazioni non lineari,
meccanismi di normalizzazione e informazioni sulla posizione dei token nella
sequenza.

\medskip

\noindent
Nel prossimo paragrafo introdurremo l’architettura Transformer, che integra il
meccanismo di self-attention in una struttura modulare e profonda, costituendo il
fondamento dei moderni modelli di linguaggio basati su embeddings contestuali,
incluso BERT.



\section{BERT e Masked Language Modeling}
\label{sec:bert}

L’architettura Transformer encoder introdotta nella sezione precedente fornisce un
meccanismo potente per la costruzione di rappresentazioni contestuali, ma da sola
non determina come tali rappresentazioni debbano essere apprese né quale obiettivo
di addestramento sia più adatto ai compiti di comprensione del linguaggio naturale.
BERT (\textit{Bidirectional Encoder Representations from Transformers}) rappresenta
una risposta a questa esigenza, combinando il Transformer encoder con un obiettivo
di addestramento specificamente progettato per produrre embeddings contestuali
profondamente bidirezionali.

\medskip

\noindent
A differenza dei modelli di linguaggio causali, che predicono la parola successiva
utilizzando esclusivamente il contesto sinistro, BERT utilizza un Transformer
\emph{encoder-only} e adotta un obiettivo di addestramento che consente al modello di
sfruttare simultaneamente il contesto sinistro e destro di ciascun token. Questa
caratteristica rende BERT particolarmente adatto a compiti interpretativi, come la
classificazione di testo, il riconoscimento di entità nominate e la disambiguazione
del senso delle parole.

\medskip

\noindent
Il principale obiettivo di addestramento di BERT è il \textit{Masked Language
Modeling} (MLM). Durante il pretraining, una frazione dei token di una frase viene
mascherata e il modello è addestrato a predire i token originali a partire dal contesto
circostante. In questo modo, il modello è costretto a costruire rappresentazioni che
integrano informazioni provenienti da entrambe le direzioni della sequenza, dando
luogo a embeddings contestuali profondamente bidirezionali.

\medskip

\noindent
Dal punto di vista architetturale, l’input di BERT è costituito dalla somma di tre
componenti: l’embedding del token (tipicamente ottenuto tramite una
tokenizzazione a sotto-parole), l’embedding di posizione e un embedding di segmento
utilizzato per distinguere parti diverse dell’input. L’output del modello è una
sequenza di vettori contestuali, uno per ciascun token, prodotti dall’ultimo strato
del Transformer encoder.

\medskip

\noindent
Un elemento distintivo di BERT è l’introduzione di un token speciale \texttt{[CLS]},
inserito all’inizio della sequenza di input. Il vettore contestuale associato a questo
token viene spesso utilizzato come rappresentazione dell’intera sequenza in compiti
di classificazione. Parallelamente, i vettori associati agli altri token forniscono
rappresentazioni contestuali a livello di parola, utilizzabili per compiti di
annotazione sequenziale e analisi semantica.

\medskip

\noindent
Come nei modelli contestuali precedenti, anche in BERT è comune non utilizzare
esclusivamente il vettore di uscita dell’ultimo strato, ma combinare le
rappresentazioni provenienti da più livelli del modello. In particolare, la media o
la somma dei vettori degli ultimi strati consente di integrare informazioni a diversi
livelli di astrazione, rendendo gli embeddings più robusti e informativi.

\medskip

\noindent
Il paradigma di addestramento di BERT segue lo schema \textit{pretrain}--\textit{finetune}.
Nella fase di pretraining, il modello apprende rappresentazioni generali del
linguaggio a partire da grandi quantità di testo non annotato. Nella fase di
finetuning, tali rappresentazioni vengono adattate a specifici compiti downstream
mediante l’aggiunta di teste di classificazione leggere e un addestramento
supervisionato su dataset più piccoli.

\medskip

\noindent
Dal punto di vista delle rappresentazioni, BERT produce embeddings contestuali
profondi, stratificati e ad alta dimensionalità, che incorporano in modo entangled
informazioni sintattiche, semantiche e pragmatiche. Questa ricchezza rappresentativa
è una delle principali ragioni del successo empirico di BERT, ma rende al contempo
complessa l’interpretazione delle singole dimensioni e delle strutture latenti degli
embedding.

\medskip

\noindent
Nel prossimo paragrafo discuteremo perché tali rappresentazioni, pur essendo
estremamente efficaci, beneficiano di tecniche di analisi e \textit{disentanglement},
motivando l’utilizzo di metodi basati su autoencoder sparsi per l’interpretazione e la
scomposizione degli embeddings di BERT, che costituisce l’obiettivo principale di
questa tesi.

\section{Motivazione per il disentanglement degli embeddings di BERT}
\label{sec:motivation_disentanglement}

Il percorso seguito in questo capitolo ha mostrato come gli embeddings moderni,
in particolare quelli prodotti da BERT, rappresentino il punto di arrivo di una
progressiva evoluzione delle rappresentazioni distribuzionali del linguaggio: da
vettori statici associati a tipi di parola a rappresentazioni contestuali profonde,
dipendenti dall’intera sequenza e apprese tramite modelli di linguaggio neurali
bidirezionali.

\medskip

\noindent
Gli embeddings di BERT sono caratterizzati da un’elevata capacità rappresentativa.
Essi catturano simultaneamente informazioni sintattiche, semantiche e pragmatiche,
distribuite su molte dimensioni e stratificate lungo i diversi livelli del modello.
Questa ricchezza informativa è alla base delle eccellenti prestazioni empiriche di
BERT in una vasta gamma di compiti di elaborazione del linguaggio naturale.

\medskip

\noindent
Tuttavia, tale potenza rappresentativa ha un costo in termini di interpretabilità.
Le informazioni codificate negli embeddings di BERT risultano fortemente
\emph{entangled}: singole dimensioni non corrispondono a proprietà linguistiche
chiaramente interpretabili e le strutture latenti che emergono nello spazio
vettoriale sono difficili da analizzare direttamente. Di conseguenza, comprendere
quali fattori semantici o sintattici contribuiscano a una determinata rappresentazione
diventa un compito non banale.

\medskip

\noindent
Questo problema è particolarmente rilevante nel contesto di applicazioni che
richiedono trasparenza, analisi qualitativa o controllo delle rappresentazioni
interne del modello. In tali scenari, non è sufficiente disporre di embeddings
accurati: è necessario poterli interpretare, analizzare e, in alcuni casi,
scomporre in componenti latenti più semplici e semanticamente coerenti.

\medskip

\noindent
Le tecniche di \textit{disentanglement} delle rappresentazioni mirano proprio a
questo obiettivo: separare i fattori latenti che contribuiscono alla costruzione di
una rappresentazione densa, rendendo esplicite strutture che risultano altrimenti
sovrapposte. In questo contesto, gli autoencoder sparsi rappresentano uno strumento
particolarmente adatto, poiché consentono di apprendere rappresentazioni latenti
compatte in cui solo un numero limitato di componenti è attivo per ciascun input.

\medskip

\noindent
Applicare tecniche di disentanglement agli embeddings di BERT consente quindi di
analizzare la struttura interna di queste rappresentazioni, individuare dimensioni o
fattori latenti interpretabili e studiare come diverse proprietà linguistiche
emergano nello spazio vettoriale. Questo approccio permette di coniugare l’elevata
capacità rappresentativa dei modelli di linguaggio moderni con un maggiore grado di
interpretabilità.

\medskip

\noindent
Nel capitolo successivo introdurremo il formalismo degli autoencoder e, in
particolare, degli \textit{sparse autoencoders}, discutendone le proprietà teoriche
e il loro utilizzo come strumento per il disentanglement di rappresentazioni dense.
Successivamente, tali tecniche verranno applicate agli embeddings di BERT
attraverso l’applicazione sviluppata in questa tesi, fornendo un caso di studio
concreto sull’analisi e l’interpretazione delle rappresentazioni contestuali.


\section{Pooling}

Data una sequenza di token
\begin{equation*}
(t_1, \dots, t_N),
\end{equation*}
ottenuta dalla tokenizzazione di un testo e fornita in input a un modello
Transformer di tipo BERT-like, il modello non restituisce una singola
rappresentazione vettoriale dell’intera sequenza, bensì una sequenza di
rappresentazioni contestuali a livello di token. Più precisamente, l’output
del modello può essere espresso come una matrice
\begin{equation*}
\mathbf{H} =
\begin{bmatrix}
\mathbf{h}_1 \\
\mathbf{h}_2 \\
\vdots \\
\mathbf{h}_N
\end{bmatrix}
\in \mathbb{R}^{N \times d},
\end{equation*}
dove $d$ indica la dimensionalità dello spazio latente del modello e ciascun
vettore $\mathbf{h}_i \in \mathbb{R}^{d}$ rappresenta l’embedding contestuale
del token $t_i$, ovvero una codifica che incorpora informazione proveniente
dall’intera sequenza grazie al meccanismo di self-attention.
In molte applicazioni di elaborazione del linguaggio naturale, tuttavia, non
si è interessati a una rappresentazione a livello di singolo token, bensì a un
unico vettore che riassuma il significato complessivo dell’intera sequenza,
come nel caso della similarità semantica, del clustering di testi o della
classificazione a livello di frase o documento. Per ottenere tale
rappresentazione globale è quindi necessario introdurre un’operazione di
\textit{pooling}.

Formalmente, un’operazione di pooling è una funzione che aggrega una
collezione di vettori in un singolo vettore:
\begin{equation*}
\text{Pooling} : \mathbb{R}^{N \times d} \rightarrow \mathbb{R}^{d}.
\end{equation*}
Applicando tale funzione alla matrice di embedding token-level $\mathbf{H}$,
si ottiene un vettore
\begin{equation*}
\mathbf{v} = \text{Pooling}(\mathbf{H}),
\end{equation*}
che può essere interpretato come una rappresentazione vettoriale globale
della sequenza di input, intesa come unità semantica.
L’operazione di pooling realizza dunque una riduzione strutturale lungo la
dimensione dei token, trasformando una rappresentazione distribuita su $N$
elementi in un singolo punto nello spazio latente. La scelta della funzione di
pooling non è univoca e dipende sia dal modello utilizzato sia dal tipo di
informazione che si desidera preservare.

\subsubsection{Strategie di pooling}

Le principali strategie di pooling utilizzate per ottenere una
rappresentazione vettoriale globale a partire dalle rappresentazioni
token-level prodotte da modelli Transformer possono essere descritte come
segue.

\begin{itemize}

\item \textbf{Mean pooling}.  
Una delle strategie più semplici e diffuse consiste nel calcolare la media
aritmetica dei vettori associati ai singoli token:
\begin{equation*}
\mathbf{v} = \frac{1}{N} \sum_{i=1}^{N} \mathbf{h}_i.
\end{equation*}
Questa operazione assegna a ciascun token lo stesso peso e produce una
rappresentazione che riflette il contributo medio dell’intera sequenza.
Il mean pooling è particolarmente adatto a compiti di similarità semantica
e ad analisi distribuzionali del significato, poiché preserva l’informazione
globale senza privilegiare posizioni specifiche della sequenza.

\item \textbf{Max pooling}.  
Un’alternativa è rappresentata dal max pooling, in cui ciascuna componente
del vettore finale è ottenuta selezionando il valore massimo osservato tra
tutti i token:
\begin{equation*}
\mathbf{v}_j = \max_{i=1,\dots,N} (\mathbf{h}_{i,j}),
\end{equation*}
dove $\mathbf{h}_{i,j}$ indica la $j$-esima componente del vettore
$\mathbf{h}_i$. Questa strategia enfatizza le feature più salienti presenti
nella sequenza, catturando segnali forti anche se localizzati in un numero
limitato di token, ma tende a sacrificare l’informazione distribuita lungo
l’intero testo.

\item \textbf{Token speciale di classificazione (\texttt{[CLS]})}.  
Nei modelli BERT-like è comune utilizzare il vettore associato a un token
speciale, tipicamente \texttt{[CLS]}, come rappresentazione globale della
sequenza:
\begin{equation*}
\mathbf{v} = \mathbf{h}_{\texttt{[CLS]}}.
\end{equation*}
In questo caso, il pooling non consiste in un’aggregazione esplicita dei
vettori token-level, ma nella selezione di un singolo vettore. Il token
\texttt{[CLS]} viene infatti addestrato durante il pretraining del modello
a catturare informazione a livello di sequenza, svolgendo il ruolo di una
rappresentazione globale implicita, particolarmente adatta a task di
classificazione.

\item \textbf{Pooling appreso}.  
Una classe più generale di approcci consiste nell’apprendere
l’operazione di pooling tramite un modello parametrico. In questo caso,
la rappresentazione globale è ottenuta applicando una funzione
$P_{\theta}$, tipicamente implementata come una rete neurale, alla matrice
delle rappresentazioni token-level:
\begin{equation*}
\mathbf{v} = P_{\theta}(\mathbf{H}).
\end{equation*}
Questo approccio consente di assegnare pesi differenti ai token e di
modellare interazioni più complesse rispetto alle strategie statiche,
al costo di una maggiore complessità del modello e della necessità di
obiettivi di addestramento specifici.

\end{itemize}

Alcune famiglie di modelli, come i \textit{Sentence-Transformers},
integrano direttamente l’operazione di pooling all’interno
dell’architettura. In tali modelli, un backbone Transformer produce le
rappresentazioni token-level, che vengono immediatamente aggregate tramite
una delle strategie descritte (tipicamente mean pooling), e l’intero
sistema è addestrato end-to-end per produrre embedding a livello di frase
ottimizzati per compiti di similarità semantica e retrieval.

\subsection{Pooling gerarchico per sequenze lunghe}

I modelli Transformer di tipo BERT-like impongono un vincolo strutturale
sulla lunghezza massima della sequenza di input, tipicamente indicata con
$L_{\max}$. Quando una sequenza di token supera tale limite, non è possibile
processarla integralmente in un singolo forward pass del modello. Sia quindi
dato un testo tokenizzato come
\begin{equation*}
T = (t_1, \dots, t_N),
\end{equation*}
con $N > L_{\max}$. In questo caso, per ottenere una rappresentazione
vettoriale globale del testo senza ricorrere alla troncatura, è necessario
adottare una strategia di aggregazione su più livelli.

\paragraph{Segmentazione in chunk}
La sequenza $T$ viene suddivisa in $K$ sotto-sequenze contigue (chunk)
\begin{equation*}
C_1, C_2, \dots, C_K,
\end{equation*}
ciascuna di lunghezza al più $W_{\text{eff}} \le L_{\max}$, dove
$W_{\text{eff}}$ rappresenta la lunghezza massima effettivamente disponibile
per i token di contenuto, una volta considerati eventuali token speciali
richiesti dal modello. Formalmente,
\begin{equation*}
C_k = (t_{(k-1)W_{\text{eff}}+1}, \dots, t_{\min(kW_{\text{eff}},\,N)}),
\qquad k = 1, \dots, K.
\end{equation*}

\paragraph{Pooling a livello di token (intra-chunk)}
Ciascun chunk $C_k$ viene processato indipendentemente dal modello
Transformer, che produce una matrice di rappresentazioni token-level
\begin{equation*}
\mathbf{H}^{(k)} \in \mathbb{R}^{m_k \times d},
\end{equation*}
dove $m_k$ è il numero di token del chunk $k$ e $d$ è la dimensionalità
latente del modello. Applicando una funzione di pooling a livello di token,
si ottiene un embedding rappresentativo del singolo chunk:
\begin{equation*}
\mathbf{e}_k = \text{Pooling}_{\text{token}}\!\left(\mathbf{H}^{(k)}\right),
\qquad \mathbf{e}_k \in \mathbb{R}^{d}.
\end{equation*}
Questa prima operazione di pooling realizza una riduzione dalla
rappresentazione distribuita sui token a una rappresentazione compatta a
livello di segmento.

\paragraph{Pooling a livello di chunk (inter-chunk)}
Una volta ottenuti gli embedding dei singoli chunk
$\mathbf{e}_1, \dots, \mathbf{e}_K$, è necessario aggregarli per costruire
una rappresentazione globale dell’intero testo. Tale aggregazione può essere
formalizzata come una seconda operazione di pooling:
\begin{equation*}
\mathbf{v}_D = \text{Pooling}_{\text{chunk}}
\bigl(\mathbf{e}_1, \dots, \mathbf{e}_K\bigr),
\qquad \mathbf{v}_D \in \mathbb{R}^{d}.
\end{equation*}
Nel caso più semplice, adottato in questo lavoro, la funzione
$\text{Pooling}_{\text{chunk}}$ coincide con la media aritmetica:
\begin{equation*}
\mathbf{v}_D = \frac{1}{K} \sum_{k=1}^{K} \mathbf{e}_k.
\end{equation*}

\paragraph{Interpretazione come pooling gerarchico}
La procedura descritta può essere interpretata come una forma di
\textit{pooling gerarchico}. Il primo livello di pooling aggrega
l’informazione a livello di token all’interno di ciascun chunk, mentre il
secondo livello aggrega l’informazione a livello di chunk sull’intero
documento. In questo modo, è possibile ottenere una rappresentazione
vettoriale globale anche per testi di lunghezza arbitraria, preservando il
contributo semantico di tutte le parti della sequenza e mantenendo la
compatibilità con i vincoli architetturali del modello di embedding.



%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Self-attention causale come somma pesata}
Consideriamo una sequenza di $n$ token e una loro rappresentazione vettoriale in input
\[
X=[x_1;\dots;x_n]\in\mathbb{R}^{n\times d},
\]
dove $d$ è la dimensionalità del modello e $x_i\in\mathbb{R}^{d}$ è il vettore alla posizione $i$. L'obiettivo di uno strato di self-attention è produrre una nuova sequenza
\[
A=[a_1;\dots;a_n]\in\mathbb{R}^{n\times d},
\]
in cui ogni $a_i$ integra informazione dai token nel contesto. Nel caso causale, il contesto di $i$ include soltanto posizioni $j\le i$ (nessun accesso al futuro).

Un modo intuitivo di vedere l'attenzione è come una combinazione lineare di vettori di contesto:
\[
a_i=\sum_{j\le i}\alpha_{ij}x_j,
\qquad
\sum_{j\le i}\alpha_{ij}=1,\ \alpha_{ij}\ge 0,
\]
dove i pesi $\alpha_{ij}$ esprimono quanto la posizione $i$ debba utilizzare informazione dalla posizione $j$. La difficoltà sta nel definire in modo differenziabile i pesi $\alpha_{ij}$ in funzione della sequenza stessa; il Transformer risolve questo problema introducendo tre ruoli distinti per ciascun token: query, key e value \cite{jm3}.

\subsection{Un head di attenzione: query, key, value e scaling}
In un singolo head di attenzione, ciascun input $x_i$ viene proiettato in tre spazi tramite matrici apprese:
\[
q_i=x_iW_Q,\qquad k_i=x_iW_K,\qquad v_i=x_iW_V,
\]
con $W_Q,W_K\in\mathbb{R}^{d\times d_k}$ e $W_V\in\mathbb{R}^{d\times d_v}$. La rilevanza tra la posizione corrente $i$ e una posizione di contesto $j$ è stimata confrontando la query corrente con la key del contesto tramite prodotto scalare, scalato per stabilità numerica:
\[
s_{ij}=\frac{q_i\cdot k_j}{\sqrt{d_k}}.
\]
I punteggi $s_{ij}$ vengono normalizzati con una softmax per ottenere i pesi di attenzione:
\[
\alpha_{ij}=\frac{\exp(s_{ij})}{\sum_{k\le i}\exp(s_{ik})}\quad\text{per }j\le i,
\]
e l'output del head alla posizione $i$ è la somma pesata dei value:
\[
\text{head}_i=\sum_{j\le i}\alpha_{ij}v_j.
\]
Infine, per riportare l'output alla dimensionalità del modello, si applica una proiezione lineare
\[
a_i=\text{head}_iW_O,
\qquad
W_O\in\mathbb{R}^{d_v\times d}.
\]
In questa forma l'attenzione è già una generalizzazione diretta del meccanismo introdotto nelle architetture encoder--decoder: la differenza sostanziale è che qui la memoria consultata è la sequenza stessa (auto-attenzione), e l'operazione viene applicata simultaneamente a tutte le posizioni \cite{jm3}.

\subsection{Mascheramento causale}
La definizione precedente assume esplicitamente $j\le i$. In implementazione, però, è comodo calcolare tutti i confronti tra tutte le coppie di posizioni e poi annullare i contributi che guardano nel futuro. Si introduce quindi una maschera triangolare superiore $M\in\mathbb{R}^{n\times n}$ tale che
\[
M_{ij}=
\begin{cases}
0 & j\le i\\
-\infty & j>i
\end{cases}
\]
e si applica la softmax ai punteggi mascherati, in modo che le probabilità verso $j>i$ diventino nulle. Questo garantisce che, in un modello autoregressivo, la predizione del token successivo non possa ``sbirciare'' i token futuri \cite{jm3}.

\subsection{Multi-head attention}
Un singolo head impone una singola nozione di somiglianza (via $W_Q,W_K,W_V$). Il Transformer usa invece $A$ head in parallelo per permettere al modello di rappresentare simultaneamente relazioni diverse tra token (dipendenze sintattiche, segnali coreferenziali, associazioni tematiche, ecc.). Per ciascun head $c\in\{1,\dots,A\}$ si hanno matrici distinte $W_Q^{(c)},W_K^{(c)},W_V^{(c)}$ e si ottiene un output $\text{head}^{(c)}_i\in\mathbb{R}^{d_v}$. Gli output dei head vengono concatenati e proiettati di nuovo in $\mathbb{R}^{d}$:
\[
a_i=\big(\text{head}^{(1)}_i\oplus\dots\oplus\text{head}^{(A)}_i\big)W_O,
\qquad
W_O\in\mathbb{R}^{Ad_v\times d}.
\]
La modularità è importante: l'input e l'output di un layer di multi-head attention hanno entrambi dimensione $d$, condizione che consente di impilare molti blocchi identici per costruire reti profonde \cite{jm3}.

\subsection{Il blocco Transformer: residual stream, layer norm e feedforward}
La self-attention è il nucleo del Transformer, ma il blocco completo contiene anche componenti che stabilizzano l'ottimizzazione e aumentano la capacità espressiva: connessioni residue, normalizzazione e una rete feedforward punto-a-punto. Indichiamo con $x_i^{(\ell)}$ la rappresentazione del token $i$ in ingresso al blocco $\ell$ e con $h_i^{(\ell)}$ l'uscita del blocco. Nella variante oggi più comune (prenorm) il blocco può essere descritto come segue:
\[
t_i=\text{LayerNorm}(x_i^{(\ell)}),
\qquad
u_i=x_i^{(\ell)}+\text{MHA}_i(t_{1:n}),
\]
\[
r_i=\text{LayerNorm}(u_i),
\qquad
h_i^{(\ell)}=u_i+\text{FFN}(r_i).
\]
Qui $\text{MHA}_i(\cdot)$ indica la multi-head attention calcolata alla posizione $i$ usando come memoria l'intera sequenza normalizzata $t_{1:n}$ (con maschera causale nel caso autoregressivo). La parte $\text{FFN}$ è una rete a due strati applicata indipendentemente a ciascuna posizione:
\[
\text{FFN}(z)=\phi(zW_1+b_1)W_2+b_2,
\]
dove tipicamente la dimensionalità interna $d_{ff}$ è maggiore di $d$ e $\phi$ è una non-linearità (ad esempio ReLU o GELU). Le connessioni residue garantiscono che l'informazione possa fluire attraverso molti strati senza degradarsi, mentre la layer normalization mantiene controllata la scala delle attivazioni durante l'addestramento \cite{jm3}.

\subsection{Forma matriciale e parallelizzazione}
Una proprietà distintiva del Transformer è che l'intero blocco può essere eseguito in parallelo su tutte le posizioni, sfruttando moltiplicazioni tra matrici. Impacchettando gli input in $X\in\mathbb{R}^{n\times d}$, si costruiscono
\[
Q=XW_Q,\qquad K=XW_K,\qquad V=XW_V,
\]
e l'attenzione (per un head) si esprime come:
\[
\text{Att}(X)=\text{softmax}\!\Big(\frac{QK^\top+M}{\sqrt{d_k}}\Big)V,
\]
dove $M$ implementa il mascheramento causale. Questa forma rende evidente che il costo computazionale della self-attention cresce quadraticamente con la lunghezza $n$ (si costruisce una matrice $n\times n$ di confronti), un aspetto che diventa critico quando si vogliono trattare contesti molto lunghi \cite{jm3}.

\subsection{Input encoding: embedding di token e informazione di posizione}
Senza ricorrenza, il Transformer non possiede un meccanismo intrinseco che codifichi l'ordine. Per questo, l'input $X$ non è dato soltanto dagli embedding dei token, ma dalla somma tra un embedding di contenuto e un embedding di posizione. Se $E\in\mathbb{R}^{|V|\times d}$ è la matrice di embedding del vocabolario e $P\in\mathbb{R}^{n\times d}$ una matrice di embedding posizionali (ad esempio assoluti, uno per posizione), allora per ogni posizione $i$:
\[
x_i=E[w_i]+P[i],
\]
e la sequenza $X$ è costruita impilando questi vettori. La componente posizionale può anche essere definita in forme alternative (sinusoidali o relative), ma l'idea fondamentale resta: rendere disponibile al modello un segnale sull'ordine per permettere all'attenzione di distinguere tra permutazioni della stessa multinsieme di token \cite{jm3}.

\subsection{Nota sul ruolo del Transformer rispetto a encoder e decoder}
Nel contesto del language modeling autoregressivo, una pila di blocchi Transformer con maschera causale realizza un modello che predice il token successivo da sinistra a destra. Questa configurazione viene spesso chiamata decoder-only, per analogia storica con l'architettura Transformer originaria encoder--decoder. Al contrario, modelli come BERT (che introdurremo nella sezione successiva) utilizzano uno stack di blocchi senza mascheramento causale, in modo da permettere a ogni token di integrare informazione sia dal contesto sinistro sia da quello destro; concettualmente, si tratta della componente encoder del Transformer \cite{jm3}. Questa distinzione sarà essenziale per comprendere perché BERT produca embeddings contestuali profondamente bidirezionali e perché l'obiettivo di addestramento debba essere diverso dal next-token prediction.
